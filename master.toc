\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Potentials in molecular dynamics}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Neural network potentials}{2}{section.1.2}
\contentsline {section}{\numberline {1.3}Goals}{3}{section.1.3}
\contentsline {section}{\numberline {1.4}Our contributions}{4}{section.1.4}
\contentsline {section}{\numberline {1.5}Structure of the thesis}{5}{section.1.5}
\contentsline {part}{I\hspace {1em}Theory}{7}{part.1}
\contentsline {chapter}{\numberline {2}Molecular dynamics}{9}{chapter.2}
\contentsline {section}{\numberline {2.1}Potential energy surfaces}{9}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}From quantum mechanics to classical potentials}{10}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Constructing potential energy surfaces}{10}{subsection.2.1.2}
\contentsline {subsubsection}{Truncation and configuration space}{11}{section*.2}
\contentsline {subsubsection}{Fitting procedure}{11}{section*.3}
\contentsline {section}{\numberline {2.2}Common empirical potentials}{12}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Lennard-Jones}{12}{subsection.2.2.1}
\contentsline {subsubsection}{Calculating total potential energy}{13}{section*.5}
\contentsline {subsection}{\numberline {2.2.2}Stillinger-Weber}{14}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Vashishta}{15}{subsection.2.2.3}
\contentsline {section}{\numberline {2.3}Time integration}{15}{section.2.3}
\contentsline {section}{\numberline {2.4}Force calculations and cutoff radius}{16}{section.2.4}
\contentsline {chapter}{\numberline {3}Machine learning}{19}{chapter.3}
\contentsline {section}{\numberline {3.1}Artificial neurons}{20}{section.3.1}
\contentsline {section}{\numberline {3.2}Neural network types}{22}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Feed-forward neural networks}{23}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Recurrent neural networks}{24}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Other types of networks}{24}{subsection.3.2.3}
\contentsline {section}{\numberline {3.3}Multilayer perceptron}{24}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Why multilayer perceptrons?}{25}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Mathematical model}{25}{subsection.3.3.2}
\contentsline {subsubsection}{Activation function of output neuron}{29}{section*.14}
\contentsline {subsubsection}{Matrix-vector notation}{29}{section*.15}
\contentsline {section}{\numberline {3.4}Activation functions}{30}{section.3.4}
\contentsline {section}{\numberline {3.5}Training}{32}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Cost functions}{32}{subsection.3.5.1}
\contentsline {section}{\numberline {3.6}Optimization}{33}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Gradient descent variants}{34}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Optimization algorithms}{34}{subsection.3.6.2}
\contentsline {subsubsection}{Momentum}{35}{section*.18}
\contentsline {subsubsection}{Adagrad}{35}{section*.19}
\contentsline {subsubsection}{Adadelta}{36}{section*.20}
\contentsline {subsubsection}{Adam}{36}{section*.21}
\contentsline {subsubsection}{Which optimizer to use?}{37}{section*.22}
\contentsline {subsection}{\numberline {3.6.3}Backpropagation}{37}{subsection.3.6.3}
\contentsline {subsubsection}{1. Forward propagation}{38}{section*.23}
\contentsline {subsubsection}{2. Backward propagation}{38}{section*.24}
\contentsline {subsubsection}{Matrix notation}{41}{section*.26}
\contentsline {subsubsection}{Training algorithm}{42}{section*.27}
\contentsline {chapter}{\numberline {4}Neural networks in molecular dynamics}{43}{chapter.4}
\contentsline {section}{\numberline {4.1}Neural network potentials}{43}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Potentials using single neural network}{43}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Potentials using multiple neural networks}{44}{subsection.4.1.2}
\contentsline {section}{\numberline {4.2}The Behler-Parrinello method}{44}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Symmetry functions}{45}{subsection.4.2.1}
\contentsline {subsubsection}{Radial symmetry functions}{46}{section*.29}
\contentsline {subsubsection}{Angular symmetry functions}{49}{section*.31}
\contentsline {subsubsection}{Setting the symmetry parameters}{49}{section*.33}
\contentsline {subsection}{\numberline {4.2.2}Symmetry functions and forces}{50}{subsection.4.2.2}
\contentsline {subsubsection}{Change of coordinates}{52}{section*.34}
\contentsline {subsection}{\numberline {4.2.3}Summary}{54}{subsection.4.2.3}
\contentsline {part}{II\hspace {1em}Implementation and results}{57}{part.2}
\contentsline {chapter}{\numberline {5}LAMMPS}{59}{chapter.5}
\contentsline {section}{\numberline {5.1}Installing LAMMPS}{59}{section.5.1}
\contentsline {section}{\numberline {5.2}LAMMPS input script}{60}{section.5.2}
\contentsline {section}{\numberline {5.3}LAMMPS structure}{64}{section.5.3}
\contentsline {section}{\numberline {5.4}Extending LAMMPS}{64}{section.5.4}
\contentsline {chapter}{\numberline {6}TensorFlow}{73}{chapter.6}
\contentsline {section}{\numberline {6.1}Installing TensorFlow}{74}{section.6.1}
\contentsline {section}{\numberline {6.2}TensorFlow basic usage}{75}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Hello world}{75}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}Creating a neural network}{76}{subsection.6.2.2}
\contentsline {subsection}{\numberline {6.2.3}Visualizing the graph}{77}{subsection.6.2.3}
\contentsline {subsection}{\numberline {6.2.4}Training a NN with TensorFlow}{79}{subsection.6.2.4}
\contentsline {chapter}{\numberline {7}Constructing a neural network potential}{83}{chapter.7}
\contentsline {section}{\numberline {7.1}Selecting the reference set}{83}{section.7.1}
\contentsline {subsection}{\numberline {7.1.1}Iterative molecular dynamics sampling}{84}{subsection.7.1.1}
\contentsline {subsection}{\numberline {7.1.2}Sampling algorithms}{85}{subsection.7.1.2}
\contentsline {subsubsection}{Initial sampling}{86}{section*.40}
\contentsline {subsubsection}{Iterative sampling}{90}{section*.42}
\contentsline {subsubsection}{Summary}{91}{section*.43}
\contentsline {section}{\numberline {7.2}Constructing the symmetry function sets}{92}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}Initial set}{92}{subsection.7.2.1}
\contentsline {subsubsection}{Angular symmetry functions}{93}{section*.45}
\contentsline {subsubsection}{Number of symmetry functions}{93}{section*.47}
\contentsline {subsection}{\numberline {7.2.2}Adjusting the set}{95}{subsection.7.2.2}
\contentsline {subsubsection}{For each symmetry function, the range of function values should be as large as possible}{95}{section*.48}
\contentsline {subsubsection}{The set of values of two different symmetry functions on a given data set should not be strongly correlated}{95}{section*.49}
\contentsline {section}{\numberline {7.3}Setting hyperparameters}{96}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Preconditioning the input data}{96}{subsection.7.3.1}
\contentsline {subsubsection}{Convergence speed vs evaluation speed}{97}{section*.50}
\contentsline {subsection}{\numberline {7.3.2}Activation functions and weight initialization}{98}{subsection.7.3.2}
\contentsline {subsection}{\numberline {7.3.3}Network architecture and overfitting}{99}{subsection.7.3.3}
\contentsline {subsubsection}{Other regularization teqhniques}{102}{section*.52}
\contentsline {subsection}{\numberline {7.3.4}Optimizer parameters}{102}{subsection.7.3.4}
\contentsline {subsection}{\numberline {7.3.5}Cost function}{103}{subsection.7.3.5}
\contentsline {subsection}{\numberline {7.3.6}Hyperparameter space exploration}{104}{subsection.7.3.6}
\contentsline {subsubsection}{Coordinate descent}{104}{section*.53}
\contentsline {subsubsection}{Grid search}{104}{section*.54}
\contentsline {subsubsection}{Random search}{105}{section*.55}
\contentsline {subsection}{\numberline {7.3.7}Summary}{105}{subsection.7.3.7}
\contentsline {section}{\numberline {7.4}Applying the neural network potential}{105}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}Transfer neural network from Python to C\texttt {++}}{105}{subsection.7.4.1}
\contentsline {chapter}{\numberline {8}Lennard-Jones validation}{109}{chapter.8}
\contentsline {section}{\numberline {8.1}Error in configuration space}{111}{section.8.1}
\contentsline {section}{\numberline {8.2}Many-neighbour Lennard-Jones}{112}{section.8.2}
\contentsline {subsection}{\numberline {8.2.1}Alternative way to obtain forces}{113}{subsection.8.2.1}
\contentsline {chapter}{\numberline {9}Neural network potential for Si}{117}{chapter.9}
\contentsline {section}{\numberline {9.1}Sampling initial set}{117}{section.9.1}
\contentsline {section}{\numberline {9.2}Initial symmetry function set}{117}{section.9.2}
\contentsline {section}{\numberline {9.3}Fitting the initial data set}{118}{section.9.3}
\contentsline {section}{\numberline {9.4}Iterative sampling}{120}{section.9.4}
\contentsline {section}{\numberline {9.5}Fitting the final data set}{122}{section.9.5}
\contentsline {section}{\numberline {9.6}Applying the NNP}{124}{section.9.6}
\contentsline {part}{III\hspace {1em}Conclusions and future work}{129}{part.3}
\contentsline {chapter}{\numberline {10}Summary}{131}{chapter.10}
\contentsline {chapter}{\numberline {11}Conclusions}{133}{chapter.11}
\contentsline {chapter}{\numberline {12}Prospects and future work}{135}{chapter.12}
\contentsline {section}{\numberline {12.1}Sampling methods}{135}{section.12.1}
\contentsline {chapter}{Appendices}{137}{section*.70}
\contentsline {chapter}{\numberline {A}Symmetry functions derivatives}{139}{Appendix.1.A}
\contentsline {chapter}{\numberline {B}Symmetry function parameters}{145}{Appendix.1.B}
