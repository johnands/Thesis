\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Goals}{chapter.1}% 2
\BOOKMARK [-1][-]{part.1}{I Theory}{}% 3
\BOOKMARK [0][-]{chapter.2}{Molecular dynamics}{part.1}% 4
\BOOKMARK [1][-]{section.2.1}{Potential energy surfaces}{chapter.2}% 5
\BOOKMARK [2][-]{subsection.2.1.1}{From quantum mechanics to classical potentials}{section.2.1}% 6
\BOOKMARK [2][-]{subsection.2.1.2}{Constructing potential energy surfaces}{section.2.1}% 7
\BOOKMARK [1][-]{section.2.2}{Common empirical potentials}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.2.1}{Lennard-Jones}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.2}{Stillinger-Weber}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.3}{Vashishta}{section.2.2}% 11
\BOOKMARK [1][-]{section.2.3}{Time integration}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.4}{Force calculations and cutoff radius}{chapter.2}% 13
\BOOKMARK [0][-]{chapter.3}{Machine learning}{part.1}% 14
\BOOKMARK [1][-]{section.3.1}{Artificial neurons}{chapter.3}% 15
\BOOKMARK [1][-]{section.3.2}{Neural network types}{chapter.3}% 16
\BOOKMARK [2][-]{subsection.3.2.1}{Feed-forward neural networks}{section.3.2}% 17
\BOOKMARK [2][-]{subsection.3.2.2}{Recurrent neural networks}{section.3.2}% 18
\BOOKMARK [2][-]{subsection.3.2.3}{Other types of networks}{section.3.2}% 19
\BOOKMARK [1][-]{section.3.3}{Multilayer perceptron}{chapter.3}% 20
\BOOKMARK [2][-]{subsection.3.3.1}{Why multilayer perceptrons?}{section.3.3}% 21
\BOOKMARK [2][-]{subsection.3.3.2}{Mathematical model}{section.3.3}% 22
\BOOKMARK [1][-]{section.3.4}{Activation functions}{chapter.3}% 23
\BOOKMARK [1][-]{section.3.5}{Training}{chapter.3}% 24
\BOOKMARK [2][-]{subsection.3.5.1}{Cost functions}{section.3.5}% 25
\BOOKMARK [1][-]{section.3.6}{Optimization}{chapter.3}% 26
\BOOKMARK [2][-]{subsection.3.6.1}{Gradient descent variants}{section.3.6}% 27
\BOOKMARK [2][-]{subsection.3.6.2}{Optimization algorithms}{section.3.6}% 28
\BOOKMARK [2][-]{subsection.3.6.3}{Backpropagation}{section.3.6}% 29
\BOOKMARK [0][-]{chapter.4}{Neural networks in molecular dynamics}{part.1}% 30
\BOOKMARK [1][-]{section.4.1}{Neural network potentials}{chapter.4}% 31
\BOOKMARK [2][-]{subsection.4.1.1}{Potentials using single neural network}{section.4.1}% 32
\BOOKMARK [2][-]{subsection.4.1.2}{Potentials using multiple neural networks}{section.4.1}% 33
\BOOKMARK [1][-]{section.4.2}{The Behler-Parinello method}{chapter.4}% 34
\BOOKMARK [2][-]{subsection.4.2.1}{Symmetry functions}{section.4.2}% 35
\BOOKMARK [2][-]{subsection.4.2.2}{Symmetry functions and forces}{section.4.2}% 36
\BOOKMARK [2][-]{subsection.4.2.3}{Summary}{section.4.2}% 37
\BOOKMARK [-1][-]{part.2}{II Implementation and results}{}% 38
\BOOKMARK [0][-]{chapter.5}{LAMMPS}{part.2}% 39
\BOOKMARK [1][-]{section.5.1}{Installing LAMMPS}{chapter.5}% 40
\BOOKMARK [1][-]{section.5.2}{LAMMPS input script}{chapter.5}% 41
\BOOKMARK [1][-]{section.5.3}{LAMMPS structure}{chapter.5}% 42
\BOOKMARK [1][-]{section.5.4}{Extending LAMMPS}{chapter.5}% 43
\BOOKMARK [0][-]{chapter.6}{TensorFlow}{part.2}% 44
\BOOKMARK [1][-]{section.6.1}{Installing TensorFlow}{chapter.6}% 45
\BOOKMARK [1][-]{section.6.2}{TensorFlow basic usage}{chapter.6}% 46
\BOOKMARK [2][-]{subsection.6.2.1}{Hello world}{section.6.2}% 47
\BOOKMARK [2][-]{subsection.6.2.2}{Creating a neural network}{section.6.2}% 48
\BOOKMARK [2][-]{subsection.6.2.3}{Visualizing the graph}{section.6.2}% 49
\BOOKMARK [2][-]{subsection.6.2.4}{Training a NN with TensorFlow}{section.6.2}% 50
\BOOKMARK [0][-]{chapter.7}{Constructing a neural network potential}{part.2}% 51
\BOOKMARK [1][-]{section.7.1}{Selecting the training data}{chapter.7}% 52
\BOOKMARK [2][-]{subsection.7.1.1}{Iterative molecular dynamics sampling}{section.7.1}% 53
\BOOKMARK [2][-]{subsection.7.1.2}{Sampling algorithms}{section.7.1}% 54
\BOOKMARK [1][-]{section.7.2}{Constructing the symmetry function sets}{chapter.7}% 55
\BOOKMARK [2][-]{subsection.7.2.1}{Initial set}{section.7.2}% 56
\BOOKMARK [2][-]{subsection.7.2.2}{Adjusting the set}{section.7.2}% 57
\BOOKMARK [1][-]{section.7.3}{Setting hyperparameters}{chapter.7}% 58
\BOOKMARK [2][-]{subsection.7.3.1}{Preconditioning the input data}{section.7.3}% 59
\BOOKMARK [2][-]{subsection.7.3.2}{Network architecture and overfitting}{section.7.3}% 60
\BOOKMARK [2][-]{subsection.7.3.3}{Activation function and saturation}{section.7.3}% 61
\BOOKMARK [2][-]{subsection.7.3.4}{Symmetry functions}{section.7.3}% 62
\BOOKMARK [2][-]{subsection.7.3.5}{Cost function}{section.7.3}% 63
\BOOKMARK [2][-]{subsection.7.3.6}{Learning rate and mini-batch size}{section.7.3}% 64
\BOOKMARK [2][-]{subsection.7.3.7}{Number of iterations and overfitting}{section.7.3}% 65
\BOOKMARK [2][-]{subsection.7.3.8}{Weight initialization}{section.7.3}% 66
\BOOKMARK [2][-]{subsection.7.3.9}{Other techniques}{section.7.3}% 67
\BOOKMARK [1][-]{section.7.4}{Transfer NN from Python to C++}{chapter.7}% 68
\BOOKMARK [0][-]{chapter.8}{Validation}{part.2}% 69
\BOOKMARK [1][-]{section.8.1}{Training Lennard-Jones potential}{chapter.8}% 70
\BOOKMARK [2][-]{subsection.8.1.1}{Many-neighbour Lennard-Jones}{section.8.1}% 71
\BOOKMARK [0][-]{chapter.9}{NN potential for Si}{part.2}% 72
\BOOKMARK [0][-]{chapter.10}{NN potential for SiO2}{part.2}% 73
\BOOKMARK [-1][-]{part.3}{III Conclusions and future work}{}% 74
\BOOKMARK [0][-]{chapter.11}{The quality of the NNP}{part.3}% 75
\BOOKMARK [0][-]{section*.56}{Appendices}{part.3}% 76
\BOOKMARK [0][-]{Appendix.1.A}{Symmetry functions derivatives}{part.3}% 77
