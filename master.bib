
@article{LeCun15,
	title = {Deep learning},
	volume = {521},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {https://www.nature.com/nature/journal/v521/n7553/full/nature14539.html},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2017-09-20},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
	note = {doi: \href{http://dx.doi.org/10.1038/nature14539}{10.1038/nature14539}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/YQLPSQAD/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/CHRW3KB3/nature14539.html:text/html}, 
}

@article{Behler11general,
	title = {Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations},
	volume = {13},
	issn = {1463-9084},
	shorttitle = {Neural network potential-energy surfaces in chemistry},
	url = {http://pubs.rsc.org/en/content/articlelanding/2011/cp/c1cp21668f},
	doi = {10.1039/C1CP21668F},
	abstract = {The accuracy of the results obtained in molecular dynamics or Monte Carlo simulations crucially depends on a reliable description of the atomic interactions. A large variety of efficient potentials has been proposed in the literature, but often the optimum functional form is difficult to find and strongly depends on the particular system. In recent years, artificial neural networks (NN) have become a promising new method to construct potentials for a wide range of systems. They offer a number of advantages: they are very general and applicable to systems as different as small molecules, semiconductors and metals; they are numerically very accurate and fast to evaluate; and they can be constructed using any electronic structure method. Significant progress has been made in recent years and a number of successful applications demonstrate the capabilities of neural network potentials. In this Perspective, the current status of NN potentials is reviewed, and their advantages and limitations are discussed.},
	language = {en},
	number = {40},
	urldate = {2017-09-20},
	journal = {Physical Chemistry Chemical Physics},
	author = {Behler, Jörg},
	month = oct,
	year = {2011},
	pages = {17930--17955},
	note = {doi: \href{http://dx.doi.org/10.1039/C1CP21668F}{10.1039/C1CP21668F}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/FFNMBUAN/Behler - 2011 - Neural network potential-energy surfaces in chemis.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/AL2AP3F9/c1cp21668f.html:text/html}
}

@article{Behler11symmetry,
	title = {Atom-centered symmetry functions for constructing high-dimensional neural network potentials},
	volume = {134},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/10.1063/1.3553717},
	doi = {10.1063/1.3553717},
	abstract = {Neural networks offer an unbiased and numerically very accurate approach to represent high-dimensional ab initio potential-energy surfaces. Once constructed, neural network potentials can provide the energies and forces many orders of magnitude faster than electronic structure calculations, and thus enable molecular dynamics simulations of large systems. However, Cartesian coordinates are not a good choice to represent the atomic positions, and a transformation to symmetry functions is required. Using simple benchmark systems, the properties of several types of symmetry functions suitable for the construction of high-dimensional neural network potential-energy surfaces are discussed in detail. The symmetry functions are general and can be applied to all types of systems such as molecules, crystalline and amorphous solids, and liquids.},
	number = {7},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Behler, J\"{o}rg},
	month = feb,
	year = {2011},
	pages = {074106},
	note = {doi: \href{http://dx.doi.org/10.1063/1.3553717}{10.1063/1.3553717}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/Q8WBUL52/Behler - 2011 - Atom-centered symmetry functions for constructing .pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/JLFSBAI7/1.html:text/html}
}

@article{Dragly14,
	title = {Bridging {Quantum} {Mechanics} and {Molecular} {Dynamics} with {Artificial} {Neural} {Networks}},
	url = {https://www.duo.uio.no/handle/10852/41843},
	abstract = {In this thesis, we explore how a classical potential can be constructed by fitting an artificial neural network to the potential energy surface of an ab initio calculation. A Hartree-Fock implementation is explained in detail and used to calculate the potential energy surface. Further, we provide details on how a molecular dynamics code is implemented. This is verified by a simulation of argon crystallization. Results from the Hartree-Fock and molecular dynamics implementations are well aligned with those found in the literature. To bridge these two implementations in a simulation of hydrogen dissociation, the potential energy surface of hydrogen is fitted with the Fast Artificial Neural Network Library and applied in molecular dynamics. The results are on par with a study using the Kohen-Tully-Stillinger potential. In comparison, the artificial neural network potential is parameterized without empirical data nor initial assumptions about the form of the potential function, but suffers a performance loss by a factor of 10 - 20. Finally, we discuss different techniques used to visualize molecules, including isosurface and volumetric rendering of electron densities, and billboard rendering of systems with millions of atoms.},
	language = {eng},
	urldate = {2017-09-20},
	author = {Dragly, Svenn-Arne},
	year = {2014},
	note = {url: \href{https://www.duo.uio.no/handle/10852/41843}{duo.uio.no/handle/10852/41843}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/XQTP2SJQ/Dragly - 2014 - Bridging Quantum Mechanics and Molecular Dynamics .pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/HIJM5VQW/41843.html:text/html}
}

@article{Hornik89,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	urldate = {2017-09-20},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	note = {doi: \href{http://dx.doi.org/10.1016/0893-6080(89)90020-8}{10.1016/0893-6080(89)90020-8}},
	file = {ScienceDirect Full Text PDF:/home/johnands/Zotero/storage/SSXRA25M/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:application/pdf;ScienceDirect Snapshot:/home/johnands/Zotero/storage/8QZR68SE/0893608089900208.html:text/html}
}

@book{Rojas96,
	title = {Neural {Networks}: {A} {Systematic} {Introduction}},
	isbn = {978-3-540-60505-8},
	shorttitle = {Neural {Networks}},
	abstract = {Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Rojas, Raul},
	month = jul,
	year = {1996},
	keywords = {Computers / Computer Simulation, Computers / Computer Vision \& Pattern Recognition, Computers / Desktop Applications / Design \& Graphics, Computers / Intelligence (AI) \& Semantics, Computers / Machine Theory, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General, Computers / Systems Architecture / General, Computers / User Interfaces, Science / Life Sciences / Biology, Science / Life Sciences / General}
}

@article{Karlik11,
	title = {Performance {Analysis} of {Various} {Activation} {Functions} in {Generalized} {MLP} {Architectures} of {Neural} {Networks}},
	volume = {1},
	abstract = {The activation function used to transform the activation level of a unit (neuron) into an output signal. There are a number of common activation functions in use with artificial neural networks (ANN). The most common choice of activation functions for multi layered perceptron (MLP) is used as transfer functions in research and engineering. Among the reasons for this popularity are its boundedness in the unit interval, the function's and its derivative's fast computability, and a number of amenable mathematical properties in the realm of approximation theory. However, considering the huge variety of problem domains MLP is applied in, it is intriguing to suspect that specific problems call for single or a set of specific activation functions. The aim of this study is to analyze the performance of generalized MLP architectures which has back-propagation algorithm using various different activation functions for the neurons of hidden and output layers. For experimental comparisons, Bi-polar sigmoid, Uni-polar sigmoid, Tanh, Conic Section, and Radial Bases Function (RBF) were used.},
	journal = {International Journal of Artificial Intelligence And Expert Systems},
	author = {Vehbi Olgac, A and Karlik, Bekir},
	month = feb,
	year = {2011},
	pages = {111--122},
	file = {Snapshot:/home/johnands/Zotero/storage/XC6SIFJN/228813985_Performance_Analysis_of_Various_Activation_Functions_in_Generalized_MLP_Architectures_.pdf:application/pdf}
}

@InProceedings{Glorot11,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dudík},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {http://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training. [pdf]}
}

@article{Rumelhart86,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {© 1986 Nature Publishing Group},
	issn = {0028-0836},
	url = {https://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2017-09-20},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	note = {doi: \href{http://dx.doi.org/10.1038/323533a0}{10.1038/323533a0}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/NL4LLTZN/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/NUYKCS7P/323533a0.html:text/html}
}

@article{Behler15,
	title = {Constructing high-dimensional neural network potentials: {A} tutorial review},
	volume = {115},
	issn = {1097-461X},
	shorttitle = {Constructing high-dimensional neural network potentials},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/qua.24890/abstract},
	doi = {10.1002/qua.24890},
	abstract = {A lot of progress has been made in recent years in the development of atomistic potentials using machine learning (ML) techniques. In contrast to most conventional potentials, which are based on physical approximations and simplifications to derive an analytic functional relation between the atomic configuration and the potential-energy, ML potentials rely on simple but very flexible mathematical terms without a direct physical meaning. Instead, in case of ML potentials the topology of the potential-energy surface is “learned” by adjusting a number of parameters with the aim to reproduce a set of reference electronic structure data as accurately as possible. Due to this bias-free construction, they are applicable to a wide range of systems without changes in their functional form, and a very high accuracy close to the underlying first-principles data can be obtained. Neural network potentials (NNPs), which have first been proposed about two decades ago, are an important class of ML potentials. Although the first NNPs have been restricted to small molecules with only a few degrees of freedom, they are now applicable to high-dimensional systems containing thousands of atoms, which enables addressing a variety of problems in chemistry, physics, and materials science. In this tutorial review, the basic ideas of NNPs are presented with a special focus on developing NNPs for high-dimensional condensed systems. A recipe for the construction of these potentials is given and remaining limitations of the method are discussed. © 2015 Wiley Periodicals, Inc.},
	language = {en},
	number = {16},
	urldate = {2017-09-20},
	journal = {International Journal of Quantum Chemistry},
	author = {Behler, Jörg},
	month = aug,
	year = {2015},
	keywords = {molecular dynamics, neural network potentials},
	pages = {1032--1050},
	note = {doi: \href{http://dx.doi.org/10.1002/qua.24890}{10.1002/qua.24890}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/EHW2ZLVD/Behler - 2015 - Constructing high-dimensional neural network poten.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/LTS9X226/abstract.html:text/html}
}

@article{Tuckerman92,
	title = {Reversible multiple time scale molecular dynamics},
	volume = {97},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.463137},
	doi = {10.1063/1.463137},
	number = {3},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Tuckerman, M. and Berne, B. J. and Martyna, G. J.},
	month = aug,
	year = {1992},
	pages = {1990--2001},
	note = {doi: \href{http://dx.doi.org/10.1063/1.463137}{10.1063/1.463137}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/CFCQDU9K/Tuckerman et al. - 1992 - Reversible multiple time scale molecular dynamics.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/GPJWX8MZ/1.html:text/html}
}

@article{Plimpton95,
	title = {Fast {Parallel} {Algorithms} for {Short}-{Range} {Molecular} {Dynamics}},
	volume = {117},
	issn = {0021-9991},
	url = {http://www.sciencedirect.com/science/article/pii/S002199918571039X},
	doi = {10.1006/jcph.1995.1039},
	abstract = {Three parallel algorithms for classical molecular dynamics are presented. The first assigns each processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute; the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models which can be difficult to parallelize efficiently—those with short-range forces where the neighbors of each atom change rapidly. They can be implemented on any distributed-memory parallel machine which allows for message-passing of data between independently executing processors. The algorithms are tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000 atoms on several parallel supercomputers--the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D. Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that the current generation of parallel machines is competitive with conventional vector supercomputers even for small problems. For large problems, the spatial algorithm achieves parallel efficiencies of 90\% and a 1840-node Intel Paragon performs up to 165 faster than a single Cray C9O processor. Trade-offs between the three algorithms and guidelines for adapting them to more complex molecular dynamics simulations are also discussed.},
	number = {1},
	urldate = {2017-09-20},
	journal = {Journal of Computational Physics},
	author = {Plimpton, Steve},
	month = mar,
	year = {1995},
	pages = {1--19},
	note = {doi: \href{http://dx.doi.org/10.1006/jcph.1995.1039}{10.1006/jcph.1995.1039}},
	file = {ScienceDirect Full Text PDF:/home/johnands/Zotero/storage/R88ANCSB/Plimpton - 1995 - Fast Parallel Algorithms for Short-Range Molecular.pdf:application/pdf;ScienceDirect Snapshot:/home/johnands/Zotero/storage/BCML9RQ9/S002199918571039X.html:text/html}
}

@article{Abadi15,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1603.04467},
	abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	urldate = {2017-09-20},
	journal = {Google Brain},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = mar,
	year = {2016},
	note = {arXiv: \href{http://arxiv.org/abs/1603.04467}{1603.04467}},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Learning},
	annote = {Comment: Version 2 updates only the metadata, to correct the formatting of Mart{\textbackslash}'in Abadi's name},
	file = {arXiv\:1603.04467 PDF:/home/johnands/Zotero/storage/W6XHFMBG/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/WPG2397V/1603.html:text/html}
}

@article{Abadi16,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	urldate = {2017-09-20},
	journal = {Google Brain},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = may,
	year = {2016},
	note = {arXiv: \href{http://arxiv.org/abs/1605.08695}{1605.08695}},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence},
	annote = {Comment: 18 pages, 9 figures; v2 has a spelling correction in the metadata},
	file = {arXiv\:1605.08695 PDF:/home/johnands/Zotero/storage/P8L9WLEJ/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learn.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/GBHZBMF8/1605.html:text/html}
}

@article{Qian99,
	title = {On the momentum term in gradient descent learning algorithms},
	volume = {12},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608098001166},
	doi = {10.1016/S0893-6080(98)00116-6},
	abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
	number = {1},
	urldate = {2017-09-20},
	journal = {Neural Networks},
	author = {Qian, Ning},
	month = jan,
	year = {1999},
	note = {doi: \href{http://dx.doi.org/10.1016/S0893-6080(98)00116-6}{10.1016/S0893-6080(98)00116-6}},
	keywords = {Critical damping, Damped harmonic oscillator, Gradient descent learning algorithm, Learning rate, Momentum, Speed of convergence},
	pages = {145-151},
	file = {ScienceDirect Full Text PDF:/home/johnands/Zotero/storage/6HJ99GKM/Qian - 1999 - On the momentum term in gradient descent learning .pdf:application/pdf;ScienceDirect Snapshot:/home/johnands/Zotero/storage/MF8BPQM3/S0893608098001166.html:text/html}
}

@article{Duchi11,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	urldate = {2017-09-20},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159},
	note = {url: \href{http://jmlr.org/papers/v12/duchi11a.html}{jmlr.org/papers/v12/duchi11a.html}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/ZUDYBKDB/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/X8Z27EBY/duchi11a.html:text/html}
}

@article{Zeiler12,
	title = {{ADADELTA}: {An} {Adaptive} {Learning} {Rate} {Method}},
	shorttitle = {{ADADELTA}},
	url = {http://arxiv.org/abs/1212.5701},
	abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
	urldate = {2017-09-20},
	author = {Zeiler, Matthew D.},
	month = dec,
	year = {2012},
	note = {arXiv: \href{http://arxiv.org/abs/1212.5701}{1212.5701}},
	keywords = {Computer Science - Learning},
	annote = {Comment: 6 pages},
	file = {arXiv\:1212.5701 PDF:/home/johnands/Zotero/storage/9CH92XBR/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/LR4XI6LA/1212.html:text/html}
}

@article{Kingma14,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2017-09-20},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: \href{http://arxiv.org/abs/1412.6980}{1412.6980}},
	keywords = {Computer Science - Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv\:1412.6980 PDF:/home/johnands/Zotero/storage/LP2TV4CF/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/K274BRES/1412.html:text/html}
}

@article{Ischtwan94,
	title = {Molecular potential energy surfaces by interpolation},
	volume = {100},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/10.1063/1.466801},
	doi = {10.1063/1.466801},
	number = {11},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Ischtwan, Josef and Collins, Michael A.},
	month = jun,
	year = {1994},
	pages = {8080--8088},
	note = {doi: \href{http://dx.doi.org/10.1063/1.466801}{10.1063/1.466801}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/X4R7RTHV/Ischtwan and Collins - 1994 - Molecular potential energy surfaces by interpolati.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/RG8JXKWK/1.html:text/html}
}

@article{Raff05,
	title = {Ab initio potential-energy surfaces for complex, multichannel systems using modified novelty sampling and feedforward neural networks},
	volume = {122},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/full/10.1063/1.1850458},
	doi = {10.1063/1.1850458},
	abstract = {A neural network/trajectory approach is presented for the development of accurate potential-energy hypersurfaces that can be utilized to conduct ab initio molecular dynamics (AIMD) and Monte Carlo studies of gas-phase chemical reactions, nanometric cutting, and nanotribology, and of a variety of mechanical properties of importance in potential microelectromechanical systems applications. The method is sufficiently robust that it can be applied to a wide range of polyatomic systems. The overall method integrates ab initio electronic structure calculations with importance sampling techniques that permit the critical regions of configuration space to be determined. The computed ab initio energies and gradients are then accurately interpolated using neural networks (NN) rather than arbitrary parametrized analytical functional forms, moving interpolation or least-squares methods. The sampling method involves a tight integration of molecular dynamics calculations with neural networks that employ early stopping and regularization procedures to improve network performance and test for convergence. The procedure can be initiated using an empirical potential surface or direct dynamics. The accuracy and interpolation power of the method has been tested for two cases, the global potential surface for vinyl bromide undergoing unimolecular decomposition via four different reaction channels and nanometric cutting of silicon. The results show that the sampling methods permit the important regions of configuration space to be easily and rapidly identified, that convergence of the NN fit to the ab initio electronic structure database can be easily monitored, and that the interpolation accuracy of the NN fits is excellent, even for systems involving five atoms or more. The method permits a substantial computational speed and accuracy advantage over existing methods, is robust, and relatively easy to implement.},
	number = {8},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Raff, L. M. and Malshe, M. and Hagan, M. and Doughan, D. I. and Rockley, M. G. and Komanduri, R.},
	month = feb,
	year = {2005},
	pages = {084104},
	note = {doi: \href{http://dx.doi.org/10.1063/1.1850458}{10.1063/1.1850458}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/WVPL7AMR/Raff et al. - 2005 - Ab initio potential-energy surfaces for complex, m.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/VGR769BA/1.html:text/html}
}

@book{Raff12,
	title = {Neural {Networks} in {Chemical} {Reaction} {Dynamics}},
	isbn = {978-0-19-976565-2},
	abstract = {This monograph presents recent advances in neural network (NN) approaches and applications to chemical reaction dynamics. Topics covered include: (i) the development of ab initio potential-energy surfaces (PES) for complex multichannel systems using modified novelty sampling and feedforward NNs; (ii) methods for sampling the configuration space of critical importance, such as trajectory and novelty sampling methods and gradient fitting methods; (iii) parametrization of interatomic potential functions using a genetic algorithm accelerated with a NN; (iv) parametrization of analytic interatomic potential functions using NNs; (v) self-starting methods for obtaining analytic PES from ab inito electronic structure calculations using direct dynamics; (vi) development of a novel method, namely, combined function derivative approximation (CFDA) for simultaneous fitting of a PES and its corresponding force fields using feedforward neural networks; (vii) development of generalized PES using many-body expansions, NNs, and moiety energy approximations; (viii) NN methods for data analysis, reaction probabilities, and statistical error reduction in chemical reaction dynamics; (ix) accurate prediction of higher-level electronic structure energies (e.g. MP4 or higher) for large databases using NNs, lower-level (Hartree-Fock) energies, and small subsets of the higher-energy database; and finally (x) illustrative examples of NN applications to chemical reaction dynamics of increasing complexity starting from simple near equilibrium structures (vibrational state studies) to more complex non-adiabatic reactions.The monograph is prepared by an interdisciplinary group of researchers working as a team for nearly two decades at Oklahoma State University, Stillwater, OK with expertise in gas phase reaction dynamics; neural networks; various aspects of MD and Monte Carlo (MC) simulations of nanometric cutting, tribology, and material properties at nanoscale; scaling laws from atomistic to continuum; and neural networks applications to chemical reaction dynamics. It is anticipated that this emerging field of NN in chemical reaction dynamics will play an increasingly important role in MD, MC, and quantum mechanical studies in the years to come.},
	publisher = {Oxford University Press},
	author = {Raff, Lionel and Komanduri, Ranga and Hagan, Martin and Bukkapatnam, Satish},
	month = feb,
	year = {2012},
	file = {Snapshot:/home/johnands/Zotero/storage/5CV6AT68/neural-networks-in-chemical-reaction-dynamics-9780199765652.html:text/html}
}

@article{Pukrittayakamee09,
	title = {Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks},
	volume = {130},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/10.1063/1.3095491},
	doi = {10.1063/1.3095491},
	abstract = {An improved neural network (NN) approach is presented for the simultaneous development of accurate potential-energy hypersurfaces and corresponding force fields that can be utilized to conduct ab initio molecular dynamics and Monte Carlo studies on gas-phase chemical reactions. The method is termed as combined function derivative approximation (CFDA). The novelty of the CFDA method lies in the fact that although the NN has only a single output neuron that represents potential energy, the network is trained in such a way that the derivatives of the NN output match the gradient of the potential-energy hypersurface. Accurate force fields can therefore be computed simply by differentiating the network. Both the computed energies and the gradients are then accurately interpolated using the NN. This approach is superior to having the gradients appear in the output layer of the NN because it greatly simplifies the required architecture of the network. The CFDA permits weighting of function fitting relative to gradient fitting. In every test that we have run on six different systems, CFDA training (without a validation set) has produced smaller out-of-sample testing error than early stopping (with a validation set) or Bayesian regularization (without a validation set). This indicates that CFDA training does a better job of preventing overfitting than the standard methods currently in use. The training data can be obtained using an empirical potential surface or any ab initio method. The accuracy and interpolation power of the method have been tested for the reaction dynamics of H+HBrH+HBr{\textless}math display="inline" overflow="scroll" altimg="eq-00001.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mtext{\textgreater}H{\textless}/mtext{\textgreater}{\textless}mo{\textgreater}+{\textless}/mo{\textgreater}{\textless}mtext{\textgreater}HBr{\textless}/mtext{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} using an analytical potential. The results show that the present NN training technique produces more accurate fits to both the potential-energy surface as well as the corresponding force fields than the previous methods. The fitting and interpolation accuracy is so high (rms error=1.2 cm−1)(rms error=1.2 cm−1){\textless}math display="inline" overflow="scroll" altimg="eq-00002.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mtext{\textgreater}rms{\textless}/mtext{\textgreater}{\textless}mtext{\textgreater} {\textless}/mtext{\textgreater}{\textless}mtext{\textgreater}error{\textless}/mtext{\textgreater}{\textless}mo{\textgreater}={\textless}/mo{\textgreater}{\textless}mn{\textgreater}1.2{\textless}/mn{\textgreater}{\textless}mtext{\textgreater} {\textless}/mtext{\textgreater}{\textless}msup{\textgreater}{\textless}mrow{\textgreater}{\textless}mtext{\textgreater}cm{\textless}/mtext{\textgreater}{\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}−{\textless}/mo{\textgreater}{\textless}mn{\textgreater}1{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msup{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} that trajectories computed on the NN potential exhibit point-by-point agreement with corresponding trajectories on the analytic surface.},
	number = {13},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Pukrittayakamee, A. and Malshe, M. and Hagan, M. and Raff, L. M. and Narulkar, R. and Bukkapatnum, S. and Komanduri, R.},
	month = apr,
	year = {2009},
	pages = {134101},
	note = {doi: \href{http://dx.doi.org/10.1063/1.3095491}{10.1063/1.3095491}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/ZQUUNHRZ/Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/92KUTFGR/1.html:text/html}
}

@article{McCulloch43,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {0007-4985, 1522-9602},
	url = {https://link.springer.com/article/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2017-09-20},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
	note = {doi: \href{http://dx.doi.org/10.1007/BF02478259}{10.1007/BF02478259}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/5YMQNFQJ/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/QDSEUF5P/10.html:text/html}
}

@unpublished{Kriesel07,
	author = {Kriesel, A.},
	title = {A Brief Introduction to Neural Networks},
	url = {http://www.dkriesel.com/},
	urldate = {2017-09-20},
	note = {url: \href{http://www.dkriesel.com/}{www.dkriesel.com/}},
	file = {News [D. Kriesel]:/home/johnands/Zotero/storage/D4V9HPXE/www.dkriesel.com.html:text/html}
}

@incollection{LeCun99,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Object {Recognition} with {Gradient}-{Based} {Learning}},
	isbn = {978-3-540-66722-3 978-3-540-46805-9},
	url = {https://link.springer.com/chapter/10.1007/3-540-46805-6_19},
	abstract = {Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.},
	language = {en},
	urldate = {2017-09-20},
	booktitle = {Shape, {Contour} and {Grouping} in {Computer} {Vision}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {LeCun, Yann and Haffner, Patrick and Bottou, Léon and Bengio, Yoshua},
	year = {1999},
	pages = {319--345},
	note = {url: \href{https://link.springer.com/chapter/10.1007/3-540-46805-6\_19}{link.springer.com/chapter/10.1007/3-540-46805-6\_19}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/TXXRQ45Z/LeCun et al. - 1999 - Object Recognition with Gradient-Based Learning.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/2GIUXDLW/10.html:text/html}
}

@article{Rosenblatt58,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization} in {The} {Brain}},
	shorttitle = {The {Perceptron}},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {65--386},
	file = {Citeseer - Full Text PDF:/home/johnands/Zotero/storage/NB68MH4C/Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf:application/pdf;Citeseer - Snapshot:/home/johnands/Zotero/storage/NDW9L6F3/summary.html:text/html}
}

@article{Ruder16,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2017-09-20},
	author = {Ruder, Sebastian},
	month = sep,
	year = {2016},
	note = {arXiv: \href{http://arxiv.org/abs/1609.04747}{1609.04747}},
	keywords = {Computer Science - Learning},
	annote = {Comment: Added derivations of AdaMax and Nadam},
	file = {arXiv\:1609.04747 PDF:/home/johnands/Zotero/storage/YFUXK7TZ/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/LJRUITHC/1609.html:text/html}
}

@article{Dawes08,
	title = {Interpolating moving least-squares methods for fitting potential energy surfaces: {A} strategy for efficient automatic data point placement in high dimensions},
	volume = {128},
	issn = {0021-9606},
	shorttitle = {Interpolating moving least-squares methods for fitting potential energy surfaces},
	url = {http://aip.scitation.org/doi/10.1063/1.2831790},
	doi = {10.1063/1.2831790},
	abstract = {An accurate and efficient method for automated molecular global potential energy surface (PES) construction and fitting is demonstrated. An interpolating moving least-squares (IMLS) method is developed with the flexibility to fit various ab initio data: (1) energies, (2) energies and gradients, or (3) energies, gradients, and Hessian data. The method is automated and flexible so that a PES can be optimally generated for trajectories, spectroscopy, or other applications. High efficiency is achieved by employing local IMLS in which fitting coefficients are stored at a limited number of expansion points, thus eliminating the need to perform weighted least-squares fits each time the potential is evaluated. An automatic point selection scheme based on the difference in two successive orders of IMLS fits is used to determine where new ab initio data need to be calculated for the most efficient fitting of the PES. A simple scan of the coordinate is shown to work well to identify these maxima in one dimension, but this search strategy scales poorly with dimension. We demonstrate the efficacy of using conjugate gradient minimizations on the difference surface to locate optimal data point placement in high dimensions. Results that are indicative of the accuracy, efficiency, and scalability are presented for a one-dimensional model potential (Morse) as well as for three-dimensional (HCN), six-dimensional (HOOH), and nine-dimensional (CH4)(CH4){\textless}math display="inline" overflow="scroll" altimg="eq-00001.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}C{\textless}/mi{\textgreater}{\textless}msub{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}H{\textless}/mi{\textgreater}{\textless}mn{\textgreater}4{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} molecular PESs.},
	number = {8},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Dawes, Richard and Thompson, Donald L. and Wagner, Albert F. and Minkoff, Michael},
	month = feb,
	year = {2008},
	pages = {084107},
	note = {doi: \href{http://dx.doi.org/10.1063/1.2831790}{10.1063/1.2831790}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/EMMBQYWI/Dawes et al. - 2008 - Interpolating moving least-squares methods for fit.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/LK6CUTKG/1.html:text/html}
}

@article{Jones24,
	title = {On the {Determination} of {Molecular} {Fields}. {II}. {From} the {Equation} of {State} of a {Gas}},
	volume = {106},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/106/738/463},
	doi = {10.1098/rspa.1924.0082},
	language = {en},
	number = {738},
	urldate = {2017-09-20},
	journal = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Jones, J. E.},
	month = oct,
	year = {1924},
	pages = {463--477},
	note = {doi: \href{http://dx.doi.org/10.1098/rspa.1924.0082}{10.1098/rspa.1924.0082}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/4CRS6BMR/Jones - 1924 - On the Determination of Molecular Fields. II. From.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/4M32R7MZ/463.html:text/html}
}

@article{Stillinger85,
	title = {Computer simulation of local order in condensed phases of silicon},
	volume = {31},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.31.5262},
	doi = {10.1103/PhysRevB.31.5262},
	abstract = {A model potential-energy function comprising both two- and three-atom contributions is proposed to describe interactions in solid and liquid forms of Si. Implications of this potential are then explored by molecular-dynamics computer simulation, using 216 atoms with periodic boundary conditions. Starting with the diamond-structure crystal at low temperature, heating causes spontaneous nucleation and melting. The resulting liquid structurally resembles the real Si melt. By carrying out steepest-descent mappings of system configurations onto potential-energy minima, two main conclusions emerge: (1) a temperature-independent inherent structure underlies the liquid phase, just as for ‘‘simple’’ liquids with only pair interactions; (2) the Lindemann melting criterion for the crystal apparently can be supplemented by a freezing criterion for the liquid, where both involve critical values of appropriately defined mean displacements from potential minima.},
	number = {8},
	urldate = {2017-09-20},
	journal = {Physical Review B},
	author = {Stillinger, Frank H. and Weber, Thomas A.},
	month = apr,
	year = {1985},
	pages = {5262--5271},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevB.31.5262}{10.1103/PhysRevB.31.5262}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/VPJUZNG5/PhysRevB.31.html:text/html}
}

@article{Molinero08,
	title = {Water {Modeled} {As} an {Intermediate} {Element} between {Carbon} and {Silicon}},
	volume = {113},
	issn = {1520-6106},
	url = {http://dx.doi.org/10.1021/jp805227c},
	doi = {10.1021/jp805227c},
	abstract = {Water and silicon are chemically dissimilar substances with common physical properties. Their liquids display a temperature of maximum density, increased diffusivity on compression, and they form tetrahedral crystals and tetrahedral amorphous phases. The common feature to water, silicon, and carbon is the formation of tetrahedrally coordinated units. We exploit these similarities to develop a coarse-grained model of water (mW) that is essentially an atom with tetrahedrality intermediate between carbon and silicon. mW mimics the hydrogen-bonded structure of water through the introduction of a nonbond angular dependent term that encourages tetrahedral configurations. The model departs from the prevailing paradigm in water modeling: the use of long-ranged forces (electrostatics) to produce short-ranged (hydrogen-bonded) structure. mW has only short-range interactions yet it reproduces the energetics, density and structure of liquid water, and its anomalies and phase transitions with comparable or better accuracy than the most popular atomistic models of water, at less than 1\% of the computational cost. We conclude that it is not the nature of the interactions but the connectivity of the molecules that determines the structural and thermodynamic behavior of water. The speedup in computing time provided by mW makes it particularly useful for the study of slow processes in deeply supercooled water, the mechanism of ice nucleation, wetting-drying transitions, and as a realistic water model for coarse-grained simulations of biomolecules and complex materials.},
	number = {13},
	urldate = {2017-09-20},
	journal = {The Journal of Physical Chemistry B},
	author = {Molinero, Valeria and Moore, Emily B.},
	month = apr,
	year = {2009},
	pages = {4008--4016},
	note = {doi: \href{http://dx.doi.org/10.1021/jp805227c}{10.1021/jp805227c}},
	file = {ACS Full Text PDF w/ Links:/home/johnands/Zotero/storage/E9YLUYY2/Molinero and Moore - 2009 - Water Modeled As an Intermediate Element between C.pdf:application/pdf;ACS Full Text Snapshot:/home/johnands/Zotero/storage/K2Z9LBT2/jp805227c.html:text/html}
}

@article{Vashishta90,
	title = {Interaction potential for $\mathrm{SiO}_2$: {A} molecular-dynamics study of structural correlations},
	volume = {41},
	shorttitle = {Interaction potential for \$\{{\textbackslash}mathrm\{{SiO}\}\}\_\{2\}\$},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.41.12197},
	doi = {10.1103/PhysRevB.41.12197},
	abstract = {An interaction potential consisting of two-body and three-body covalent interactions is proposed for SiO2. The interaction potential is used in molecular-dynamics studies of structural and dynamical correlations of crystalline, molten, and vitreous states under various conditions of densities and temperatures. The two-body contribution to the interaction potential consists of steric repulsion due to atomic sizes, Coulomb interactions resulting from charge transfer, and charge-dipole interaction to include the effects of large electronic polarizability of anions. The three-body covalent contributions include O-Si-O and Si-O-Si interactions which are angle dependent and functions of Si-O distance. In lattice-structure calculations with the total potential function, α-cristobalite and α-quartz are found to have the lowest and almost degenerate energies, in agreement with experiments. The energies for β-cristobalite, β-quartz, and keatite are found to be higher than those for α-cristobalite and α-quartz. Molecular-dynamics calculations with this potential function correctly describe the short- and intermediate-range order in molten and vitreous states.},
	number = {17},
	urldate = {2017-09-20},
	journal = {Physical Review B},
	author = {Vashishta, P. and Kalia, Rajiv K. and Rino, José P. and Ebbsjö, Ingvar},
	month = jun,
	year = {1990},
	pages = {12197--12209},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevB.41.12197}{10.1103/PhysRevB.41.12197}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/P7JI5QVC/PhysRevB.41.html:text/html}
}

@article{Vashishta07,
	title = {Interaction potential for silicon carbide: {A} molecular dynamics study of elastic constants and vibrational density of states for crystalline and amorphous silicon carbide},
	volume = {101},
	issn = {0021-8979},
	shorttitle = {Interaction potential for silicon carbide},
	url = {http://aip.scitation.org/doi/full/10.1063/1.2724570},
	doi = {10.1063/1.2724570},
	abstract = {An effective interatomic interaction potential for SiC is proposed. The potential consists of two-body and three-body covalent interactions. The two-body potential includes steric repulsions due to atomic sizes, Coulomb interactions resulting from charge transfer between atoms, charge-induced dipole-interactions due to the electronic polarizability of ions, and induced dipole-dipole (van der Waals) interactions. The covalent characters of the Si–C–Si and C–Si–C bonds are described by the three-body potential. The proposed three-body interaction potential is a modification of the Stillinger-Weber form proposed to describe Si. Using the molecular dynamics method, the interaction potential is used to study structural, elastic, and dynamical properties of crystalline (3C), amorphous, and liquid states of SiC for several densities and temperatures. The structural energy for cubic (3C) structure has the lowest energy, followed by the wurtzite (2H) and rock-salt (RS) structures. The pressure for the structural transformation from 3C-to-RS from the common tangent is found to be 90 GPa. For 3C-SiC, our computed elastic constants (C11C11{\textless}math baseline="-6.5" display="inline" overflow="scroll" altimg="eq-00001.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}C{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}11{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}, C12C12{\textless}math baseline="-6.5" display="inline" overflow="scroll" altimg="eq-00002.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}C{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}12{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}, and C44C44{\textless}math baseline="-6.5" display="inline" overflow="scroll" altimg="eq-00003.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}C{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mn{\textgreater}44{\textless}/mn{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}), melting temperature, vibrational density-of-states, and specific heat agree well with the experiments. Predictions are made for the elastic constant as a function of density for the crystalline and amorphous phase. Structural correlations, such as pair distribution function and neutron and x-ray static structure factors are calculated for the amorphous and liquid state.},
	number = {10},
	urldate = {2017-09-20},
	journal = {Journal of Applied Physics},
	author = {Vashishta, Priya and Kalia, Rajiv K. and Nakano, Aiichiro and Rino, José Pedro},
	month = may,
	year = {2007},
	pages = {103515},
	note = {doi: \href{http://dx.doi.org/10.1063/1.2724570}{10.1063/1.2724570}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/Q4ZLAE2E/Vashishta et al. - 2007 - Interaction potential for silicon carbide A molec.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/6YME3VAM/1.html:text/html}
}

@book{Frenkel01,
	edition = {1st},
	title = {Understanding {Molecular} {Simulation}: {From} {Algorithms} to {Applications}},
	isbn = {978-0-12-267370-2},
	shorttitle = {Understanding {Molecular} {Simulation}},
	abstract = {From the Publisher:This book explains the physics behind the "recipes" of molecular simulation for materials science. Computer simulators are continuously confronted with questions concerning the choice of a particular technique for a given application. Since a wide variety of computational tools exists, the choice of technique requires a good understanding of the basic principles. More importantly, such understanding may greatly improve the efficiency of a simulation program. The implementation of simulation methods is illustrated in pseudocodes and their practical use in the case studies used in the text. Examples are included that highlight current applications, and the codes of the case studies are available on the World Wide Web. No prior knowledge of computer simulation is assumed.},
	publisher = {Academic Press},
	editor = {Frenkel, Daan and Smit, Berend},
	year = {1996}
}

@article{Agrawal06,
	title = {Molecular dynamics investigations of the dissociation of {SiO}2 on an ab initio potential energy surface obtained using neural network methods},
	volume = {124},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/10.1063/1.2185638},
	doi = {10.1063/1.2185638},
	abstract = {The neural network (NN) procedure to interpolate ab initio data for the purpose of molecular dynamics (MD) simulations has been tested on the SiO2SiO2{\textless}math display="inline" overflow="scroll" altimg="eq-00003.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}Si{\textless}/mi{\textgreater}{\textless}msub{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}O{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} system. Unlike other similar NN studies, here, we studied the dissociation of SiO2SiO2{\textless}math display="inline" overflow="scroll" altimg="eq-00004.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}Si{\textless}/mi{\textgreater}{\textless}msub{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}O{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} without the initial use of any empirical potential. During the dissociation of SiO2SiO2{\textless}math display="inline" overflow="scroll" altimg="eq-00005.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}Si{\textless}/mi{\textgreater}{\textless}msub{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}O{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} into Si+OSi+O{\textless}math display="inline" overflow="scroll" altimg="eq-00006.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}Si{\textless}/mi{\textgreater}{\textless}mo{\textgreater}+{\textless}/mo{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}O{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater} or Si+O2Si+O2{\textless}math display="inline" overflow="scroll" altimg="eq-00007.gif"{\textgreater}{\textless}mrow{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}Si{\textless}/mi{\textgreater}{\textless}mo{\textgreater}+{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi mathvariant="normal"{\textgreater}O{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}, the spin multiplicity of the system changes from singlet to triplet in the first reaction and from singlet to pentet in the second. This paper employs four potential surfaces. The first is a NN fit [NN(STP)] to a database comprising the lowest of the singlet, triplet, and pentet energies obtained from density functional calculations in 6673 nuclear configurations. The other three potential surfaces are obtained from NN fits to the singlet, triplet, and pentet-state energies. The dissociation dynamics on the singlet-state and NN(STP) surfaces are reported. The results obtained using the singlet surface correspond to those expected if the reaction were to occur adiabatically. The dynamics on the NN(STP) surface represent those expected if the reaction follows a minimum-energy pathway. This study on a small system demonstrates the application of NNs for MD studies using ab initio data when the spin multiplicity of the system changes during the dissociation process.},
	number = {13},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Agrawal, Paras M. and Raff, Lionel M. and Hagan, Martin T. and Komanduri, Ranga},
	month = apr,
	year = {2006},
	pages = {134306},
	note = {doi: \href{http://dx.doi.org/10.1063/1.2185638}{10.1063/1.2185638}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/WDBKYEU4/Agrawal et al. - 2006 - Molecular dynamics investigations of the dissociat.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/6Y5IIZ8B/1.html:text/html}
}

@article{Bholoa07,
	series = {Computer {Simulation} of {Radiation} {Effects} in {Solids}},
	title = {A new approach to potential fitting using neural networks},
	volume = {255},
	issn = {0168-583X},
	url = {http://www.sciencedirect.com/science/article/pii/S0168583X06010548},
	doi = {10.1016/j.nimb.2006.11.040},
	abstract = {A methodology is presented for developing transferable empirical potential functions without following the usual procedure of postulating a functional form. Instead, a neural network (NN) is employed to learn the functional relationships of potential energy surfaces from the local geometric arrangement of atoms. The methodology is illustrated by training the NN model on tens of thousands of individual data points derived from the tight-binding (TB) method for a wide range of silicon systems including both small clusters and bulk structures. Comparisons of the potential’s properties with experimental data, quantum methods and other Si potentials have been made. The NN model successfully fitted energy variations of the different test cases as a function of bond distances, bond angles, lattice constants and elastic properties for both equilibrium and non-equilibrium small cluster and bulk structures. This indicates a robust and consistent methodology for fitting empirical potentials which can be applied to a wide range of materials independent of the type of bonding or their crystal structure.},
	number = {1},
	urldate = {2017-09-20},
	journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
	author = {Bholoa, A. and Kenny, S. D. and Smith, R.},
	month = feb,
	year = {2007},
	keywords = {Empirical potentials, Neural network, Tight-binding},
	pages = {1--7},
	note = {doi: \href{http://dx.doi.org/10.1016/j.nimb.2006.11.040}{10.1016/j.nimb.2006.11.040}},
	file = {ScienceDirect Full Text PDF:/home/johnands/Zotero/storage/UDYQYPDS/Bholoa et al. - 2007 - A new approach to potential fitting using neural n.pdf:application/pdf;ScienceDirect Snapshot:/home/johnands/Zotero/storage/YZBH27TT/S0168583X06010548.html:text/html}
}

@article{Behler07,
	title = {Generalized {Neural}-{Network} {Representation} of {High}-{Dimensional} {Potential}-{Energy} {Surfaces}},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.98.146401},
	doi = {10.1103/PhysRevLett.98.146401},
	abstract = {The accurate description of chemical processes often requires the use of computationally demanding methods like density-functional theory (DFT), making long simulations of large systems unfeasible. In this Letter we introduce a new kind of neural-network representation of DFT potential-energy surfaces, which provides the energy and forces as a function of all atomic positions in systems of arbitrary size and is several orders of magnitude faster than DFT. The high accuracy of the method is demonstrated for bulk silicon and compared with empirical potentials and DFT. The method is general and can be applied to all types of periodic and nonperiodic systems.},
	number = {14},
	urldate = {2017-09-20},
	journal = {Physical Review Letters},
	author = {Behler, Jörg and Parrinello, Michele},
	month = apr,
	year = {2007},
	pages = {146401},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevLett.98.146401}{10.1103/PhysRevLett.98.146401}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/2DPZ8RX3/PhysRevLett.98.html:text/html}
}

@incollection{LeCun12,
author="LeCun, Yann A.
and Bottou, L{\'e}on
and Orr, Genevieve B.
and M{\"u}ller, Klaus-Robert",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Efficient BackProp",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--48",
abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_3",
note = {url: \href{https://link.springer.com/chapter/10.1007/978-3-642-35289-8\_3}{link.springer.com/chapter/10.1007/978-3-642-35289-8\_3}},
url="https://doi.org/10.1007/978-3-642-35289-8_3"
}

@incollection{Bengio12,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Practical {Recommendations} for {Gradient}-{Based} {Training} of {Deep} {Architectures}},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8_26},
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	language = {en},
	urldate = {2017-09-20},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bengio, Yoshua},
	year = {2012},
	pages = {437--478},
	file = {Full Text PDF:/home/johnands/Zotero/storage/NA3GBTTU/Bengio - 2012 - Practical Recommendations for Gradient-Based Train.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/NC2XLI72/978-3-642-35289-8_26.html:text/html}
}

@article{Larochelle09,
	title = {Exploring {Strategies} for {Training} {Deep} {Neural} {Networks}},
	volume = {10},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v10/larochelle09a.html},
	number = {Jan},
	urldate = {2017-09-20},
	journal = {Journal of Machine Learning Research},
	author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, Jérôme and Lamblin, Pascal},
	year = {2009},
	pages = {1--40},
	note = {url: \href{http://www.jmlr.org/papers/v10/larochelle09a.html}{www.jmlr.org/papers/v10/larochelle09a.html}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/TEHEL5KJ/Larochelle et al. - 2009 - Exploring Strategies for Training Deep Neural Netw.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/ZP23PW2S/larochelle09a.html:text/html}
}

@article{Witkoskie05,
	title = {Neural {Network} {Models} of {Potential} {Energy} {Surfaces}:  {Prototypical} {Examples}},
	volume = {1},
	issn = {1549-9618},
	shorttitle = {Neural {Network} {Models} of {Potential} {Energy} {Surfaces}},
	url = {http://dx.doi.org/10.1021/ct049976i},
	doi = {10.1021/ct049976i},
	abstract = {Neural networks can be used generate potential energy hypersurfaces by fitting to a data set of energies at discrete geometries, as might be obtained from ab initio calculations. Prior work has shown that this method can generate accurate fits in complex systems of several dimensions. The present paper explores fundamental properties of neural network potential representations in some simple prototypes of one, two, and three dimensions. Optimal fits to the data are achieved by adjusting the network parameters using an extended Kalman filtering algorithm, which is described in detail. The examples provide insight into the relationships between the form of the function being fit, the amount of data needed for an adequate fit, and the optimal network configuration and number of neurons needed. The quality of the network interpolation is substantially improved if gradients as well as the energy are available for fitting. The fitting algorithm is effective in providing an accurate interpolation of the underlying potential function even when random noise is added to the data used in the fit.},
	number = {1},
	urldate = {2017-09-20},
	journal = {Journal of Chemical Theory and Computation},
	author = {Witkoskie, James B. and Doren, Douglas J.},
	month = jan,
	year = {2005},
	pages = {14--23},
	note = {doi: \href{http://dx.doi.org/10.1021/ct049976i}{10.1021/ct049976i}},
	file = {ACS Full Text Snapshot:/home/johnands/Zotero/storage/BC6ZBH9W/ct049976i.html:text/html}
}


@incollection{Bengio07,
title = "Scaling learning algorithms towards {AI}",
author = "Yoshua Bengio and Yann Lecun",
year = "2007",
editor = "L. Bottou and O. Chapelle and D. DeCoste and J. Weston",
booktitle = "Large-scale kernel machines",
publisher = "MIT Press",
}



@InProceedings{Glorot10,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Xavier Glorot and Yoshua Bengio},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Yee Whye Teh and Mike Titterington},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
  note = {url: \href{http://proceedings.mlr.press/v9/glorot10a.html}{proceedings.mlr.press/v9/glorot10a.html}},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


@incollection{Krogh1992,
	title = {A {Simple} {Weight} {Decay} {Can} {Improve} {Generalization}},
	url = {http://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization.pdf},
	urldate = {2017-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 4},
	publisher = {Morgan-Kaufmann},
	author = {Krogh, Anders and Hertz, John A.},
	editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
	year = {1992},
	pages = {950--957},
	file = {NIPS Full Text PDF:/home/johnands/Zotero/storage/K68UWSSY/Krogh and Hertz - 1992 - A Simple Weight Decay Can Improve Generalization.pdf:application/pdf;NIPS Snapshort:/home/johnands/Zotero/storage/4E3WFU8N/563-a-simple-weight-decay-can-improve-generalization.html:text/html}
}

@article{Srivastava14,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	urldate = {2017-09-20},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	note = {url: \href{http://jmlr.org/papers/v15/srivastava14a.html}{jmlr.org/papers/v15/srivastava14a.html}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/7QZ7P7M4/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/BVAL3FBS/srivastava14a.html:text/html}
}

@incollection{Krizhevsky12,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2017-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:/home/johnands/Zotero/storage/3Z93LFV8/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshort:/home/johnands/Zotero/storage/98TSWNZQ/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{Hinton12,
	title = {Deep {Neural} {Networks} for {Acoustic} {Modeling} in {Speech} {Recognition}},
	url = {https://research.google.com/pubs/pub38131.html},
	urldate = {2017-09-20},
	author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
	year = {2012},
	note = {url: \href{https://research.google.com/pubs/pub38131.html}{research.google.com/pubs/pub38131.html}},
	file = {Snapshot:/home/johnands/Zotero/storage/P76A95TV/pub38131.html:text/html}
}

@article{Collobert11,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2017-09-20},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: \href{http://arxiv.org/abs/1103.0398}{1103.0398}},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1103.0398 PDF:/home/johnands/Zotero/storage/YFLWN26C/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:application/pdf;arXiv.org Snapshot:/home/johnands/Zotero/storage/HEWX2833/1103.html:text/html}
}

@article{Ciodaro12,
	title = {Online particle detection with {Neural} {Networks} based on topological calorimetry information},
	volume = {368},
	issn = {1742-6596},
	url = {http://stacks.iop.org/1742-6596/368/i=1/a=012030},
	doi = {10.1088/1742-6596/368/1/012030},
	abstract = {This paper presents the latest results from the Ringer algorithm, which is based on artificial neural networks for the electron identification at the online filtering system of the ATLAS particle detector, in the context of the LHC experiment at CERN. The algorithm performs topological feature extraction using the ATLAS calorimetry information (energy measurements). The extracted information is presented to a neural network classifier. Studies showed that the Ringer algorithm achieves high detection efficiency, while keeping the false alarm rate low. Optimizations, guided by detailed analysis, reduced the algorithm execution time by 59\%. Also, the total memory necessary to store the Ringer algorithm information represents less than 6.2 percent of the total filtering system amount.},
	language = {en},
	number = {1},
	urldate = {2017-09-20},
	journal = {Journal of Physics: Conference Series},
	author = {Ciodaro, T. and Deva, D. and Seixas, J. M. de and Damazio, D.},
	year = {2012},
	pages = {012030},
	note = {doi: \href{http://dx.doi.org/10.1088/1742-6596/368/1/012030}{10.1088/1742-6596/368/1/012030}},
	file = {IOP Full Text PDF:/home/johnands/Zotero/storage/LBJSSPQX/Ciodaro et al. - 2012 - Online particle detection with Neural Networks bas.pdf:application/pdf}
}

@article{Helmstaedter13,
	title = {Connectomic reconstruction of the inner plexiform layer in the mouse retina},
	volume = {500},
	copyright = {© 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {https://www.nature.com/nature/journal/v500/n7461/full/nature12346.html},
	doi = {10.1038/nature12346},
	abstract = {Comprehensive high-resolution structural maps are central to functional exploration and understanding in biology. For the nervous system, in which high resolution and large spatial extent are both needed, such maps are scarce as they challenge data acquisition and analysis capabilities. Here we present for the mouse inner plexiform layer—the main computational neuropil region in the mammalian retina—the dense reconstruction of 950 neurons and their mutual contacts. This was achieved by applying a combination of crowd-sourced manual annotation and machine-learning-based volume segmentation to serial block-face electron microscopy data. We characterize a new type of retinal bipolar interneuron and show that we can subdivide a known type based on connectivity. Circuit motifs that emerge from our data indicate a functional mechanism for a known cellular response in a ganglion cell that detects localized motion, and predict that another ganglion cell is motion sensitive.},
	language = {en},
	number = {7461},
	urldate = {2017-09-20},
	journal = {Nature},
	author = {Helmstaedter, Moritz and Briggman, Kevin L. and Turaga, Srinivas C. and Jain, Viren and Seung, H. Sebastian and Denk, Winfried},
	month = aug,
	year = {2013},
	keywords = {Retina},
	pages = {168--174},
	note = {doi: \href{http://dx.doi.org/10.1038/nature12346}{10.1038/nature12346}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/T8A4TTVS/Helmstaedter et al. - 2013 - Connectomic reconstruction of the inner plexiform .pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/9BCBUKMV/nature12346.html:text/html}
}

@article{Carleo17,
	title = {Solving the quantum many-body problem with artificial neural networks},
	volume = {355},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/355/6325/602},
	doi = {10.1126/science.aag2302},
	abstract = {Machine learning and quantum physics
Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox.
Science, this issue p. 602; see also p. 580
The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.
A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem.
A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem.},
	language = {en},
	number = {6325},
	urldate = {2017-09-20},
	journal = {Science},
	author = {Carleo, Giuseppe and Troyer, Matthias},
	month = feb,
	year = {2017},
	pmid = {28183973},
	pages = {602--606},
	note = {doi: \href{http://dx.doi.org/10.1126/science.aag2302}{10.1126/science.aag2302}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/5EM982PW/Carleo and Troyer - 2017 - Solving the quantum many-body problem with artific.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/VVF2LGLZ/602.html:text/html}
}

@article{Krenn16,
	title = {Automated {Search} for new {Quantum} {Experiments}},
	volume = {116},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.116.090405},
	doi = {10.1103/PhysRevLett.116.090405},
	abstract = {Quantum mechanics predicts a number of, at first sight, counterintuitive phenomena. It therefore remains a question whether our intuition is the best way to find new experiments. Here, we report the development of the computer algorithm Melvin which is able to find new experimental implementations for the creation and manipulation of complex quantum states. Indeed, the discovered experiments extensively use unfamiliar and asymmetric techniques which are challenging to understand intuitively. The results range from the first implementation of a high-dimensional Greenberger-Horne-Zeilinger state, to a vast variety of experiments for asymmetrically entangled quantum states—a feature that can only exist when both the number of involved parties and dimensions is larger than 2. Additionally, new types of high-dimensional transformations are found that perform cyclic operations. Melvin autonomously learns from solutions for simpler systems, which significantly speeds up the discovery rate of more complex experiments. The ability to automate the design of a quantum experiment can be applied to many quantum systems and allows the physical realization of quantum states previously thought of only on paper.},
	number = {9},
	urldate = {2017-09-20},
	journal = {Physical Review Letters},
	author = {Krenn, Mario and Malik, Mehul and Fickler, Robert and Lapkiewicz, Radek and Zeilinger, Anton},
	month = mar,
	year = {2016},
	pages = {090405},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevLett.116.090405}{10.1103/PhysRevLett.116.090405}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/QK82A9KE/PhysRevLett.116.html:text/html}
}

@article{Prudente98,
	title = {The fitting of potential energy surfaces using neural networks: {Application} to the study of vibrational levels of {H}3+},
	volume = {109},
	issn = {0021-9606},
	shorttitle = {The fitting of potential energy surfaces using neural networks},
	url = {http://aip.scitation.org/doi/10.1063/1.477550},
	doi = {10.1063/1.477550},
	number = {20},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Prudente, Frederico V. and Acioli, Paulo H. and Neto, J. J. Soares},
	month = nov,
	year = {1998},
	pages = {8801--8808},
	note = {doi: \href{http://dx.doi.org/10.1063/1.477550}{10.1063/1.477550}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/QE2V7RKN/Prudente et al. - 1998 - The fitting of potential energy surfaces using neu.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/YNK7YC5J/1.html:text/html}
}

@article{Natarajan16,
	title = {Neural network molecular dynamics simulations of solid-liquid interfaces: water at low-index copper surfaces},
	volume = {18},
	issn = {1463-9084},
	shorttitle = {Neural network molecular dynamics simulations of solid–liquid interfaces},
	url = {http://pubs.rsc.org/en/content/articlelanding/2016/cp/c6cp05711j},
	doi = {10.1039/C6CP05711J},
	abstract = {Solid–liquid interfaces have received considerable attention in recent years due to their central role in many technologically relevant fields like electrochemistry, heterogeneous catalysis and corrosion. As the chemical processes in these examples take place primarily at the interface, understanding the structural and dynamical properties of the interfacial water molecules is of vital importance. Here, we use a first-principles quality high-dimensional neural network potential built from dispersion-corrected density functional theory data in molecular dynamics simulations to investigate water–copper interfaces as a prototypical case. After performing convergence tests concerning the required supercell size and water film diameter, we investigate numerous properties of the interfacial water molecules at the low-index copper (111), (100) and (110) surfaces. These include density profiles, hydrogen bond properties, lateral mean squared displacements and residence times of the water molecules at the surface. We find that in general the copper–water interaction is rather weak with the strongest interactions observed at the Cu(110) surface, followed by the Cu(100) and Cu(111) surfaces. The distribution of the water molecules in the first hydration layer exhibits a double peak structure. In all cases, the molecules closest to the surface are predominantly allocated on top of the metal sites and are aligned nearly parallel with the oxygen pointing slightly to the surface. The more distant molecules in the first hydration layer at the Cu(111) and Cu(100) surfaces are mainly found in between the top sites, whereas at the Cu(110) surface most of these water molecules are found above the trenches of the close packed atom rows at the surface.},
	language = {en},
	number = {41},
	urldate = {2017-09-20},
	journal = {Physical Chemistry Chemical Physics},
	author = {Natarajan, Suresh Kondati and Behler, J\"{o}rg},
	month = oct,
	year = {2016},
	pages = {28704--28725},
	note = {doi: \href{http://dx.doi.org/10.1039/C6CP05711J}{10.1039/C6CP05711J}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/VJGWGEQT/Natarajan and Behler - 2016 - Neural network molecular dynamics simulations of s.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/UX7HZW3Q/C6CP05711J.html:text/html}
}

@article{Artrith12,
	title = {High-dimensional neural network potentials for metal surfaces: {A} prototype study for copper},
	volume = {85},
	shorttitle = {High-dimensional neural network potentials for metal surfaces},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.85.045439},
	doi = {10.1103/PhysRevB.85.045439},
	abstract = {The atomic environments at metal surfaces differ strongly from the bulk, and, in particular, in case of reconstructions or imperfections at “real surfaces,” very complicated atomic configurations can be present. This structural complexity poses a significant challenge for the development of accurate interatomic potentials suitable for large-scale molecular dynamics simulations. In recent years, artificial neural networks (NN) have become a promising new method for the construction of potential-energy surfaces for difficult systems. In the present work, we explore the applicability of such high-dimensional NN potentials to metal surfaces using copper as a benchmark system. A detailed analysis of the properties of bulk copper and of a wide range of surface structures shows that NN potentials can provide results of almost density functional theory (DFT) quality at a small fraction of the computational costs.},
	number = {4},
	urldate = {2017-09-20},
	journal = {Physical Review B},
	author = {Artrith, Nongnuch and Behler, Jörg},
	month = jan,
	year = {2012},
	pages = {045439},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevB.85.045439}{10.1103/PhysRevB.85.045439}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/WGNZXQR3/PhysRevB.85.html:text/html}
}

@article{Hopcroft10,
	title = {What is the {Young}'s {Modulus} of {Silicon}?},
	volume = {19},
	issn = {1057-7157},
	doi = {10.1109/JMEMS.2009.2039697},
	abstract = {The Young's modulus (E) of a material is a key parameter for mechanical engineering design. Silicon, the most common single material used in microelectromechanical systems (MEMS), is an anisotropic crystalline material whose material properties depend on orientation relative to the crystal lattice. This fact means that the correct value of E for analyzing two different designs in silicon may differ by up to 45\%. However, perhaps, because of the perceived complexity of the subject, many researchers oversimplify silicon elastic behavior and use inaccurate values for design and analysis. This paper presents the best known elasticity data for silicon, both in depth and in a summary form, so that it may be readily accessible to MEMS designers.},
	number = {2},
	journal = {Journal of Microelectromechanical Systems},
	author = {Hopcroft, M. A. and Nix, W. D. and Kenny, T. W.},
	month = apr,
	year = {2010},
	note = {doi: \href{http://dx.doi.org/10.1109/JMEMS.2009.2039697}{10.1109/JMEMS.2009.2039697}},
	keywords = {anisotropic crystalline material, crystal orientation, Elastic modulus, elasticity, elemental semiconductors, MEMS, microelectromechanical systems, microelectromechanical systems (MEMS) design, micromechanical devices, Poisson's ratio, shear modulus, Si, silicon, Young's modulus},
	pages = {229--238},
	file = {IEEE Xplore Abstract Record:/home/johnands/Zotero/storage/SRHV2SQZ/5430873.html:text/html;IEEE Xplore Full Text PDF:/home/johnands/Zotero/storage/YV6MGXNA/Hopcroft et al. - 2010 - What is the Young's Modulus of Silicon.pdf:application/pdf}
}

@article{Cowley88,
	title = {Lattice {Dynamics} of {Silicon} with {Empirical} {Many}-{Body} {Potentials}},
	volume = {60},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.60.2379},
	doi = {10.1103/PhysRevLett.60.2379},
	abstract = {Several recently proposed models for the interatomic potential in silicon are used to generate the interatomic force constants. These are then used to calculate the elastic constants and selected normal-mode frequencies. A comparison of the calculated and experimental values provides a test of the models. None of the models tested is completely satisfactory.},
	number = {23},
	urldate = {2017-09-20},
	journal = {Physical Review Letters},
	author = {Cowley, E. Roger},
	month = jun,
	year = {1988},
	pages = {2379--2381},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevLett.60.2379}{10.1103/PhysRevLett.60.2379}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/449RIB24/PhysRevLett.60.html:text/html}
}

@article{Sherrill10,
	title = {Frontiers in electronic structure theory},
	volume = {132},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/10.1063/1.3369628},
	doi = {10.1063/1.3369628},
	abstract = {Current and emerging research areas in electronic structure theory promise to greatly extend the scope and quality of quantum chemical computations. Two particularly challenging problems are the accurate description of electronic near-degeneracies (as occur in bond-breaking reactions, first-row transition elements, etc.) and the description of long-range dispersion interactions in density functional theory. Additionally, even with the emergence of reduced-scaling electronic structure methods and basis set extrapolation techniques, quantum chemical computations remain very time-consuming for large molecules or large basis sets. A variety of techniques, including density fitting and explicit correlation methods, are making rapid progress toward solving these challenges.},
	number = {11},
	urldate = {2017-09-20},
	journal = {The Journal of Chemical Physics},
	author = {Sherrill, C. David},
	month = mar,
	year = {2010},
	pages = {110902},
	note = {doi: \href{http://dx.doi.org/10.1063/1.3369628}{10.1063/1.3369628}},
	file = {Full Text PDF:/home/johnands/Zotero/storage/FJYRFQFB/Sherrill - 2010 - Frontiers in electronic structure theory.pdf:application/pdf;Snapshot:/home/johnands/Zotero/storage/AKH4U4MA/1.html:text/html}
}

@article{Eshet12,
	title = {\textit{Ab initio} quality neural-network potential for sodium},
	volume = {81},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.81.184107},
	doi = {10.1103/PhysRevB.81.184107},
	abstract = {An interatomic potential for high-pressure high-temperature (HPHT) crystalline and liquid phases of sodium is created using a neural-network (NN) representation of the ab initio potential-energy surface. It is demonstrated that the NN potential provides an ab initio quality description of multiple properties of liquid sodium and bcc, fcc, and cI16 crystal phases in the P−T region up to 120 GPa and 1200 K. The unique combination of computational efficiency of the NN potential and its ability to reproduce quantitatively experimental properties of sodium in the wide P−T range enables molecular-dynamics simulations of physicochemical processes in HPHT sodium of unprecedented quality.},
	number = {18},
	urldate = {2017-09-20},
	journal = {Physical Review B},
	author = {Eshet, Hagai and Khaliullin, Rustam Z. and K\"{u}hne, Thomas D. and Behler, Jörg and Parrinello, Michele},
	month = may,
	year = {2010},
	pages = {184107},
	note = {doi: \href{http://dx.doi.org/10.1103/PhysRevB.81.184107}{10.1103/PhysRevB.81.184107}},
	file = {APS Snapshot:/home/johnands/Zotero/storage/NLJZDS4K/PhysRevB.81.html:text/html}
}
