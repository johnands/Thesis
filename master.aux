\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Krizhevsky12}
\citation{Hinton12}
\citation{Collobert11}
\citation{Ciodaro12}
\citation{Helmstaedter13}
\citation{Carleo17}
\citation{Krenn16}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Potentials in molecular dynamics}{1}{section.1.1}}
\citation{Jones24}
\citation{Stillinger85}
\citation{Dawes08}
\citation{Ischtwan94}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Neural Network potentials}{2}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Goals}{2}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Structure of the thesis}{3}{section.1.4}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Theory}{5}{part.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Molecular dynamics}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{forcePES}{{2.1}{9}{Molecular dynamics}{equation.2.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Potential energy surfaces}{9}{section.2.1}}
\newlabel{sec:potentialEnergySurfaces}{{2.1}{9}{Potential energy surfaces}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}From quantum mechanics to classical potentials}{10}{subsection.2.1.1}}
\newlabel{sec:bornOppenheimer}{{2.1.1}{10}{From quantum mechanics to classical potentials}{subsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Constructing potential energy surfaces}{10}{subsection.2.1.2}}
\newlabel{sec:constructingPES}{{2.1.2}{10}{Constructing potential energy surfaces}{subsection.2.1.2}{}}
\newlabel{generalPotential}{{2.2}{10}{Constructing potential energy surfaces}{equation.2.1.2}{}}
\citation{Dawes08}
\citation{Ischtwan94}
\@writefile{toc}{\contentsline {subsubsection}{Truncation and configuration space}{11}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Fitting procedure}{11}{section*.3}}
\citation{Jones24}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Common empirical potentials}{12}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Lennard-Jones}{12}{subsection.2.2.1}}
\newlabel{Lennard-Jones}{{2.4}{12}{Lennard-Jones}{equation.2.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Lennard-Jones potential \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {Lennard-Jones}\unskip \@@italiccorr )}} as a function of inter-atomic distance for two different parameter sets. The depth of the potential well (potential strength) is given by $\epsilon $, while $\sigma $ is the inter-atomic distance for which the potential is zero.\relax }}{13}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:LJ}{{2.1}{13}{The Lennard-Jones potential \eqref {Lennard-Jones} as a function of inter-atomic distance for two different parameter sets. The depth of the potential well (potential strength) is given by $\epsilon $, while $\sigma $ is the inter-atomic distance for which the potential is zero.\relax }{figure.caption.4}{}}
\citation{Stillinger85}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \relax }}{14}{figure.caption.6}}
\newlabel{fig:triplets}{{2.2}{14}{\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Calculating total potential energy}{14}{section*.5}}
\newlabel{twoBodyPotentialEnergy}{{2.5}{14}{Calculating total potential energy}{equation.2.2.5}{}}
\newlabel{threeBodyPotentialEnergy1}{{2.7}{14}{Calculating total potential energy}{equation.2.2.7}{}}
\newlabel{threeBodyPotentialEnergy2}{{2.8}{14}{Calculating total potential energy}{equation.2.2.8}{}}
\newlabel{totalPotentialEnergy}{{2.9}{14}{Calculating total potential energy}{equation.2.2.9}{}}
\citation{Stillinger85}
\citation{Molinero08}
\citation{Vashishta90}
\citation{Vashishta07}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Stillinger-Weber}{15}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Vashishta}{15}{subsection.2.2.3}}
\citation{Frenkel01}
\citation{Frenkel01}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Time integration}{16}{section.2.3}}
\newlabel{equationsOfMotion1}{{2.14}{16}{Time integration}{equation.2.3.14}{}}
\newlabel{equationsOfMotion2}{{2.15}{16}{Time integration}{equation.2.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Force calculations and cutoff radius}{17}{section.2.4}}
\newlabel{sec:forceCutoff}{{2.4}{17}{Force calculations and cutoff radius}{section.2.4}{}}
\newlabel{forceCutoff}{{2.21}{17}{Force calculations and cutoff radius}{equation.2.4.21}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine learning}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:machineLearning}{{3}{19}{Machine learning}{chapter.3}{}}
\citation{McCulloch43}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Machine learning, taken from from http://www.isaziconsulting.co.za/machinelearning.html.\relax }}{20}{figure.caption.7}}
\newlabel{fig:machineLearningDiagram}{{3.1}{20}{Machine learning, taken from from http://www.isaziconsulting.co.za/machinelearning.html.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Artificial neurons}{20}{section.3.1}}
\newlabel{sec:ANN}{{3.1}{20}{Artificial neurons}{section.3.1}{}}
\citation{Rojas96}
\citation{Kriesel07}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots  ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }}{21}{figure.caption.8}}
\newlabel{fig:neuronModel}{{3.2}{21}{Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }{figure.caption.8}{}}
\newlabel{artificialNeuron}{{3.1}{21}{Artificial neurons}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Biological model of a neuron. Source: \href  {https://askabiologist.asu.edu/neuron-anatomy}{https://askabiologist.asu.edu/neuron-anatomy}\relax }}{22}{figure.caption.10}}
\newlabel{fig:neuronBiological}{{3.3}{22}{Biological model of a neuron. Source: \href {https://askabiologist.asu.edu/neuron-anatomy}{https://askabiologist.asu.edu/neuron-anatomy}\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural network types}{22}{section.3.2}}
\citation{LeCun99}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit  {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }}{23}{figure.caption.11}}
\newlabel{fig:networkGeneral}{{3.4}{23}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Feed-forward neural networks}{23}{subsection.3.2.1}}
\citation{Rosenblatt58}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Recurrent neural networks}{24}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Other types of networks}{24}{subsection.3.2.3}}
\citation{Hornik89}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multilayer perceptron}{25}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Why multilayer perceptrons?}{25}{subsection.3.3.1}}
\newlabel{sec:whyMLP}{{3.3.1}{25}{Why multilayer perceptrons?}{subsection.3.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are listed.\relax }}{26}{figure.caption.12}}
\newlabel{fig:networkNotation}{{3.5}{26}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are listed.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Mathematical model}{26}{subsection.3.3.2}}
\newlabel{sec:MLPmodel}{{3.3.2}{26}{Mathematical model}{subsection.3.3.2}{}}
\newlabel{artificialNeuron2}{{3.2}{26}{Mathematical model}{equation.3.3.2}{}}
\newlabel{outputLayer1}{{3.4}{27}{Mathematical model}{equation.3.3.4}{}}
\newlabel{generalLayer}{{3.5}{27}{Mathematical model}{equation.3.3.5}{}}
\newlabel{outputLayer2}{{3.7}{27}{Mathematical model}{equation.3.3.7}{}}
\newlabel{completeNN}{{3.10}{28}{Mathematical model}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Activation function of output neuron}{28}{section*.14}}
\newlabel{outputActivation}{{3.12}{28}{Activation function of output neuron}{equation.3.3.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix representation}{28}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively.\relax }}{29}{figure.caption.13}}
\newlabel{fig:activationsFlex}{{3.6}{29}{The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively.\relax }{figure.caption.13}{}}
\citation{Hornik89}
\citation{Rojas96}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Activation functions}{30}{section.3.4}}
\newlabel{sec:activationFunctions}{{3.4}{30}{Activation functions}{section.3.4}{}}
\citation{Karlik11}
\citation{LeCun15}
\citation{Glorot11}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }}{31}{figure.caption.16}}
\newlabel{fig:SigmoidActivationFunctions}{{3.7}{31}{Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }{figure.caption.16}{}}
\newlabel{sigmoidActivationFunction}{{3.15}{31}{Activation functions}{equation.3.4.15}{}}
\newlabel{tanhActivationFunction}{{3.16}{31}{Activation functions}{equation.3.4.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }}{32}{figure.caption.17}}
\newlabel{fig:reluActivationFunction}{{3.8}{32}{Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training}{32}{section.3.5}}
\newlabel{sec:training}{{3.5}{32}{Training}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Cost functions}{33}{subsection.3.5.1}}
\newlabel{costFunctions}{{3.5.1}{33}{Cost functions}{subsection.3.5.1}{}}
\newlabel{generalCost}{{3.18}{33}{Cost functions}{equation.3.5.18}{}}
\newlabel{quadraticCost}{{3.19}{33}{Cost functions}{equation.3.5.19}{}}
\citation{Ruder16}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Optimization}{34}{section.3.6}}
\newlabel{sec:optimization}{{3.6}{34}{Optimization}{section.3.6}{}}
\newlabel{gradientDescent}{{3.20}{34}{Optimization}{equation.3.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Gradient descent variants}{34}{subsection.3.6.1}}
\newlabel{gradientDescentVariants}{{3.6.1}{34}{Gradient descent variants}{subsection.3.6.1}{}}
\citation{Qian99}
\citation{Duchi11}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Optimization algorithms}{35}{subsection.3.6.2}}
\newlabel{sec:optimizationAlgorithms}{{3.6.2}{35}{Optimization algorithms}{subsection.3.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{35}{section*.18}}
\newlabel{Momentum}{{3.21}{35}{Momentum}{equation.3.6.21}{}}
\citation{Zeiler12}
\@writefile{toc}{\contentsline {subsubsection}{Adagrad}{36}{section*.19}}
\newlabel{Adagrad}{{3.24}{36}{Adagrad}{equation.3.6.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adadelta}{36}{section*.20}}
\newlabel{decayingAverageVector}{{3.25}{36}{Adadelta}{equation.3.6.25}{}}
\citation{Kingma14}
\citation{Kingma14}
\newlabel{preliminiaryAdadelta}{{3.27}{37}{Adadelta}{equation.3.6.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{37}{section*.21}}
\citation{Kingma14}
\citation{Rumelhart86}
\@writefile{toc}{\contentsline {subsubsection}{Which optimizer to use?}{38}{section*.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Backpropagation}{38}{subsection.3.6.3}}
\newlabel{sec:backprop}{{3.6.3}{38}{Backpropagation}{subsection.3.6.3}{}}
\newlabel{partialDerivatives}{{3.34}{38}{Backpropagation}{equation.3.6.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{1. Forward propagation}{39}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{2. Backward propagation}{39}{section*.24}}
\newlabel{partialDerivativeExpanded}{{3.37}{39}{2. Backward propagation}{equation.3.6.37}{}}
\newlabel{neuronError}{{3.38}{39}{2. Backward propagation}{equation.3.6.38}{}}
\newlabel{derivativeSecondTerm}{{3.40}{40}{2. Backward propagation}{equation.3.6.40}{}}
\newlabel{weightGradient}{{3.41}{40}{2. Backward propagation}{equation.3.6.41}{}}
\newlabel{forwardProp}{{3.42}{40}{2. Backward propagation}{equation.3.6.42}{}}
\newlabel{errorTerms}{{3.45}{40}{2. Backward propagation}{equation.3.6.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref  {fig:neuronModel}. A hidden neuron $j$ recieves a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t to its net input $u_j$.\relax }}{41}{figure.caption.25}}
\newlabel{fig:backprop}{{3.9}{41}{Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref {fig:neuronModel}. A hidden neuron $j$ recieves a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t to its net input $u_j$.\relax }{figure.caption.25}{}}
\newlabel{backprop}{{3.48}{41}{2. Backward propagation}{equation.3.6.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix notation}{42}{section*.26}}
\@writefile{toc}{\contentsline {subsubsection}{Training algorithm}{42}{section*.27}}
\newlabel{forwardPropMatrix}{{3.58}{43}{Training algorithm}{equation.3.6.58}{}}
\newlabel{backPropMatrix}{{3.60}{43}{Training algorithm}{equation.3.6.60}{}}
\newlabel{weightUpdate}{{3.61}{43}{Training algorithm}{equation.3.6.61}{}}
\citation{Behler11general}
\citation{Agrawal06}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural networks in molecular dynamics}{45}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:NNsAndMD}{{4}{45}{Neural networks in molecular dynamics}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural network potentials}{45}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Potentials using single neural network}{45}{subsection.4.1.1}}
\citation{Bholoa07}
\citation{Behler07}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Potentials using multiple neural networks}{46}{subsection.4.1.2}}
\newlabel{systemEnergy}{{4.1}{46}{Potentials using multiple neural networks}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Behler-Parinello method}{46}{section.4.2}}
\newlabel{BPatomicEnergySi}{{4.2}{47}{The Behler-Parinello method}{equation.4.2.2}{}}
\newlabel{BPatomicEnergy}{{4.3}{47}{The Behler-Parinello method}{equation.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Symmetry functions}{47}{subsection.4.2.1}}
\newlabel{sec:symmetryFunctions}{{4.2.1}{47}{Symmetry functions}{subsection.4.2.1}{}}
\citation{Behler11symmetry}
\citation{cutoffFunction}
\citation{Behler11symmetry}
\newlabel{cutoffFunction}{{4.4}{48}{Symmetry functions}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Radial symmetry functions}{48}{section*.29}}
\newlabel{G1}{{4.5}{48}{Radial symmetry functions}{equation.4.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plot of the cutoff function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {cutoffFunction}\unskip \@@italiccorr )}} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }}{49}{figure.caption.28}}
\newlabel{fig:cutoffFunction}{{4.1}{49}{Plot of the cutoff function \eqref {cutoffFunction} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }{figure.caption.28}{}}
\newlabel{G2}{{4.6}{49}{Radial symmetry functions}{equation.4.2.6}{}}
\newlabel{G3}{{4.7}{50}{Radial symmetry functions}{equation.4.2.7}{}}
\newlabel{fig:radialSymmetriFunctions:a}{{\caption@xref {fig:radialSymmetriFunctions:a}{ on input line 1563}}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{sub@fig:radialSymmetriFunctions:a}{{}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{fig:radialSymmetriFunctions:b}{{\caption@xref {fig:radialSymmetriFunctions:b}{ on input line 1569}}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{sub@fig:radialSymmetriFunctions:b}{{}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{fig:radialSymmetriFunctions:c}{{\caption@xref {fig:radialSymmetriFunctions:c}{ on input line 1575}}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{sub@fig:radialSymmetriFunctions:c}{{}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{fig:radialSymmetriFunctions:d}{{\caption@xref {fig:radialSymmetriFunctions:d}{ on input line 1580}}{50}{Radial symmetry functions}{figure.caption.30}{}}
\newlabel{sub@fig:radialSymmetriFunctions:d}{{}{50}{Radial symmetry functions}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }}{50}{figure.caption.30}}
\newlabel{fig:radialSymmetriFunctions}{{4.2}{50}{Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Angular symmetry functions}{51}{section*.31}}
\newlabel{G4}{{4.8}{51}{Angular symmetry functions}{equation.4.2.8}{}}
\newlabel{G5}{{4.9}{51}{Angular symmetry functions}{equation.4.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Setting the symmetry parameters}{51}{section*.33}}
\citation{Behler11symmetry}
\newlabel{fig:angularSymmetryFunctions:a}{{4.3a}{52}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:angularSymmetryFunctions:a}{{a}{52}{\relax }{figure.caption.32}{}}
\newlabel{fig:angularSymmetryFunctions:b}{{4.3b}{52}{\relax }{figure.caption.32}{}}
\newlabel{sub@fig:angularSymmetryFunctions:b}{{b}{52}{\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref  {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref  {fig:angularSymmetryFunctions:b}.\relax }}{52}{figure.caption.32}}
\newlabel{fig:angularSymmetryFunctions}{{4.3}{52}{Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref {fig:angularSymmetryFunctions:b}.\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Symmetry functions and forces}{52}{subsection.4.2.2}}
\newlabel{sec:symmAndForces}{{4.2.2}{52}{Symmetry functions and forces}{subsection.4.2.2}{}}
\newlabel{forcePES2}{{4.10}{52}{Symmetry functions and forces}{equation.4.2.10}{}}
\newlabel{forceAtomk}{{4.11}{52}{Symmetry functions and forces}{equation.4.2.11}{}}
\newlabel{forceAtomkChainRule}{{4.12}{53}{Symmetry functions and forces}{equation.4.2.12}{}}
\newlabel{BPatomicForce}{{4.13}{53}{Symmetry functions and forces}{equation.4.2.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Change of coordinates}{54}{section*.34}}
\newlabel{changeOfCoordinates}{{4.14}{54}{Change of coordinates}{equation.4.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Example system of three atoms of the same type. Only atom 2 is inside the cutoff sphere of atom 1, thus we only need to take the energy of atom 1 and 2 into account to find the force on atom 1. Since the atoms are of the same type, they have identical atomic NNs and symmetry function sets.\relax }}{54}{figure.caption.35}}
\newlabel{fig:threeAtoms}{{4.4}{54}{Example system of three atoms of the same type. Only atom 2 is inside the cutoff sphere of atom 1, thus we only need to take the energy of atom 1 and 2 into account to find the force on atom 1. Since the atoms are of the same type, they have identical atomic NNs and symmetry function sets.\relax }{figure.caption.35}{}}
\newlabel{forceOnAtom1}{{4.15}{54}{Change of coordinates}{equation.4.2.15}{}}
\newlabel{energyAtom1}{{4.16}{55}{Change of coordinates}{equation.4.2.16}{}}
\newlabel{energyAtom2}{{4.17}{55}{Change of coordinates}{equation.4.2.17}{}}
\newlabel{changeOfCoordsSymmetry1}{{4.18}{55}{Change of coordinates}{equation.4.2.18}{}}
\newlabel{changeOfCoordsSymmetry2}{{4.19}{55}{Change of coordinates}{equation.4.2.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Summary}{56}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Implementation and results}{59}{part.2}}
\citation{Plimpton95}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}LAMMPS}{61}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:lammps}{{5}{61}{LAMMPS}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Installing LAMMPS}{61}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}LAMMPS input script}{62}{section.5.2}}
\citation{Tuckerman92}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}LAMMPS structure}{66}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Extending LAMMPS}{66}{section.5.4}}
\newlabel{sec:extendingLammps}{{5.4}{66}{Extending LAMMPS}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Class hierarchy of LAMMPS. The inheritance is from left to right. The top-level classes (blue) are visible everywhere in the code. Only a selected set of sub-classes (green and yellow) is displayed, where we recognize many of the input script commands.\relax }}{67}{figure.caption.36}}
\newlabel{fig:lammpsClasses}{{5.1}{67}{Class hierarchy of LAMMPS. The inheritance is from left to right. The top-level classes (blue) are visible everywhere in the code. Only a selected set of sub-classes (green and yellow) is displayed, where we recognize many of the input script commands.\relax }{figure.caption.36}{}}
\zref@newlabel{mdf@pagelabel-1}{\default{5.4}\page{69}\abspage{79}\mdf@pagevalue{69}}
\zref@newlabel{mdf@pagelabel-2}{\default{5.4}\page{69}\abspage{79}\mdf@pagevalue{69}}
\zref@newlabel{mdf@pagelabel-3}{\default{5.4}\page{70}\abspage{80}\mdf@pagevalue{70}}
\zref@newlabel{mdf@pagelabel-4}{\default{5.4}\page{70}\abspage{80}\mdf@pagevalue{70}}
\newlabel{forwardPropMatrixLammpsChapter}{{5.1}{71}{Extending LAMMPS}{equation.5.4.1}{}}
\zref@newlabel{mdf@pagelabel-5}{\default{5.4}\page{71}\abspage{81}\mdf@pagevalue{71}}
\newlabel{backPropMatrixLammpsChapter}{{5.2}{71}{Extending LAMMPS}{equation.5.4.2}{}}
\zref@newlabel{mdf@pagelabel-6}{\default{5.4}\page{71}\abspage{81}\mdf@pagevalue{71}}
\newlabel{forceAtomkChainRuleLammpsChapter}{{5.3}{71}{Extending LAMMPS}{equation.5.4.3}{}}
\zref@newlabel{mdf@pagelabel-7}{\default{5.4}\page{72}\abspage{82}\mdf@pagevalue{72}}
\citation{Abadi15}
\citation{Abadi16}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}TensorFlow}{75}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{quadraticEquationSolution}{{6.2}{75}{TensorFlow}{equation.6.0.2}{}}
\newlabel{quadraticEquation}{{6.3}{75}{TensorFlow}{equation.6.0.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Data flow graph for the solutions \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticEquationSolution}\unskip \@@italiccorr )}} of a quadratic equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticEquation}\unskip \@@italiccorr )}}. The nodes (green circles) are mathematical operations connected by edges (arrows) that show the data flow direction. Inputs and outputs are marked as rectangular boxes. Reproduced from: \href  {http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}{http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}\relax }}{76}{figure.caption.37}}
\newlabel{fig:DFG}{{6.1}{76}{Data flow graph for the solutions \eqref {quadraticEquationSolution} of a quadratic equation \eqref {quadraticEquation}. The nodes (green circles) are mathematical operations connected by edges (arrows) that show the data flow direction. Inputs and outputs are marked as rectangular boxes. Reproduced from: \href {http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}{http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Installing TensorFlow}{76}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}TensorFlow basic usage}{77}{section.6.2}}
\newlabel{sec:TensorFlowBasic}{{6.2}{77}{TensorFlow basic usage}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Hello world}{77}{subsection.6.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Creating a neural network}{78}{subsection.6.2.2}}
\newlabel{sec:TensorFlowCreatingNN}{{6.2.2}{78}{Creating a neural network}{subsection.6.2.2}{}}
\newlabel{forwardPropMatrix}{{6.4}{78}{Creating a neural network}{equation.6.2.4}{}}
\zref@newlabel{mdf@pagelabel-8}{\default{6.2.2}\page{78}\abspage{88}\mdf@pagevalue{78}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Visualizing the graph}{79}{subsection.6.2.3}}
\zref@newlabel{mdf@pagelabel-9}{\default{6.2.3}\page{79}\abspage{89}\mdf@pagevalue{79}}
\zref@newlabel{mdf@pagelabel-10}{\default{6.2.3}\page{80}\abspage{90}\mdf@pagevalue{80}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }}{81}{figure.caption.38}}
\newlabel{fig:graphExample}{{6.2}{81}{Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Training a NN with TensorFlow}{81}{subsection.6.2.4}}
\zref@newlabel{mdf@pagelabel-11}{\default{6.2.4}\page{81}\abspage{91}\mdf@pagevalue{81}}
\zref@newlabel{mdf@pagelabel-12}{\default{6.2.4}\page{81}\abspage{91}\mdf@pagevalue{81}}
\citation{Qian99}
\citation{Duchi11}
\citation{Zeiler12}
\citation{Kingma14}
\zref@newlabel{mdf@pagelabel-13}{\default{6.2.4}\page{82}\abspage{92}\mdf@pagevalue{82}}
\zref@newlabel{mdf@pagelabel-14}{\default{6.2.4}\page{82}\abspage{92}\mdf@pagevalue{82}}
\zref@newlabel{mdf@pagelabel-15}{\default{6.2.4}\page{82}\abspage{92}\mdf@pagevalue{82}}
\zref@newlabel{mdf@pagelabel-16}{\default{6.2.4}\page{83}\abspage{93}\mdf@pagevalue{83}}
\zref@newlabel{mdf@pagelabel-17}{\default{6.2.4}\page{83}\abspage{93}\mdf@pagevalue{83}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt  {gradients} computes the gradient of all nodes in the graph, \texttt  {trainStep} implements the algorithm for updating the NN parameters, while \texttt  {beta1\_power} and \texttt  {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }}{84}{figure.caption.39}}
\newlabel{fig:tensorBoardTraining}{{6.3}{84}{Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt {gradients} computes the gradient of all nodes in the graph, \texttt {trainStep} implements the algorithm for updating the NN parameters, while \texttt {beta1\_power} and \texttt {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Constructing a neural network potential}{85}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:constructingNNP}{{7}{85}{Constructing a neural network potential}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Selecting the reference set}{85}{section.7.1}}
\newlabel{sec:selectingTrainingData}{{7.1}{85}{Selecting the reference set}{section.7.1}{}}
\citation{Behler11general}
\citation{Raff12}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Iterative molecular dynamics sampling}{86}{subsection.7.1.1}}
\newlabel{sec:iterativeMDsampling}{{7.1.1}{86}{Iterative molecular dynamics sampling}{subsection.7.1.1}{}}
\newlabel{sample}{{1}{86}{Iterative molecular dynamics sampling}{Item.30}{}}
\newlabel{refEnergies}{{2}{86}{Iterative molecular dynamics sampling}{Item.31}{}}
\newlabel{NNPalgorithm}{{7}{86}{Iterative molecular dynamics sampling}{Item.36}{}}
\citation{Ischtwan94}
\citation{Raff05}
\citation{Raff05}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Sampling algorithms}{87}{subsection.7.1.2}}
\citation{Raff12}
\citation{Pukrittayakamee09}
\@writefile{toc}{\contentsline {subsubsection}{Initial sampling}{88}{section*.40}}
\newlabel{variableIntervalSampling}{{7.1}{88}{Initial sampling}{equation.7.1.1}{}}
\citation{Pukrittayakamee09}
\zref@newlabel{mdf@pagelabel-18}{\default{\caption@xref {??}{ on input line 2980}}\page{89}\abspage{99}\mdf@pagevalue{89}}
\newlabel{samplingAlgorithmModified}{{7.2}{89}{Initial sampling}{equation.7.1.2}{}}
\zref@newlabel{mdf@pagelabel-19}{\default{\caption@xref {??}{ on input line 3048}}\page{90}\abspage{100}\mdf@pagevalue{90}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Distribution of forces for one Si atom in a quartz $\textrm  {SiO}_2$ crystal with and withouth the use of the sampling algorithm \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {samplingAlgorithmModified}\unskip \@@italiccorr )}}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm  {max} = 10$.\relax }}{91}{figure.caption.41}}
\newlabel{fig:forceDistSamplingAlgo}{{7.1}{91}{Distribution of forces for one Si atom in a quartz $\textrm {SiO}_2$ crystal with and withouth the use of the sampling algorithm \eqref {samplingAlgorithmModified}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm {max} = 10$.\relax }{figure.caption.41}{}}
\citation{Raff05}
\citation{Behler11general}
\@writefile{toc}{\contentsline {subsubsection}{Iterative sampling}{92}{section*.42}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{93}{section*.43}}
\citation{Behler11symmetry}
\citation{Behler15}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Constructing the symmetry function sets}{94}{section.7.2}}
\newlabel{sec:constructingSymmetry}{{7.2}{94}{Constructing the symmetry function sets}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Initial set}{94}{subsection.7.2.1}}
\newlabel{sec:initialSymmSet}{{7.2.1}{94}{Initial set}{subsection.7.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Example of an initial radial symmetry function set. The parameters $\eta $ and $R_s$ are varied to probe the radial arrangement of neighbouring atoms. All symmetry functions have the same cutoff $R_c = 6.0$. $R_{ij}$, $R_s$ and $R_c$ are in units of $\SI {}{\angstrom }$, while $\eta $ have units $\SI {}{\angstrom }^{-2}$.\relax }}{95}{figure.caption.44}}
\newlabel{fig:radialParams}{{7.2}{95}{Example of an initial radial symmetry function set. The parameters $\eta $ and $R_s$ are varied to probe the radial arrangement of neighbouring atoms. All symmetry functions have the same cutoff $R_c = 6.0$. $R_{ij}$, $R_s$ and $R_c$ are in units of $\SI {}{\angstrom }$, while $\eta $ have units $\SI {}{\angstrom }^{-2}$.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Angular symmetry functions}{95}{section*.45}}
\@writefile{toc}{\contentsline {subsubsection}{Number of symmetry functions}{95}{section*.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Example of an initial angular symmetry function set with $\eta = \SI {0.01}{\angstrom }^{-2}$ and $\lambda = 1$. A corresponding set of functions with $\lambda = -1$ should also be added to make the set more complete. The $\zeta $ parameter is increased in a non-linear way to avoid excessively overlapping functions. \relax }}{96}{figure.caption.46}}
\newlabel{fig:angularParams}{{7.3}{96}{Example of an initial angular symmetry function set with $\eta = \SI {0.01}{\angstrom }^{-2}$ and $\lambda = 1$. A corresponding set of functions with $\lambda = -1$ should also be added to make the set more complete. The $\zeta $ parameter is increased in a non-linear way to avoid excessively overlapping functions. \relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Adjusting the set}{97}{subsection.7.2.2}}
\newlabel{sec:adjustSymmSet}{{7.2.2}{97}{Adjusting the set}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{For each symmetry function, the range of function values should be as large as possible}{97}{section*.48}}
\@writefile{toc}{\contentsline {subsubsection}{The set of values of two different symmetry functions on a given data set should not be strongly correlated}{97}{section*.49}}
\zref@newlabel{mdf@pagelabel-20}{\default{7.2.2}\page{97}\abspage{107}\mdf@pagevalue{97}}
\citation{Bengio12}
\citation{Behler15}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Setting hyperparameters}{98}{section.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Preconditioning the input data}{98}{subsection.7.3.1}}
\newlabel{sec:transformInputData}{{7.3.1}{98}{Preconditioning the input data}{subsection.7.3.1}{}}
\newlabel{scalingInputData}{{7.5}{98}{Preconditioning the input data}{equation.7.3.5}{}}
\citation{LeCun12}
\citation{LeCun12}
\citation{LeCun99}
\newlabel{shiftInputData}{{7.6}{99}{Preconditioning the input data}{equation.7.3.6}{}}
\newlabel{scaleVariance}{{7.7}{99}{Preconditioning the input data}{equation.7.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Convergence speed vs evaluation speed}{99}{section*.50}}
\citation{Karlik11}
\citation{Glorot10}
\citation{Bengio12}
\citation{LeCun12}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Activation functions and weight initialization}{100}{subsection.7.3.2}}
\newlabel{sec:hyperParamsActFunctions}{{7.3.2}{100}{Activation functions and weight initialization}{subsection.7.3.2}{}}
\citation{Glorot10}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Network architecture and overfitting}{101}{subsection.7.3.3}}
\newlabel{sec:overfitting}{{7.3.3}{101}{Network architecture and overfitting}{subsection.7.3.3}{}}
\citation{Larochelle09}
\citation{Behler07}
\citation{Raff05}
\citation{Witkoskie05}
\citation{Bengio07}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Demonstration of overfitting. Both figures show the RMSE of a NN evaluated on both the training set and the test set. The training set consists of 5578 training examples from a Si Stillinger-Weber simulation. In a), the NN has been trained on the whole training set, while in b) only the first 300 training examples were used. The test set is identical in both cases. We clearly see that overfitting occurs in b).\relax }}{102}{figure.caption.51}}
\newlabel{fig:overfitting}{{7.4}{102}{Demonstration of overfitting. Both figures show the RMSE of a NN evaluated on both the training set and the test set. The training set consists of 5578 training examples from a Si Stillinger-Weber simulation. In a), the NN has been trained on the whole training set, while in b) only the first 300 training examples were used. The test set is identical in both cases. We clearly see that overfitting occurs in b).\relax }{figure.caption.51}{}}
\citation{Krogh1992}
\citation{Srivastava14}
\citation{Bengio12}
\citation{LeCun99}
\@writefile{toc}{\contentsline {subsubsection}{Other regularization teqhniques}{103}{section*.52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Optimizer parameters}{103}{subsection.7.3.4}}
\citation{Bengio12}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.5}Hyperparameter space exploration}{104}{subsection.7.3.5}}
\newlabel{sec:optimizingHyperparameters}{{7.3.5}{104}{Hyperparameter space exploration}{subsection.7.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Coordinate descent}{104}{section*.53}}
\@writefile{toc}{\contentsline {subsubsection}{Grid search}{105}{section*.54}}
\@writefile{toc}{\contentsline {subsubsection}{Random search}{105}{section*.55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.6}Summary}{105}{subsection.7.3.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Applying the neural network potential}{105}{section.7.4}}
\newlabel{sec:applyingNNP}{{7.4}{105}{Applying the neural network potential}{section.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{\mathrm  {TFP}}$), the TF C++ API ($T_{\mathrm  {TFC}}$) and Armadillo ($T_{\mathrm  {ARMA}}$). $L$ is the number of hidden layers, $n$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture.\relax }}{107}{figure.caption.56}}
\newlabel{fig:timeComparisonEvaluateNetworkTotalScatter}{{7.5}{107}{Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{\mathrm {TFP}}$), the TF C++ API ($T_{\mathrm {TFC}}$) and Armadillo ($T_{\mathrm {ARMA}}$). $L$ is the number of hidden layers, $n$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Validation}{109}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Training Lennard-Jones potential}{109}{section.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }}{109}{figure.caption.57}}
\newlabel{fig:errorLJTest}{{8.1}{109}{Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }}{110}{figure.caption.58}}
\newlabel{fig:errorBackPropTest}{{8.2}{110}{Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }{figure.caption.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Many-neighbour Lennard-Jones}{111}{subsection.8.1.1}}
\newlabel{zoomedOut}{{8.3a}{112}{Zoomed out\relax }{figure.caption.59}{}}
\newlabel{sub@zoomedOut}{{a}{112}{Zoomed out\relax }{figure.caption.59}{}}
\newlabel{zoomedIn}{{8.3b}{112}{Zoomed in\relax }{figure.caption.59}{}}
\newlabel{sub@zoomedIn}{{b}{112}{Zoomed in\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Quadratic cost \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticCostFunction}\unskip \@@italiccorr )}} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sigmoidActivationFunction}\unskip \@@italiccorr )}}. \autoref  {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref  {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }}{112}{figure.caption.59}}
\newlabel{fig:trainingManyNeighbourNN}{{8.3}{112}{Quadratic cost \eqref {quadraticCostFunction} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \eqref {sigmoidActivationFunction}. \autoref {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }}{115}{figure.caption.60}}
\newlabel{fig:errorManyNeighbourNN}{{8.4}{115}{Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}NN potential for Si}{117}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:SiPotentialResults}{{9}{117}{NN potential for Si}{chapter.9}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}NN potential for SiO2}{119}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Conclusions and future work}{121}{part.3}}
\citation{Behler11general}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}The quality of the NNP}{123}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendices}{125}{section*.61}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Symmetry functions derivatives}{127}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:appendixA1}{{A}{127}{Symmetry functions derivatives}{Appendix.1.A}{}}
\newlabel{cutOffFunctionDerivative}{{A.3}{127}{Symmetry functions derivatives}{equation.1.A.0.3}{}}
\newlabel{cutOffFunctionDerivative1}{{A.4}{127}{Symmetry functions derivatives}{equation.1.A.0.4}{}}
\newlabel{cutOffFunctionDerivative1}{{A.5}{127}{Symmetry functions derivatives}{equation.1.A.0.5}{}}
\newlabel{cutOffFunctionDerivative1}{{A.7}{127}{Symmetry functions derivatives}{equation.1.A.0.7}{}}
\newlabel{cutOffFunctionDerivative1}{{A.8}{128}{Symmetry functions derivatives}{equation.1.A.0.8}{}}
\newlabel{G4Derivative}{{A.16}{128}{Symmetry functions derivatives}{equation.1.A.0.16}{}}
\bibcite{Behler11general}{1}
\bibcite{Behler11symmetry}{2}
\bibcite{Dragly14}{3}
\bibcite{Hornik89}{4}
\bibcite{Rojas96}{5}
\bibcite{Karlik11}{6}
\bibcite{LeCun15}{7}
\bibcite{Glorot11}{8}
\bibcite{Rumelhart86}{9}
\bibcite{Behler15}{10}
\bibcite{Tuckerman92}{11}
\bibcite{Plimpton95}{12}
\bibcite{Abadi15}{13}
\bibcite{Abadi16}{14}
\bibcite{Qian99}{15}
\bibcite{Duchi11}{16}
\bibcite{Zeiler12}{17}
\bibcite{Kingma14}{18}
\bibcite{Ischtwan94}{19}
\bibcite{Raff05}{20}
\bibcite{Raff12}{21}
\bibcite{Pukrittayakamee09}{22}
\bibcite{McCulloch43}{23}
\bibcite{Kriesel07}{24}
\bibcite{LeCun99}{25}
\bibcite{Rosenblatt58}{26}
\bibcite{Ruder16}{27}
\bibcite{Dawes08}{28}
\bibcite{Jones24}{29}
\bibcite{Stillinger85}{30}
\bibcite{Molinero08}{31}
\bibcite{Vashishta90}{32}
\bibcite{Vashishta07}{33}
\bibcite{Frenkel01}{34}
\bibcite{Agrawal06}{35}
\bibcite{Bholoa07}{36}
\bibcite{Behler07}{37}
\bibcite{LeCun12}{38}
\bibcite{Bengio12}{39}
\bibcite{Larochelle09}{40}
\bibcite{Witkoskie05}{41}
\bibcite{Bengio07}{42}
\bibcite{Glorot10}{43}
\bibcite{Krogh1992}{44}
\bibcite{Srivastava14}{45}
\bibcite{Krizhevsky12}{46}
\bibcite{Hinton12}{47}
\bibcite{Collobert11}{48}
\bibcite{Ciodaro12}{49}
\bibcite{Helmstaedter13}{50}
\bibcite{Carleo17}{51}
\bibcite{Krenn16}{52}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  borland.pygstyle,
  default-pyg-prefix.pygstyle,
  07E5BCD4BB70BA9368B69A6D2EBBEB08B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  8FFEE2B7E646C9CB93160A4E393FED99B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  BBB954BDA323F884008F1770DA6A25CDB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  65855A335FC7E9A31E1F2A68AE6F9F51B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  AE37967E17E8CF1C078485BBDA7D452DB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  32356922E3125AD5A6783150D17503A8B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  6AD7B58BDD73B14B919DA05C8FD64BF522779287BE07124B95CB8A8CAE72113E.pygtex,
  BF272435D2DD3F8B7BA9FE7C0AC002B122779287BE07124B95CB8A8CAE72113E.pygtex,
  C07C9AFAD0035F19D4E0DBC50D7B35F122779287BE07124B95CB8A8CAE72113E.pygtex,
  F52DE0F8154C7046EFA236DCF7679B7222779287BE07124B95CB8A8CAE72113E.pygtex,
  BACEDF1DE405A925744C7A215D6EF65922779287BE07124B95CB8A8CAE72113E.pygtex,
  B33B5AD4BCFFE5428EF8AD9F136D497822779287BE07124B95CB8A8CAE72113E.pygtex,
  03643CA163278E0C0B14CBEAB18E262222779287BE07124B95CB8A8CAE72113E.pygtex,
  5E9977EA2B2E3C41304161C4670DCB8822779287BE07124B95CB8A8CAE72113E.pygtex,
  B96A2F9C2C21DC9B6C0E9E12BC3292C5B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  44891C627509FD12201166A00012ABEDB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  7DBC237B4708E63BA3D155BE0023B31F22779287BE07124B95CB8A8CAE72113E.pygtex}
