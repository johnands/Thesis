\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Krizhevsky12}
\citation{Hinton12}
\citation{Collobert11}
\citation{Ciodaro12}
\citation{Helmstaedter13}
\citation{Carleo17}
\citation{Krenn16}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Potentials in molecular dynamics}{1}{section.1.1}}
\citation{Sherrill10}
\citation{Jones24}
\citation{Stillinger85}
\citation{Dawes08}
\citation{Ischtwan94}
\citation{Behler11general}
\citation{Agrawal06}
\citation{Prudente98}
\citation{Bholoa07}
\citation{Behler11symmetry}
\citation{Raff12}
\citation{Artrith12}
\citation{Eshet12}
\citation{Natarajan16}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Neural network potentials}{2}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Goals}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Our contributions}{4}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Structure of the thesis}{5}{section.1.5}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Introductory theory}{7}{part.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Molecular dynamics}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:MD}{{2}{9}{Molecular dynamics}{chapter.2}{}}
\newlabel{forcePES}{{2.1}{9}{Molecular dynamics}{equation.2.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Potential energy surfaces}{9}{section.2.1}}
\newlabel{sec:potentialEnergySurfaces}{{2.1}{9}{Potential energy surfaces}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}From quantum mechanics to classical potentials}{10}{subsection.2.1.1}}
\newlabel{sec:bornOppenheimer}{{2.1.1}{10}{From quantum mechanics to classical potentials}{subsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Constructing potential energy surfaces}{10}{subsection.2.1.2}}
\newlabel{sec:constructingPES}{{2.1.2}{10}{Constructing potential energy surfaces}{subsection.2.1.2}{}}
\newlabel{generalPotential}{{2.2}{10}{Constructing potential energy surfaces}{equation.2.1.2}{}}
\citation{Dawes08}
\citation{Ischtwan94}
\@writefile{toc}{\contentsline {subsubsection}{Truncation and configuration space}{11}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Fitting procedure}{11}{section*.3}}
\citation{Jones24}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Common empirical potentials}{12}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Lennard-Jones}{12}{subsection.2.2.1}}
\newlabel{Lennard-Jones}{{2.4}{12}{Lennard-Jones}{equation.2.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The Lennard-Jones potential \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {Lennard-Jones}\unskip \@@italiccorr )}} as a function of interatomic distance for two different parameter sets. The depth of the potential well (potential strength) is given by $\epsilon $, while $\sigma $ is the interatomic distance for which the potential is zero.\relax }}{13}{figure.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:LJ}{{2.1}{13}{The Lennard-Jones potential \eqref {Lennard-Jones} as a function of interatomic distance for two different parameter sets. The depth of the potential well (potential strength) is given by $\epsilon $, while $\sigma $ is the interatomic distance for which the potential is zero.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Calculating total potential energy}{13}{section*.5}}
\newlabel{twoBodyPotentialEnergy}{{2.5}{13}{Calculating total potential energy}{equation.2.2.5}{}}
\citation{Stillinger85}
\citation{Stillinger85}
\citation{Molinero08}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Three atoms $(i,j,k)$ define three different triplets that generally have different energies. They must therefore be tallied individually when computing the total energy of a system.\relax }}{14}{figure.caption.6}}
\newlabel{fig:triplets}{{2.2}{14}{Three atoms $(i,j,k)$ define three different triplets that generally have different energies. They must therefore be tallied individually when computing the total energy of a system.\relax }{figure.caption.6}{}}
\newlabel{threeBodyPotentialEnergy1}{{2.7}{14}{Calculating total potential energy}{equation.2.2.7}{}}
\newlabel{threeBodyPotentialEnergy2}{{2.8}{14}{Calculating total potential energy}{equation.2.2.8}{}}
\newlabel{totalPotentialEnergy}{{2.9}{14}{Calculating total potential energy}{equation.2.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Stillinger-Weber}{14}{subsection.2.2.2}}
\newlabel{sec:stillingerWeber}{{2.2.2}{14}{Stillinger-Weber}{subsection.2.2.2}{}}
\newlabel{StillingerWeber2Body}{{2.10}{14}{Stillinger-Weber}{equation.2.2.10}{}}
\newlabel{StillingerWeber3Body}{{2.11}{14}{Stillinger-Weber}{equation.2.2.11}{}}
\citation{Vashishta90}
\citation{Vashishta07}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Vashishta}{15}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Time integration}{15}{section.2.3}}
\newlabel{sec:timeIntegration}{{2.3}{15}{Time integration}{section.2.3}{}}
\newlabel{equationsOfMotion1}{{2.14}{15}{Time integration}{equation.2.3.14}{}}
\newlabel{equationsOfMotion2}{{2.15}{15}{Time integration}{equation.2.3.15}{}}
\citation{Frenkel01}
\citation{Frenkel01}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Force calculations and cutoff radius}{16}{section.2.4}}
\newlabel{sec:forceCutoff}{{2.4}{16}{Force calculations and cutoff radius}{section.2.4}{}}
\newlabel{forceCutoff}{{2.21}{17}{Force calculations and cutoff radius}{equation.2.4.21}{}}
\newlabel{shiftedLJ}{{2.22}{17}{Force calculations and cutoff radius}{equation.2.4.22}{}}
\citation{Krizhevsky12}
\citation{Collobert11}
\citation{Hinton12}
\citation{LeCun15}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine learning}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:machineLearning}{{3}{19}{Machine learning}{chapter.3}{}}
\citation{McCulloch43}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Overview of various approaches to machine learning and their applications. Source: \href  {http://www.isaziconsulting.co.za/machinelearning.html}{isaziconsulting.co.za}.\relax }}{20}{figure.caption.7}}
\newlabel{fig:machineLearningDiagram}{{3.1}{20}{Overview of various approaches to machine learning and their applications. Source: \href {http://www.isaziconsulting.co.za/machinelearning.html}{isaziconsulting.co.za}.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Artificial neurons}{20}{section.3.1}}
\newlabel{sec:ANN}{{3.1}{20}{Artificial neurons}{section.3.1}{}}
\citation{Rojas96}
\citation{Kriesel07}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots  ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }}{21}{figure.caption.8}}
\newlabel{fig:neuronModel}{{3.2}{21}{Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }{figure.caption.8}{}}
\newlabel{artificialNeuron}{{3.1}{21}{Artificial neurons}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Biological model of a nervous cell. The various parts of the cell are briefly explained in the text. Source: \href  {https://askabiologist.asu.edu/neuron-anatomy}{askabiologist.asu.edu}.\relax }}{22}{figure.caption.10}}
\newlabel{fig:neuronBiological}{{3.3}{22}{Biological model of a nervous cell. The various parts of the cell are briefly explained in the text. Source: \href {https://askabiologist.asu.edu/neuron-anatomy}{askabiologist.asu.edu}.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural network types}{22}{section.3.2}}
\citation{LeCun99}
\citation{LeCun15}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit  {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }}{23}{figure.caption.11}}
\newlabel{fig:networkGeneral}{{3.4}{23}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Feed-forward neural networks}{23}{subsection.3.2.1}}
\citation{Rosenblatt58}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Recurrent neural networks}{24}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Other types of networks}{24}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multilayer perceptron}{24}{section.3.3}}
\newlabel{sec:MLP}{{3.3}{24}{Multilayer perceptron}{section.3.3}{}}
\citation{Hornik89}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Why multilayer perceptrons?}{25}{subsection.3.3.1}}
\newlabel{sec:whyMLP}{{3.3.1}{25}{Why multilayer perceptrons?}{subsection.3.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Mathematical model}{25}{subsection.3.3.2}}
\newlabel{sec:MLPmodel}{{3.3.2}{25}{Mathematical model}{subsection.3.3.2}{}}
\newlabel{artificialNeuron2}{{3.2}{25}{Mathematical model}{equation.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are displayed.\relax }}{26}{figure.caption.12}}
\newlabel{fig:networkNotation}{{3.5}{26}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are displayed.\relax }{figure.caption.12}{}}
\newlabel{outputLayer1}{{3.4}{27}{Mathematical model}{equation.3.3.4}{}}
\newlabel{generalLayer}{{3.5}{27}{Mathematical model}{equation.3.3.5}{}}
\newlabel{outputLayer2}{{3.7}{27}{Mathematical model}{equation.3.3.7}{}}
\newlabel{completeNN}{{3.10}{27}{Mathematical model}{equation.3.3.10}{}}
\citation{Behler11general}
\citation{Behler11general}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$ \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {completeNN}\unskip \@@italiccorr )}}, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively. Here $f$ is the hyperbolic tangent \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {tanhActivationFunction}\unskip \@@italiccorr )}}. Reproduced from Behler \cite  {Behler11general}.\relax }}{28}{figure.caption.13}}
\newlabel{fig:activationsFlex}{{3.6}{28}{The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$ \eqref {completeNN}, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively. Here $f$ is the hyperbolic tangent \eqref {tanhActivationFunction}. Reproduced from Behler \cite {Behler11general}.\relax }{figure.caption.13}{}}
\citation{Hornik89}
\@writefile{toc}{\contentsline {subsubsection}{Activation function of output neuron}{29}{section*.14}}
\newlabel{outputActivation}{{3.12}{29}{Activation function of output neuron}{equation.3.3.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix-vector notation}{29}{section*.15}}
\citation{Rojas96}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Two of the most common activation functions for neural networks, the sigmoid \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sigmoidActivationFunction}\unskip \@@italiccorr )}} and the hyperbolic tangent \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {tanhActivationFunction}\unskip \@@italiccorr )}}. Both meet the requirements of the universal approximation theorem.\relax }}{30}{figure.caption.16}}
\newlabel{fig:SigmoidActivationFunctions}{{3.7}{30}{Two of the most common activation functions for neural networks, the sigmoid \eqref {sigmoidActivationFunction} and the hyperbolic tangent \eqref {tanhActivationFunction}. Both meet the requirements of the universal approximation theorem.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Activation functions}{30}{section.3.4}}
\newlabel{sec:activationFunctions}{{3.4}{30}{Activation functions}{section.3.4}{}}
\newlabel{sigmoidActivationFunction}{{3.15}{30}{Activation functions}{equation.3.4.15}{}}
\citation{Karlik11}
\citation{LeCun15}
\citation{Glorot11}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The rectifier \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {reluActivationFunction}\unskip \@@italiccorr )}} has become the most popular activation function for deep convolutional NNs.\relax }}{31}{figure.caption.17}}
\newlabel{fig:reluActivationFunction}{{3.8}{31}{The rectifier \eqref {reluActivationFunction} has become the most popular activation function for deep convolutional NNs.\relax }{figure.caption.17}{}}
\newlabel{tanhActivationFunction}{{3.16}{31}{Activation functions}{equation.3.4.16}{}}
\newlabel{reluActivationFunction}{{3.17}{31}{Activation functions}{equation.3.4.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training}{32}{section.3.5}}
\newlabel{sec:training}{{3.5}{32}{Training}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cost functions}{33}{section*.18}}
\newlabel{costFunctions}{{3.5}{33}{Cost functions}{section*.18}{}}
\newlabel{generalCost}{{3.18}{33}{Cost functions}{equation.3.5.18}{}}
\newlabel{quadraticCost}{{3.19}{33}{Cost functions}{equation.3.5.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Optimization}{33}{section.3.6}}
\newlabel{sec:optimization}{{3.6}{33}{Optimization}{section.3.6}{}}
\newlabel{gradientDescent}{{3.20}{33}{Optimization}{equation.3.6.20}{}}
\citation{Ruder16}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Gradient descent variants}{34}{subsection.3.6.1}}
\newlabel{sec:gradientDescentVariants}{{3.6.1}{34}{Gradient descent variants}{subsection.3.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Optimization algorithms}{34}{subsection.3.6.2}}
\newlabel{sec:optimizationAlgorithms}{{3.6.2}{34}{Optimization algorithms}{subsection.3.6.2}{}}
\citation{Qian99}
\citation{Duchi11}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{35}{section*.19}}
\newlabel{Momentum}{{3.21}{35}{Momentum}{equation.3.6.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adagrad}{35}{section*.20}}
\newlabel{Adagrad}{{3.24}{35}{Adagrad}{equation.3.6.24}{}}
\citation{Zeiler12}
\citation{Kingma14}
\@writefile{toc}{\contentsline {subsubsection}{Adadelta}{36}{section*.21}}
\newlabel{decayingAverageVector}{{3.25}{36}{Adadelta}{equation.3.6.25}{}}
\newlabel{preliminiaryAdadelta}{{3.27}{36}{Adadelta}{equation.3.6.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{36}{section*.22}}
\citation{Kingma14}
\citation{Kingma14}
\newlabel{adamUpdateRule}{{3.32}{37}{Adam}{equation.3.6.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{Which optimizer to use?}{37}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Backpropagation}{37}{subsection.3.6.3}}
\newlabel{sec:backprop}{{3.6.3}{37}{Backpropagation}{subsection.3.6.3}{}}
\citation{Rumelhart86}
\newlabel{weightDerivative}{{3.34}{38}{Backpropagation}{equation.3.6.34}{}}
\newlabel{biasDerivative}{{3.35}{38}{Backpropagation}{equation.3.6.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{1. Forward propagation}{38}{section*.24}}
\@writefile{toc}{\contentsline {subsubsection}{2. Backward propagation}{39}{section*.25}}
\newlabel{weightDerivativeExpanded}{{3.38}{39}{2. Backward propagation}{equation.3.6.38}{}}
\newlabel{neuronError}{{3.39}{39}{2. Backward propagation}{equation.3.6.39}{}}
\newlabel{derivativeSecondTerm}{{3.41}{39}{2. Backward propagation}{equation.3.6.41}{}}
\newlabel{weightGradient}{{3.42}{39}{2. Backward propagation}{equation.3.6.42}{}}
\newlabel{forwardProp}{{3.43}{39}{2. Backward propagation}{equation.3.6.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref  {fig:neuronModel}. A hidden neuron $j$ receives a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t. to its net input $u_j$.\relax }}{40}{figure.caption.26}}
\newlabel{fig:backprop}{{3.9}{40}{Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref {fig:neuronModel}. A hidden neuron $j$ receives a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t. to its net input $u_j$.\relax }{figure.caption.26}{}}
\newlabel{errorTerms}{{3.46}{40}{2. Backward propagation}{equation.3.6.46}{}}
\newlabel{backprop}{{3.49}{41}{2. Backward propagation}{equation.3.6.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix notation}{41}{section*.27}}
\@writefile{toc}{\contentsline {subsubsection}{Training algorithm}{42}{section*.28}}
\newlabel{forwardPropMatrix}{{3.60}{42}{Training algorithm}{equation.3.6.60}{}}
\newlabel{backPropMatrix}{{3.62}{42}{Training algorithm}{equation.3.6.62}{}}
\newlabel{weightUpdate}{{3.63}{43}{Training algorithm}{equation.3.6.63}{}}
\citation{Behler11general}
\citation{Agrawal06}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural network potentials}{45}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:NNPs}{{4}{45}{Neural network potentials}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural network potentials}{45}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Potentials using single neural network}{45}{subsection.4.1.1}}
\citation{Bholoa07}
\citation{Behler07}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Potentials using multiple neural networks}{46}{subsection.4.1.2}}
\newlabel{systemEnergy}{{4.1}{46}{Potentials using multiple neural networks}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}The Behler-Parrinello method}{46}{section.4.2}}
\newlabel{BPatomicEnergySi}{{4.2}{47}{The Behler-Parrinello method}{equation.4.2.2}{}}
\newlabel{BPatomicEnergy}{{4.3}{47}{The Behler-Parrinello method}{equation.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Symmetry functions}{47}{subsection.4.2.1}}
\newlabel{sec:symmetryFunctions}{{4.2.1}{47}{Symmetry functions}{subsection.4.2.1}{}}
\citation{Behler11symmetry}
\citation{Behler11symmetry}
\newlabel{cutoffFunction}{{4.4}{48}{Symmetry functions}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Radial symmetry functions}{48}{section*.30}}
\newlabel{G1}{{4.5}{48}{Radial symmetry functions}{equation.4.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plot of the cutoff function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {cutoffFunction}\unskip \@@italiccorr )}} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $r_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }}{49}{figure.caption.29}}
\newlabel{fig:cutoffFunction}{{4.1}{49}{Plot of the cutoff function \eqref {cutoffFunction} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $r_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }{figure.caption.29}{}}
\newlabel{G2}{{4.6}{49}{Radial symmetry functions}{equation.4.2.6}{}}
\newlabel{G3}{{4.7}{49}{Radial symmetry functions}{equation.4.2.7}{}}
\newlabel{fig:radialSymmetriFunctions:a}{{\caption@xref {fig:radialSymmetriFunctions:a}{ on input line 1717}}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{sub@fig:radialSymmetriFunctions:a}{{}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{fig:radialSymmetriFunctions:b}{{\caption@xref {fig:radialSymmetriFunctions:b}{ on input line 1723}}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{sub@fig:radialSymmetriFunctions:b}{{}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{fig:radialSymmetriFunctions:c}{{\caption@xref {fig:radialSymmetriFunctions:c}{ on input line 1729}}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{sub@fig:radialSymmetriFunctions:c}{{}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{fig:radialSymmetriFunctions:d}{{\caption@xref {fig:radialSymmetriFunctions:d}{ on input line 1734}}{50}{Radial symmetry functions}{figure.caption.31}{}}
\newlabel{sub@fig:radialSymmetriFunctions:d}{{}{50}{Radial symmetry functions}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distribution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $r_c = \SI {11.0}{\angstrom }$ has been used. $r_{ij}$, $r_c$ and $r_s$ are in units of $\SI {}{\angstrom }$, while $\eta $ and $\kappa $ have units $\SI {}{\angstrom }^{-2}$ and $\SI {}{\angstrom }^{-1}$ respectively.\relax }}{50}{figure.caption.31}}
\newlabel{fig:radialSymmetriFunctions}{{4.2}{50}{Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distribution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $r_c = \SI {11.0}{\angstrom }$ has been used. $r_{ij}$, $r_c$ and $r_s$ are in units of $\SI {}{\angstrom }$, while $\eta $ and $\kappa $ have units $\SI {}{\angstrom }^{-2}$ and $\SI {}{\angstrom }^{-1}$ respectively.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{Angular symmetry functions}{50}{section*.32}}
\newlabel{G4}{{4.8}{51}{Angular symmetry functions}{equation.4.2.8}{}}
\newlabel{G5}{{4.9}{51}{Angular symmetry functions}{equation.4.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Determining the symmetry parameters}{51}{section*.34}}
\citation{Behler11symmetry}
\newlabel{fig:angularSymmetryFunctions:a}{{4.3a}{52}{\relax }{figure.caption.33}{}}
\newlabel{sub@fig:angularSymmetryFunctions:a}{{a}{52}{\relax }{figure.caption.33}{}}
\newlabel{fig:angularSymmetryFunctions:b}{{4.3b}{52}{\relax }{figure.caption.33}{}}
\newlabel{sub@fig:angularSymmetryFunctions:b}{{b}{52}{\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Angular part of symmetry functions $G_i^4$ and $G_i^5$ for an atom with one neighbour only. A set of such functions represents the angular distribution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref  {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref  {fig:angularSymmetryFunctions:b}.\relax }}{52}{figure.caption.33}}
\newlabel{fig:angularSymmetryFunctions}{{4.3}{52}{Angular part of symmetry functions $G_i^4$ and $G_i^5$ for an atom with one neighbour only. A set of such functions represents the angular distribution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref {fig:angularSymmetryFunctions:b}.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Symmetry functions and forces}{52}{subsection.4.2.2}}
\newlabel{sec:symmAndForces}{{4.2.2}{52}{Symmetry functions and forces}{subsection.4.2.2}{}}
\newlabel{forcePES2}{{4.11}{52}{Symmetry functions and forces}{equation.4.2.11}{}}
\newlabel{forceAtomk}{{4.12}{52}{Symmetry functions and forces}{equation.4.2.12}{}}
\newlabel{forceAtomkChainRule}{{4.13}{53}{Symmetry functions and forces}{equation.4.2.13}{}}
\newlabel{BPatomicForce}{{4.14}{53}{Symmetry functions and forces}{equation.4.2.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Change of coordinates}{53}{section*.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Example system of three atoms of the same type. Only atom 2 is inside the cutoff sphere of atom 1, thus we only need to take the energy of atom 1 and 2 into account to find the force on atom 1. Since the atoms are of the same type, they have identical atomic NNs and symmetry function sets.\relax }}{54}{figure.caption.36}}
\newlabel{fig:threeAtoms}{{4.4}{54}{Example system of three atoms of the same type. Only atom 2 is inside the cutoff sphere of atom 1, thus we only need to take the energy of atom 1 and 2 into account to find the force on atom 1. Since the atoms are of the same type, they have identical atomic NNs and symmetry function sets.\relax }{figure.caption.36}{}}
\newlabel{changeOfCoordinates}{{4.15}{54}{Change of coordinates}{equation.4.2.15}{}}
\newlabel{forceOnAtom1}{{4.16}{54}{Change of coordinates}{equation.4.2.16}{}}
\newlabel{energyAtom1}{{4.17}{55}{Change of coordinates}{equation.4.2.17}{}}
\newlabel{energyAtom2}{{4.18}{55}{Change of coordinates}{equation.4.2.18}{}}
\newlabel{changeOfCoordsSymmetry1}{{4.19}{55}{Change of coordinates}{equation.4.2.19}{}}
\newlabel{changeOfCoordsSymmetry2}{{4.20}{55}{Change of coordinates}{equation.4.2.20}{}}
\newlabel{changeOfCoords3body1}{{4.21}{55}{Change of coordinates}{equation.4.2.21}{}}
\newlabel{changeOfCoords3body2}{{4.22a}{55}{Change of coordinates}{equation.4.2.22a}{}}
\newlabel{changeOfCoords3body3}{{4.22b}{55}{Change of coordinates}{equation.4.2.22b}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Summary}{56}{section.4.3}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Advanced theory, implementation and results}{57}{part.2}}
\citation{Plimpton95}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}LAMMPS}{59}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:lammps}{{5}{59}{LAMMPS}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Installing LAMMPS}{59}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}LAMMPS input script}{60}{section.5.2}}
\newlabel{sec:lammpsInputScript}{{5.2}{60}{LAMMPS input script}{section.5.2}{}}
\citation{Tuckerman92}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}LAMMPS structure}{64}{section.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Extending LAMMPS}{64}{section.5.4}}
\newlabel{sec:extendingLammps}{{5.4}{64}{Extending LAMMPS}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Class hierarchy of LAMMPS. The inheritance is from left to right. The top-level classes (blue) are visible everywhere in the code. Only a selected set of sub-classes (green and yellow) are displayed, where we recognize many of the input script commands.\relax }}{65}{figure.caption.37}}
\newlabel{fig:lammpsClasses}{{5.1}{65}{Class hierarchy of LAMMPS. The inheritance is from left to right. The top-level classes (blue) are visible everywhere in the code. Only a selected set of sub-classes (green and yellow) are displayed, where we recognize many of the input script commands.\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Potential initialization}{66}{subsection.5.4.1}}
\zref@newlabel{mdf@pagelabel-1}{\default{5.4.1}\page{66}\abspage{76}\mdf@pagevalue{66}}
\zref@newlabel{mdf@pagelabel-2}{\default{5.4.1}\page{66}\abspage{76}\mdf@pagevalue{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Computing energies and forces}{67}{subsection.5.4.2}}
\zref@newlabel{mdf@pagelabel-3}{\default{5.4.2}\page{67}\abspage{77}\mdf@pagevalue{67}}
\zref@newlabel{mdf@pagelabel-4}{\default{5.4.2}\page{68}\abspage{78}\mdf@pagevalue{68}}
\zref@newlabel{mdf@pagelabel-5}{\default{5.4.2}\page{68}\abspage{78}\mdf@pagevalue{68}}
\zref@newlabel{mdf@pagelabel-6}{\default{5.4.2}\page{69}\abspage{79}\mdf@pagevalue{69}}
\newlabel{forwardPropMatrixLammpsChapter}{{5.1}{69}{Computing energies and forces}{equation.5.4.1}{}}
\zref@newlabel{mdf@pagelabel-7}{\default{5.4.2}\page{69}\abspage{79}\mdf@pagevalue{69}}
\zref@newlabel{mdf@pagelabel-8}{\default{5.4.2}\page{70}\abspage{80}\mdf@pagevalue{70}}
\newlabel{backPropMatrixLammpsChapter}{{5.2}{70}{Computing energies and forces}{equation.5.4.2}{}}
\zref@newlabel{mdf@pagelabel-9}{\default{5.4.2}\page{70}\abspage{80}\mdf@pagevalue{70}}
\newlabel{forceAtomkChainRuleLammpsChapter}{{5.3}{70}{Computing energies and forces}{equation.5.4.3}{}}
\zref@newlabel{mdf@pagelabel-10}{\default{5.4.2}\page{70}\abspage{80}\mdf@pagevalue{70}}
\zref@newlabel{mdf@pagelabel-11}{\default{5.4.2}\page{71}\abspage{81}\mdf@pagevalue{71}}
\zref@newlabel{mdf@pagelabel-12}{\default{5.4.2}\page{71}\abspage{81}\mdf@pagevalue{71}}
\zref@newlabel{mdf@pagelabel-13}{\default{5.4.2}\page{72}\abspage{82}\mdf@pagevalue{72}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Summary}{72}{section.5.5}}
\citation{Abadi15}
\citation{Abadi16}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}TensorFlow}{73}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:tensorFlow}{{6}{73}{TensorFlow}{chapter.6}{}}
\newlabel{quadraticEquationSolution1}{{6.1a}{73}{TensorFlow}{equation.6.0.1a}{}}
\newlabel{quadraticEquationSolution2}{{6.1b}{73}{TensorFlow}{equation.6.0.1b}{}}
\newlabel{quadraticEquation}{{6.2}{73}{TensorFlow}{equation.6.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Data flow graph for the solutions (\ref  {quadraticEquationSolution1},\nobreakspace  {}\ref  {quadraticEquationSolution2}) of a quadratic equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticEquation}\unskip \@@italiccorr )}}. The nodes (green circles) are mathematical operations connected by edges (arrows) that show the data flow direction. Inputs and outputs are marked as rectangular boxes. Reproduced from: \href  {http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}{web.cecs.pdx.edu}\relax }}{74}{figure.caption.38}}
\newlabel{fig:DFG}{{6.1}{74}{Data flow graph for the solutions (\ref {quadraticEquationSolution1},~\ref {quadraticEquationSolution2}) of a quadratic equation \eqref {quadraticEquation}. The nodes (green circles) are mathematical operations connected by edges (arrows) that show the data flow direction. Inputs and outputs are marked as rectangular boxes. Reproduced from: \href {http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}{web.cecs.pdx.edu}\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Installing TensorFlow}{74}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}TensorFlow basic usage}{75}{section.6.2}}
\newlabel{sec:TensorFlowBasic}{{6.2}{75}{TensorFlow basic usage}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Hello world}{75}{subsection.6.2.1}}
\zref@newlabel{mdf@pagelabel-14}{\default{6.2.1}\page{75}\abspage{85}\mdf@pagevalue{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Creating a neural network}{76}{subsection.6.2.2}}
\newlabel{sec:TensorFlowCreatingNN}{{6.2.2}{76}{Creating a neural network}{subsection.6.2.2}{}}
\newlabel{preActivation2}{{6.3a}{76}{Creating a neural network}{equation.6.2.3a}{}}
\newlabel{activation2}{{6.3b}{76}{Creating a neural network}{equation.6.2.3b}{}}
\zref@newlabel{mdf@pagelabel-15}{\default{6.2.2}\page{76}\abspage{86}\mdf@pagevalue{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Visualizing the graph}{77}{subsection.6.2.3}}
\zref@newlabel{mdf@pagelabel-16}{\default{6.2.3}\page{77}\abspage{87}\mdf@pagevalue{77}}
\zref@newlabel{mdf@pagelabel-17}{\default{6.2.3}\page{77}\abspage{87}\mdf@pagevalue{77}}
\zref@newlabel{mdf@pagelabel-18}{\default{6.2.3}\page{78}\abspage{88}\mdf@pagevalue{78}}
\zref@newlabel{mdf@pagelabel-19}{\default{6.2.3}\page{78}\abspage{88}\mdf@pagevalue{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }}{79}{figure.caption.39}}
\newlabel{fig:graphExample}{{6.2}{79}{Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Training a NN with TensorFlow}{79}{subsection.6.2.4}}
\zref@newlabel{mdf@pagelabel-20}{\default{6.2.4}\page{79}\abspage{89}\mdf@pagevalue{79}}
\zref@newlabel{mdf@pagelabel-21}{\default{6.2.4}\page{79}\abspage{89}\mdf@pagevalue{79}}
\citation{Qian99}
\citation{Duchi11}
\citation{Zeiler12}
\citation{Kingma14}
\zref@newlabel{mdf@pagelabel-22}{\default{6.2.4}\page{80}\abspage{90}\mdf@pagevalue{80}}
\zref@newlabel{mdf@pagelabel-23}{\default{6.2.4}\page{80}\abspage{90}\mdf@pagevalue{80}}
\zref@newlabel{mdf@pagelabel-24}{\default{6.2.4}\page{80}\abspage{90}\mdf@pagevalue{80}}
\zref@newlabel{mdf@pagelabel-25}{\default{6.2.4}\page{81}\abspage{91}\mdf@pagevalue{81}}
\zref@newlabel{mdf@pagelabel-26}{\default{6.2.4}\page{81}\abspage{91}\mdf@pagevalue{81}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt  {gradients} computes the gradient of all nodes in the graph, \texttt  {trainStep} implements the algorithm for updating the NN parameters, while \texttt  {beta1\_power} and \texttt  {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }}{82}{figure.caption.40}}
\newlabel{fig:tensorBoardTraining}{{6.3}{82}{Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt {gradients} computes the gradient of all nodes in the graph, \texttt {trainStep} implements the algorithm for updating the NN parameters, while \texttt {beta1\_power} and \texttt {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Constructing a neural network potential}{83}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:constructingNNP}{{7}{83}{Constructing a neural network potential}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Selecting the reference set}{83}{section.7.1}}
\newlabel{sec:selectingTrainingData}{{7.1}{83}{Selecting the reference set}{section.7.1}{}}
\citation{Raff12}
\citation{Behler11general}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Iterative molecular dynamics sampling}{84}{subsection.7.1.1}}
\newlabel{sec:iterativeMDsampling}{{7.1.1}{84}{Iterative molecular dynamics sampling}{subsection.7.1.1}{}}
\newlabel{sample}{{1}{84}{Iterative molecular dynamics sampling}{Item.41}{}}
\newlabel{refEnergies}{{2}{84}{Iterative molecular dynamics sampling}{Item.42}{}}
\citation{Ischtwan94}
\citation{Raff05}
\citation{Raff05}
\newlabel{NNPalgorithm}{{7}{85}{Iterative molecular dynamics sampling}{Item.47}{}}
\citation{Raff12}
\citation{Pukrittayakamee09}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Sampling algorithms}{86}{subsection.7.1.2}}
\newlabel{sec:samplingAlgorithms}{{7.1.2}{86}{Sampling algorithms}{subsection.7.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Initial sampling}{86}{section*.41}}
\newlabel{variableIntervalSampling}{{7.1}{86}{Initial sampling}{equation.7.1.1}{}}
\citation{Pukrittayakamee09}
\zref@newlabel{mdf@pagelabel-27}{\default{7.1.2}\page{87}\abspage{97}\mdf@pagevalue{87}}
\newlabel{samplingAlgorithmModified}{{7.2}{87}{Initial sampling}{equation.7.1.2}{}}
\zref@newlabel{mdf@pagelabel-28}{\default{7.1.2}\page{88}\abspage{98}\mdf@pagevalue{88}}
\newlabel{fig:forceDistSamplingAlgo:a}{{7.1a}{89}{\relax }{figure.caption.42}{}}
\newlabel{sub@fig:forceDistSamplingAlgo:a}{{a}{89}{\relax }{figure.caption.42}{}}
\newlabel{fig:forceDistSapmlingAlgo:b}{{7.1b}{89}{\relax }{figure.caption.42}{}}
\newlabel{sub@fig:forceDistSapmlingAlgo:b}{{b}{89}{\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Distribution of forces for one Si atom in a quartz $\textrm  {SiO}_2$ crystal with and without the use of the sampling algorithm \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {samplingAlgorithmModified}\unskip \@@italiccorr )}}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm  {max} = 10$.\relax }}{89}{figure.caption.42}}
\newlabel{fig:forceDistSamplingAlgo}{{7.1}{89}{Distribution of forces for one Si atom in a quartz $\textrm {SiO}_2$ crystal with and without the use of the sampling algorithm \eqref {samplingAlgorithmModified}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm {max} = 10$.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Iterative sampling}{89}{section*.43}}
\citation{Raff05}
\citation{Behler11general}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Summary}{91}{subsection.7.1.3}}
\citation{Behler11symmetry}
\citation{Behler15}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Constructing the symmetry function sets}{92}{section.7.2}}
\newlabel{sec:constructingSymmetry}{{7.2}{92}{Constructing the symmetry function sets}{section.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Initial set}{92}{subsection.7.2.1}}
\newlabel{sec:initialSymmSet}{{7.2.1}{92}{Initial set}{subsection.7.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Example of an initial radial symmetry function set. The parameters $\eta $ and $r_s$ are varied to probe the radial arrangement of neighbouring atoms. All symmetry functions have the same cutoff $r_c = 6.0$. $r_{ij}$, $r_s$ and $r_c$ are in units of $\SI {}{\angstrom }$, while $\eta $ have units $\SI {}{\angstrom }^{-2}$.\relax }}{93}{figure.caption.44}}
\newlabel{fig:radialParams}{{7.2}{93}{Example of an initial radial symmetry function set. The parameters $\eta $ and $r_s$ are varied to probe the radial arrangement of neighbouring atoms. All symmetry functions have the same cutoff $r_c = 6.0$. $r_{ij}$, $r_s$ and $r_c$ are in units of $\SI {}{\angstrom }$, while $\eta $ have units $\SI {}{\angstrom }^{-2}$.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Angular symmetry functions}{93}{section*.45}}
\@writefile{toc}{\contentsline {subsubsection}{Number of symmetry functions}{93}{section*.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Example of an initial angular symmetry function set with $\eta = \SI {0.01}{\angstrom }^{-2}$ and $\lambda = 1$. A corresponding set of functions with $\lambda = -1$ should also be added to make the set more complete. The $\zeta $ parameter is increased in a non-linear way to avoid excessively overlapping functions. \relax }}{94}{figure.caption.46}}
\newlabel{fig:angularParams}{{7.3}{94}{Example of an initial angular symmetry function set with $\eta = \SI {0.01}{\angstrom }^{-2}$ and $\lambda = 1$. A corresponding set of functions with $\lambda = -1$ should also be added to make the set more complete. The $\zeta $ parameter is increased in a non-linear way to avoid excessively overlapping functions. \relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Adjusting the set}{94}{subsection.7.2.2}}
\newlabel{sec:adjustSymmSet}{{7.2.2}{94}{Adjusting the set}{subsection.7.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{For each symmetry function, the range of function values should be as large as possible}{95}{section*.48}}
\@writefile{toc}{\contentsline {subsubsection}{The set of values of two different symmetry functions on a given data set should not be strongly correlated}{95}{section*.49}}
\zref@newlabel{mdf@pagelabel-29}{\default{7.2.2}\page{95}\abspage{105}\mdf@pagevalue{95}}
\citation{Behler15}
\citation{LeCun12}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Setting hyperparameters}{96}{section.7.3}}
\newlabel{sec:settingHyperParams}{{7.3}{96}{Setting hyperparameters}{section.7.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Preconditioning the input data}{96}{subsection.7.3.1}}
\newlabel{sec:transformInputData}{{7.3.1}{96}{Preconditioning the input data}{subsection.7.3.1}{}}
\newlabel{scalingInputData}{{7.5}{96}{Preconditioning the input data}{equation.7.3.5}{}}
\citation{LeCun12}
\citation{LeCun99}
\newlabel{shiftInputData}{{7.6}{97}{Preconditioning the input data}{equation.7.3.6}{}}
\newlabel{scaleVariance}{{7.7}{97}{Preconditioning the input data}{equation.7.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Convergence speed vs evaluation speed}{97}{section*.50}}
\citation{Karlik11}
\citation{Glorot10}
\citation{Bengio12}
\citation{LeCun12}
\citation{Glorot10}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Activation functions and weight initialization}{98}{subsection.7.3.2}}
\newlabel{sec:hyperParamsActFunctions}{{7.3.2}{98}{Activation functions and weight initialization}{subsection.7.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Network architecture and overfitting}{99}{subsection.7.3.3}}
\newlabel{sec:overfitting}{{7.3.3}{99}{Network architecture and overfitting}{subsection.7.3.3}{}}
\citation{Larochelle09}
\citation{Behler07}
\citation{Raff05}
\citation{Witkoskie05}
\citation{Bengio07}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Demonstration of overfitting. Both figures show the RMSE of a NN evaluated on both the training set and the test set. The training set consists of 5578 training examples from a Si Stillinger-Weber simulation. In a), the NN has been trained on the whole training set, while in b) only the first 300 training examples were used. The test set is identical in both cases. We clearly see that overfitting occurs in b).\relax }}{100}{figure.caption.51}}
\newlabel{fig:overfitting}{{7.4}{100}{Demonstration of overfitting. Both figures show the RMSE of a NN evaluated on both the training set and the test set. The training set consists of 5578 training examples from a Si Stillinger-Weber simulation. In a), the NN has been trained on the whole training set, while in b) only the first 300 training examples were used. The test set is identical in both cases. We clearly see that overfitting occurs in b).\relax }{figure.caption.51}{}}
\citation{Krogh1992}
\citation{Srivastava14}
\citation{Bengio12}
\@writefile{toc}{\contentsline {subsubsection}{Other regularization techniques}{101}{section*.52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Optimizer parameters}{101}{subsection.7.3.4}}
\citation{LeCun99}
\citation{Bengio12}
\citation{Pukrittayakamee09}
\citation{Behler15}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.5}Cost function}{102}{subsection.7.3.5}}
\newlabel{sec:hyperParamsCostFunction}{{7.3.5}{102}{Cost function}{subsection.7.3.5}{}}
\newlabel{CFDA}{{7.13}{102}{Cost function}{equation.7.3.13}{}}
\newlabel{forcesCFDA1}{{7.14a}{103}{Cost function}{equation.7.3.14a}{}}
\newlabel{forcesCFDA2}{{7.14b}{103}{Cost function}{equation.7.3.14b}{}}
\zref@newlabel{mdf@pagelabel-30}{\default{7.3.5}\page{103}\abspage{113}\mdf@pagevalue{103}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.6}Hyperparameter space exploration}{103}{subsection.7.3.6}}
\newlabel{sec:optimizingHyperparameters}{{7.3.6}{103}{Hyperparameter space exploration}{subsection.7.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Coordinate descent}{104}{section*.53}}
\@writefile{toc}{\contentsline {subsubsection}{Grid search}{104}{section*.54}}
\@writefile{toc}{\contentsline {subsubsection}{Random search}{104}{section*.55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.7}Summary}{104}{subsection.7.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Applying the neural network potential}{104}{section.7.4}}
\newlabel{sec:applyingNNP}{{7.4}{104}{Applying the neural network potential}{section.7.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Summary}{105}{section.7.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Scatter plot of time usage when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{\mathrm  {TFP}}$), the TF C\texttt  {++} API ($T_{\mathrm  {TFC}}$) and Armadillo ($T_{\mathrm  {ARMA}}$). $L$ is the number of hidden layers, $n$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been calculated by averaging over 50 evaluations for each NN architecture.\relax }}{106}{figure.caption.56}}
\newlabel{fig:timeComparisonEvaluateNetworkTotalScatter}{{7.5}{106}{Scatter plot of time usage when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{\mathrm {TFP}}$), the TF C\texttt {++} API ($T_{\mathrm {TFC}}$) and Armadillo ($T_{\mathrm {ARMA}}$). $L$ is the number of hidden layers, $n$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been calculated by averaging over 50 evaluations for each NN architecture.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Lennard-Jones validation}{107}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:LJValidation}{{8}{107}{Lennard-Jones validation}{chapter.8}{}}
\zref@newlabel{mdf@pagelabel-31}{\default{8}\page{107}\abspage{117}\mdf@pagevalue{107}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Hyperparameters used in the fitting of the truncated and shifted Lennard-Jones potential \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {shiftedLJ}\unskip \@@italiccorr )}}. The meaning of each parameter is found in the text. ''Hidden nodes'' signifies the number of nodes in each hidden layer. \relax }}{108}{table.caption.57}}
\newlabel{tab:hyperParamsLJ}{{8.1}{108}{Hyperparameters used in the fitting of the truncated and shifted Lennard-Jones potential \eqref {shiftedLJ}. The meaning of each parameter is found in the text. ''Hidden nodes'' signifies the number of nodes in each hidden layer. \relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Error in configuration space}{108}{section.8.1}}
\zref@newlabel{mdf@pagelabel-32}{\default{8.1}\page{108}\abspage{118}\mdf@pagevalue{108}}
\citation{Natarajan16}
\citation{Artrith12}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Absolute error of a NN trained to reproduce the truncated and shifted Lennard-Jones potential \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {shiftedLJ}\unskip \@@italiccorr )}}. The plot is as a visualization of the error distribution in the configuration space $r_{ij} \in [a,b]$ defined by the test set. The employed hyperparameters are listed in \autoref  {tab:hyperParamsLJ}.\relax }}{109}{figure.caption.58}}
\newlabel{fig:LJError}{{8.1}{109}{Absolute error of a NN trained to reproduce the truncated and shifted Lennard-Jones potential \eqref {shiftedLJ}. The plot is as a visualization of the error distribution in the configuration space $r_{ij} \in [a,b]$ defined by the test set. The employed hyperparameters are listed in \autoref {tab:hyperParamsLJ}.\relax }{figure.caption.58}{}}
\zref@newlabel{mdf@pagelabel-33}{\default{8.1}\page{109}\abspage{119}\mdf@pagevalue{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Absolute error of the gradient of a NN trained to reproduce the truncated and shifted Lennard-Jones potential \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {shiftedLJ}\unskip \@@italiccorr )}}. Comparing with \autoref  {fig:LJError}, we see that the graph is of the same shape, but with larger oscillations. The employed hyperparameters are listed in \autoref  {tab:hyperParamsLJ}. \relax }}{110}{figure.caption.59}}
\newlabel{fig:LJErrorDerivative}{{8.2}{110}{Absolute error of the gradient of a NN trained to reproduce the truncated and shifted Lennard-Jones potential \eqref {shiftedLJ}. Comparing with \autoref {fig:LJError}, we see that the graph is of the same shape, but with larger oscillations. The employed hyperparameters are listed in \autoref {tab:hyperParamsLJ}. \relax }{figure.caption.59}{}}
\citation{Stillinger85}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Neural network potential for Si}{111}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:SiPotentialResults}{{9}{111}{Neural network potential for Si}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Sampling initial set}{111}{section.9.1}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Initial symmetry function set}{111}{section.9.2}}
\citation{Kingma14}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Fitting the initial data set}{112}{section.9.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Initial set of 8 $G_i^2$ and 16 $G_i^5$ symmetry functions for constructing a Si NNP. Only the angular part $G_i^\theta $ of $G_i^5$ is displayed. Further, only 8 of these are visible because pairs of $G_i^5$ functions with identical angular parts overlap. The dotted lines are the radial and angular distribution of neighbours defined by the training set. The corresponding symmetry parameters are not included. \relax }}{113}{figure.caption.60}}
\newlabel{fig:SiInitialSymmSet}{{9.1}{113}{Initial set of 8 $G_i^2$ and 16 $G_i^5$ symmetry functions for constructing a Si NNP. Only the angular part $G_i^\theta $ of $G_i^5$ is displayed. Further, only 8 of these are visible because pairs of $G_i^5$ functions with identical angular parts overlap. The dotted lines are the radial and angular distribution of neighbours defined by the training set. The corresponding symmetry parameters are not included. \relax }{figure.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Hyperparameters used in the fitting of the initial data set sampled from Stilling-Weber Si simulations. The data set is split into a training set and a test set, where the latter is 10 \% of the total number of structures.\relax }}{113}{table.caption.61}}
\newlabel{tab:hyperParamsSiInitial}{{9.1}{113}{Hyperparameters used in the fitting of the initial data set sampled from Stilling-Weber Si simulations. The data set is split into a training set and a test set, where the latter is 10 \% of the total number of structures.\relax }{table.caption.61}{}}
\zref@newlabel{mdf@pagelabel-34}{\default{9.3}\page{114}\abspage{124}\mdf@pagevalue{114}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Iterative sampling}{114}{section.9.4}}
\newlabel{sec:SiIterativeSampling}{{9.4}{114}{Iterative sampling}{section.9.4}{}}
\zref@newlabel{mdf@pagelabel-35}{\default{9.4}\page{114}\abspage{124}\mdf@pagevalue{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Example plot of the multiple-NN method. Four NNs with different architectures have been fitted to the same data set to similar accuracies. Then, several configurations are generated with one of these NNPs, and the potential energy predictions of each NNP for these structures are compared. The plot shows the energies of a single atom.\relax }}{115}{figure.caption.62}}
\newlabel{fig:multipleNNP}{{9.2}{115}{Example plot of the multiple-NN method. Four NNs with different architectures have been fitted to the same data set to similar accuracies. Then, several configurations are generated with one of these NNPs, and the potential energy predictions of each NNP for these structures are compared. The plot shows the energies of a single atom.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Fitting the final data set}{116}{section.9.5}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Results of a grid search for finding the optimal NN architecture to fit the final Si training set. Each NN architecture is trained for 40000 epochs, and the smallest RMSE obtained during each session are listed, together with the number of epochs and time elapsed to reach this RMSE. The RMSEs are in units of meV, while the time is measured in seconds.\relax }}{117}{table.caption.63}}
\newlabel{tab:gridSearch}{{9.2}{117}{Results of a grid search for finding the optimal NN architecture to fit the final Si training set. Each NN architecture is trained for 40000 epochs, and the smallest RMSE obtained during each session are listed, together with the number of epochs and time elapsed to reach this RMSE. The RMSEs are in units of meV, while the time is measured in seconds.\relax }{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Hyperparameters used in the fitting of the final training set for construction of the Si potential.\relax }}{117}{table.caption.64}}
\newlabel{tab:hyperParamsSiFinal}{{9.3}{117}{Hyperparameters used in the fitting of the final training set for construction of the Si potential.\relax }{table.caption.64}{}}
\citation{Behler07}
\citation{Artrith12}
\citation{Natarajan16}
\citation{Artrith12}
\citation{Natarajan16}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Applying the NNP}{118}{section.9.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The RMSE of the Si NNP force predictions on a single atom as a function of the total force on the atom, demonstrating a clear correlation. The force errors were calculated based on 3000 configurations generated by Stillinger-Weber simulations.\relax }}{119}{figure.caption.65}}
\newlabel{fig:SiForces}{{9.3}{119}{The RMSE of the Si NNP force predictions on a single atom as a function of the total force on the atom, demonstrating a clear correlation. The force errors were calculated based on 3000 configurations generated by Stillinger-Weber simulations.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Comparison of the radial distribution function of a Stillinger-Weber (SW) simulation and a NNP simulation of a Si crystal at temperature of $T \approx 150$ K. The crystal structure of Si is reproduced by the NNP, with a small difference attributed to the increased kinetic energy of the atoms due to the energy error. \relax }}{119}{figure.caption.66}}
\newlabel{fig:radialDist}{{9.4}{119}{Comparison of the radial distribution function of a Stillinger-Weber (SW) simulation and a NNP simulation of a Si crystal at temperature of $T \approx 150$ K. The crystal structure of Si is reproduced by the NNP, with a small difference attributed to the increased kinetic energy of the atoms due to the energy error. \relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.1}Radial distribution function}{120}{subsection.9.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6.2}Mechanical properties}{120}{subsection.9.6.2}}
\citation{Hopcroft10}
\citation{Cowley88}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Various mechanical properties measured from NNP simulations of Si. The analytical values for Stillinger-Weber are included, along with the relative error. The bulk modulus and shear modulus have units of GPa, while the Poisson ratio is unitless.\relax }}{121}{table.caption.67}}
\newlabel{tab:SiMechanical}{{9.4}{121}{Various mechanical properties measured from NNP simulations of Si. The analytical values for Stillinger-Weber are included, along with the relative error. The bulk modulus and shear modulus have units of GPa, while the Poisson ratio is unitless.\relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Conclusions and future work}{123}{part.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Summary and conclusion}{125}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Behler11general}
\@writefile{toc}{\contentsline {subsubsection}{Properties of a high-dimensional neural network potential}{126}{section*.68}}
\citation{Behler15}
\citation{Behler15}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Prospects and future work}{129}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Force accuracy}{129}{section.11.1}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Other systems}{129}{section.11.2}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Optimization}{130}{section.11.3}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{131}{section*.69}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Symmetry function derivatives}{133}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:appendixA1}{{A}{133}{Symmetry function derivatives}{Appendix.1.A}{}}
\zref@newlabel{mdf@pagelabel-36}{\default{A}\page{133}\abspage{143}\mdf@pagevalue{133}}
\zref@newlabel{mdf@pagelabel-37}{\default{A}\page{134}\abspage{144}\mdf@pagevalue{134}}
\@writefile{toc}{\contentsline {subsubsection}{Notation and symmetries}{134}{section*.70}}
\newlabel{symmetryDerivativeApp1}{{A.3a}{134}{Notation and symmetries}{equation.1.A.0.3a}{}}
\newlabel{symmetryDerivativeApp2}{{A.3b}{134}{Notation and symmetries}{equation.1.A.0.3b}{}}
\@writefile{toc}{\contentsline {subsubsection}{Derivatives of cutoff function and radial symmetry functions}{134}{section*.71}}
\newlabel{cutOffFunctionDerivative}{{A.4}{134}{Derivatives of cutoff function and radial symmetry functions}{equation.1.A.0.4}{}}
\newlabel{cutOffFunctionDerivative2}{{A.5}{135}{Derivatives of cutoff function and radial symmetry functions}{equation.1.A.0.5}{}}
\newlabel{cutOffFunctionDerivative3}{{A.6}{135}{Derivatives of cutoff function and radial symmetry functions}{equation.1.A.0.6}{}}
\newlabel{cutOffFunctionDerivative4}{{A.8}{135}{Derivatives of cutoff function and radial symmetry functions}{equation.1.A.0.8}{}}
\newlabel{cutOffFunctionDerivative5}{{A.9}{135}{Derivatives of cutoff function and radial symmetry functions}{equation.1.A.0.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Derivatives of angular symmetry functions}{135}{section*.72}}
\newlabel{G4Derivative}{{A.16b}{135}{Derivatives of angular symmetry functions}{equation.1.A.0.16b}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Symmetry function parameters}{139}{Appendix.1.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:symmFuncParameters}{{B}{139}{Symmetry function parameters}{Appendix.1.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Symmetry function parameters for $G_i^2$ employed in the fitting of the Si NNP in \autoref  {sec:SiPotentialResults}. The same parameters were used in the initial fit and all subsequent fits. $r_c$ and $r_s$ have units \SI {}{\angstrom }, while $\eta $ is in units of $\SI {}{\angstrom }^{-2}$.\relax }}{139}{table.caption.73}}
\newlabel{tab:symmParamsInitialG2}{{B.1}{139}{Symmetry function parameters for $G_i^2$ employed in the fitting of the Si NNP in \autoref {sec:SiPotentialResults}. The same parameters were used in the initial fit and all subsequent fits. $r_c$ and $r_s$ have units \SI {}{\angstrom }, while $\eta $ is in units of $\SI {}{\angstrom }^{-2}$.\relax }{table.caption.73}{}}
\bibstyle{ieeetr}
\bibdata{master}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces Symmetry function parameters for $G_i^5$ employed in the fitting of the Si NNP in \autoref  {sec:SiPotentialResults}. For parameters that were adjusted between the initial and final fits, the values of the initial fit are enclosed in parentheses. $r_c$ have units $\SI {}{\angstrom }$, while $\eta $ is in units of $\SI {}{\angstrom }^{-2}$. $\zeta $ and $\lambda $ are unitless. \relax }}{140}{table.caption.74}}
\newlabel{tab:symmParamsInitialG5}{{B.2}{140}{Symmetry function parameters for $G_i^5$ employed in the fitting of the Si NNP in \autoref {sec:SiPotentialResults}. For parameters that were adjusted between the initial and final fits, the values of the initial fit are enclosed in parentheses. $r_c$ have units $\SI {}{\angstrom }$, while $\eta $ is in units of $\SI {}{\angstrom }^{-2}$. $\zeta $ and $\lambda $ are unitless. \relax }{table.caption.74}{}}
\bibcite{Krizhevsky12}{1}
\bibcite{Hinton12}{2}
\bibcite{Collobert11}{3}
\bibcite{Ciodaro12}{4}
\bibcite{Helmstaedter13}{5}
\bibcite{Carleo17}{6}
\bibcite{Krenn16}{7}
\bibcite{Sherrill10}{8}
\bibcite{Jones24}{9}
\bibcite{Stillinger85}{10}
\bibcite{Dawes08}{11}
\bibcite{Ischtwan94}{12}
\bibcite{Behler11general}{13}
\bibcite{Agrawal06}{14}
\bibcite{Prudente98}{15}
\bibcite{Bholoa07}{16}
\bibcite{Behler11symmetry}{17}
\bibcite{Raff12}{18}
\bibcite{Artrith12}{19}
\bibcite{Eshet12}{20}
\bibcite{Natarajan16}{21}
\bibcite{Molinero08}{22}
\bibcite{Vashishta90}{23}
\bibcite{Vashishta07}{24}
\bibcite{Frenkel01}{25}
\bibcite{LeCun15}{26}
\bibcite{McCulloch43}{27}
\bibcite{Rojas96}{28}
\bibcite{Kriesel07}{29}
\bibcite{LeCun99}{30}
\bibcite{Rosenblatt58}{31}
\bibcite{Hornik89}{32}
\bibcite{Karlik11}{33}
\bibcite{Glorot11}{34}
\bibcite{Ruder16}{35}
\bibcite{Qian99}{36}
\bibcite{Duchi11}{37}
\bibcite{Zeiler12}{38}
\bibcite{Kingma14}{39}
\bibcite{Rumelhart86}{40}
\bibcite{Behler07}{41}
\bibcite{Plimpton95}{42}
\bibcite{Tuckerman92}{43}
\bibcite{Abadi15}{44}
\bibcite{Abadi16}{45}
\bibcite{Raff05}{46}
\bibcite{Pukrittayakamee09}{47}
\bibcite{Behler15}{48}
\bibcite{LeCun12}{49}
\bibcite{Glorot10}{50}
\bibcite{Bengio12}{51}
\bibcite{Larochelle09}{52}
\bibcite{Witkoskie05}{53}
\bibcite{Bengio07}{54}
\bibcite{Krogh1992}{55}
\bibcite{Srivastava14}{56}
\bibcite{Hopcroft10}{57}
\bibcite{Cowley88}{58}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  borland.pygstyle,
  default-pyg-prefix.pygstyle,
  C49C9C21BF0EC943574D83827B21978C5ADC847D9CD2AC8271E7988943C71362.pygtex,
  7C8C1E9C54D9F0A374F957108024987F5ADC847D9CD2AC8271E7988943C71362.pygtex,
  07E5BCD4BB70BA9368B69A6D2EBBEB08B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  2BEC34256C7C5C514D616EB284F525D2B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  AEB8AA439E5A8A759B369CB2654C52FFB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  A2FE7918EFCF016B18BBEE6795EBCE5EB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  AE37967E17E8CF1C078485BBDA7D452DB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  19DE4477EF116B80F083E0DA3BCA2D34B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  70991BED142D05A3545574F5AE5008DB5ADC847D9CD2AC8271E7988943C71362.pygtex,
  72BBBBC8035D1825499476F6070BE6851BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  DDBA58711AF54E050CC6D1527345766D22779287BE07124B95CB8A8CAE72113E.pygtex,
  52EAE8B9FAC65A3038B2077E8B8439F31BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  BF272435D2DD3F8B7BA9FE7C0AC002B122779287BE07124B95CB8A8CAE72113E.pygtex,
  92F3309B4882274FBF01EAAD1C6F59DA1BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  C07C9AFAD0035F19D4E0DBC50D7B35F122779287BE07124B95CB8A8CAE72113E.pygtex,
  D4A2BD7BC47310DD227F097159E9EA2022779287BE07124B95CB8A8CAE72113E.pygtex,
  BACEDF1DE405A925744C7A215D6EF65922779287BE07124B95CB8A8CAE72113E.pygtex,
  86830B8EA456862A2A876193F707B19722779287BE07124B95CB8A8CAE72113E.pygtex,
  F5FC65BA7F2E99EC8025248713FB9ABB22779287BE07124B95CB8A8CAE72113E.pygtex,
  5E9977EA2B2E3C41304161C4670DCB8822779287BE07124B95CB8A8CAE72113E.pygtex,
  B96A2F9C2C21DC9B6C0E9E12BC3292C55ADC847D9CD2AC8271E7988943C71362.pygtex,
  44891C627509FD12201166A00012ABEDB1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  7DBC237B4708E63BA3D155BE0023B31F22779287BE07124B95CB8A8CAE72113E.pygtex,
  FEFB449E0F3A60CFD077D90DF00779F11BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  813DE4EF1B2E2E5BC1630E2F3248B3011BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  0FD6C932108E14A90D25E212153F977C1BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  6F9F3DCE5128E522CCF72D23C3AEB80B1BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  3A46390F2CD6EEE5FD59A0316E1E827A1BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex,
  1E55E0DEC750FDC3C03F33F69A64107C5ADC847D9CD2AC8271E7988943C71362.pygtex,
  66107EB3EAEA54ACB35B1946BF3B5DA41BE631E8E4850E2A9AE02C4CEA74BD6B.pygtex}
