\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Goals}{1}{section.1.1}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Theory}{3}{part.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Molecular dynamics}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Potential energy surfaces}{7}{section.2.1}}
\newlabel{sec:potentialEnergySurfaces}{{2.1}{7}{Potential energy surfaces}{section.2.1}{}}
\newlabel{forcePES}{{2.2}{7}{Potential energy surfaces}{equation.2.1.2}{}}
\citation{Behler11general}
\citation{Dragly14}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine learning}{11}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{Hornik89}
\citation{Behler11general}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Artificial Neural Networks}{12}{section.3.1}}
\newlabel{sec:ANN}{{3.1}{12}{Artificial Neural Networks}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural network\relax }}{12}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NN}{{3.1}{12}{Neural network\relax }{figure.caption.2}{}}
\citation{Hornik89}
\citation{Rojas96}
\newlabel{completeNN}{{3.4}{13}{Artificial Neural Networks}{equation.3.1.4}{}}
\newlabel{sigmoidActivationFunction}{{3.5}{13}{Artificial Neural Networks}{equation.3.1.5}{}}
\citation{Karlik11}
\citation{LeCun15}
\citation{Glorot11}
\newlabel{tanhActivationFunction}{{3.6}{14}{Artificial Neural Networks}{equation.3.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Activation functions\relax }}{14}{figure.caption.3}}
\newlabel{fig:activations}{{3.2}{14}{Activation functions\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Convolution}{15}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training}{15}{section.3.2}}
\newlabel{quadraticCostFunction}{{3.9}{15}{Training}{equation.3.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Backpropagation}{15}{subsection.3.2.1}}
\newlabel{sec:backprop}{{3.2.1}{15}{Backpropagation}{subsection.3.2.1}{}}
\citation{Rumelhart86}
\newlabel{gradientDescent}{{3.10}{16}{Backpropagation}{equation.3.2.10}{}}
\newlabel{weightChange}{{3.11}{16}{Backpropagation}{equation.3.2.11}{}}
\newlabel{neuronError}{{3.13}{16}{Backpropagation}{equation.3.2.13}{}}
\newlabel{forwardProp}{{3.20}{17}{Backpropagation}{equation.3.2.20}{}}
\newlabel{errorTerms}{{3.23}{17}{Backpropagation}{equation.3.2.23}{}}
\newlabel{backProp}{{3.26}{18}{Backpropagation}{equation.3.2.26}{}}
\citation{Behler15}
\citation{Behler11symmetry}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural networks in molecular dynamics}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}High-dimensional NNPs}{22}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \relax }}{23}{figure.caption.4}}
\newlabel{fig:cutOffNeighbours}{{4.1}{23}{\relax }{figure.caption.4}{}}
\newlabel{systemEnergy}{{4.1}{23}{High-dimensional NNPs}{equation.4.1.1}{}}
\citation{Behler15}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Symmetry functions}{24}{subsection.4.1.1}}
\newlabel{sec:symmetryFunctions}{{4.1.1}{24}{Symmetry functions}{subsection.4.1.1}{}}
\newlabel{cutoffFunction}{{4.2}{24}{Symmetry functions}{equation.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Plot of the cutoff function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {cutoffFunction}\unskip \@@italiccorr )}} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }}{25}{figure.caption.5}}
\newlabel{fig:cutoffFunction}{{4.2}{25}{Plot of the cutoff function \eqref {cutoffFunction} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }{figure.caption.5}{}}
\newlabel{G1}{{4.3}{26}{Symmetry functions}{equation.4.1.3}{}}
\newlabel{G2}{{4.4}{26}{Symmetry functions}{equation.4.1.4}{}}
\newlabel{G3}{{4.5}{26}{Symmetry functions}{equation.4.1.5}{}}
\newlabel{fig:radialSymmetriFunctions:a}{{\caption@xref {fig:radialSymmetriFunctions:a}{ on input line 645}}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{sub@fig:radialSymmetriFunctions:a}{{}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{fig:radialSymmetriFunctions:b}{{\caption@xref {fig:radialSymmetriFunctions:b}{ on input line 651}}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{sub@fig:radialSymmetriFunctions:b}{{}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{fig:radialSymmetriFunctions:c}{{\caption@xref {fig:radialSymmetriFunctions:c}{ on input line 657}}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{sub@fig:radialSymmetriFunctions:c}{{}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{fig:radialSymmetriFunctions:d}{{\caption@xref {fig:radialSymmetriFunctions:d}{ on input line 662}}{27}{Symmetry functions}{figure.caption.6}{}}
\newlabel{sub@fig:radialSymmetriFunctions:d}{{}{27}{Symmetry functions}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }}{27}{figure.caption.6}}
\newlabel{fig:radialSymmetriFunctions}{{4.3}{27}{Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }{figure.caption.6}{}}
\newlabel{G4}{{4.7}{27}{Symmetry functions}{equation.4.1.7}{}}
\newlabel{G5}{{4.9}{28}{Symmetry functions}{equation.4.1.9}{}}
\newlabel{fig:angularSymmetryFunctions:a}{{4.4a}{28}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:angularSymmetryFunctions:a}{{a}{28}{\relax }{figure.caption.7}{}}
\newlabel{fig:angularSymmetryFunctions:b}{{4.4b}{28}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:angularSymmetryFunctions:b}{{b}{28}{\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref  {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref  {fig:angularSymmetryFunctions:b}.\relax }}{28}{figure.caption.7}}
\newlabel{fig:angularSymmetryFunctions}{{4.4}{28}{Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref {fig:angularSymmetryFunctions:b}.\relax }{figure.caption.7}{}}
\citation{Behler11symmetry}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Symmetry functions and forces}{29}{subsection.4.1.2}}
\newlabel{forcePES2}{{4.10}{29}{Symmetry functions and forces}{equation.4.1.10}{}}
\newlabel{forceAtomk}{{4.11}{29}{Symmetry functions and forces}{equation.4.1.11}{}}
\newlabel{forceAtomkChainRule}{{4.12}{29}{Symmetry functions and forces}{equation.4.1.12}{}}
\newlabel{changeOfCoordinates}{{4.13}{30}{Symmetry functions and forces}{equation.4.1.13}{}}
\newlabel{changeOfCoordsSymmetry1}{{4.17}{31}{Symmetry functions and forces}{equation.4.1.17}{}}
\newlabel{changeOfCoordsSymmetry2}{{4.18}{31}{Symmetry functions and forces}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Implementation and validation}{33}{part.2}}
\citation{Plimpton95}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}LAMMPS}{35}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Installing LAMMPS}{35}{subsection.5.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}LAMMPS input script}{36}{subsection.5.0.2}}
\citation{Tuckerman92}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}LAMMPS structure}{40}{subsection.5.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Extending LAMMPS}{40}{subsection.5.0.4}}
\newlabel{sec:extendingLammps}{{5.0.4}{40}{Extending LAMMPS}{subsection.5.0.4}{}}
\citation{Abadi15}
\citation{Abadi16}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}TensorFlow}{43}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{quadraticEquationSolution}{{6.2}{43}{TensorFlow}{equation.6.0.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Installing TensorFlow}{44}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}TensorFlow basic usage}{44}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Hello world}{45}{subsection.6.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Creating a neural network}{45}{subsection.6.2.2}}
\newlabel{preActivationLayer}{{6.4}{45}{Creating a neural network}{equation.6.2.4}{}}
\newlabel{activationLayer}{{6.5}{45}{Creating a neural network}{equation.6.2.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{Scripts/TensorFlow/networkExample.py}{46}{lstlisting.6.-26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Visualizing the graph}{47}{subsection.6.2.3}}
\@writefile{lol}{\contentsline {lstlisting}{Scripts/TensorFlow/tensorBoardExample.py}{47}{lstlisting.6.-28}}
\newlabel{lst:tensorBoardExample}{{6.2.3}{48}{Visualizing the graph}{lstnumber.-28.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }}{49}{figure.caption.8}}
\newlabel{fig:graphExample}{{6.1}{49}{Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }{figure.caption.8}{}}
\citation{Qian99}
\citation{Duchi11}
\citation{Zeiler12}
\citation{Kingma14}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Training a NN with TensorFlow}{50}{subsection.6.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt  {gradients} computes the gradient of all nodes in the graph, \texttt  {trainStep} implements the algorithm for updating the NN parameters, while \texttt  {beta1\_power} and \texttt  {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }}{52}{figure.caption.9}}
\newlabel{fig:tensorBoardTraining}{{6.2}{52}{Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt {gradients} computes the gradient of all nodes in the graph, \texttt {trainStep} implements the algorithm for updating the NN parameters, while \texttt {beta1\_power} and \texttt {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Training procedure}{55}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Selecting the training data}{55}{section.7.1}}
\citation{Behler11general}
\citation{Behler15}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Iterative molecular dynamics sampling}{56}{subsection.7.1.1}}
\newlabel{sample}{{1}{56}{Iterative molecular dynamics sampling}{Item.14}{}}
\newlabel{refEnergies}{{2}{56}{Iterative molecular dynamics sampling}{Item.15}{}}
\citation{Ischtwan94}
\citation{Raff05}
\citation{Raff05}
\@writefile{toc}{\contentsline {subsubsection}{Sampling methods}{57}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Constructing the symmetry function sets}{57}{section.7.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Validation}{59}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Time usage}{59}{section.8.1}}
\newlabel{sec:timeUsage}{{8.1}{59}{Time usage}{section.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }}{60}{figure.caption.11}}
\newlabel{fig:timeComparisonEvaluateNetworkTotalScatter}{{8.1}{60}{Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }}{61}{figure.caption.12}}
\newlabel{fig:timeComparisonEvaluateNetwork2}{{8.2}{61}{Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Training Lennard-Jones potential}{61}{section.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }}{62}{figure.caption.13}}
\newlabel{fig:errorLJTest}{{8.3}{62}{Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }}{63}{figure.caption.14}}
\newlabel{fig:errorBackPropTest}{{8.4}{63}{Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Many-neighbour Lennard-Jones}{63}{subsection.8.2.1}}
\newlabel{zoomedOut}{{8.5a}{64}{Zoomed out\relax }{figure.caption.15}{}}
\newlabel{sub@zoomedOut}{{a}{64}{Zoomed out\relax }{figure.caption.15}{}}
\newlabel{zoomedIn}{{8.5b}{64}{Zoomed in\relax }{figure.caption.15}{}}
\newlabel{sub@zoomedIn}{{b}{64}{Zoomed in\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Quadratic cost \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticCostFunction}\unskip \@@italiccorr )}} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sigmoidActivationFunction}\unskip \@@italiccorr )}}. \autoref  {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref  {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }}{64}{figure.caption.15}}
\newlabel{fig:trainingManyNeighbourNN}{{8.5}{64}{Quadratic cost \eqref {quadraticCostFunction} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \eqref {sigmoidActivationFunction}. \autoref {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }}{67}{figure.caption.16}}
\newlabel{fig:errorManyNeighbourNN}{{8.6}{67}{Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Results and discussion}{69}{part.3}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}NN potential for Si}{71}{section.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}NN potential for SiO2}{71}{section.8.4}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{73}{section*.17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Symmetry functions derivatives}{75}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:appendixA1}{{A}{75}{Symmetry functions derivatives}{Appendix.1.A}{}}
\newlabel{cutOffFunctionDerivative}{{A.3}{75}{Symmetry functions derivatives}{equation.1.A.0.3}{}}
\newlabel{cutOffFunctionDerivative1}{{A.4}{75}{Symmetry functions derivatives}{equation.1.A.0.4}{}}
\newlabel{cutOffFunctionDerivative1}{{A.5}{75}{Symmetry functions derivatives}{equation.1.A.0.5}{}}
\newlabel{cutOffFunctionDerivative1}{{A.7}{75}{Symmetry functions derivatives}{equation.1.A.0.7}{}}
\newlabel{cutOffFunctionDerivative1}{{A.8}{76}{Symmetry functions derivatives}{equation.1.A.0.8}{}}
\newlabel{G4Derivative}{{A.16}{76}{Symmetry functions derivatives}{equation.1.A.0.16}{}}
\bibcite{Behler11general}{1}
\bibcite{Behler11symmetry}{2}
\bibcite{Dragly14}{3}
\bibcite{Hornik89}{4}
\bibcite{Rojas96}{5}
\bibcite{Karlik11}{6}
\bibcite{LeCun15}{7}
\bibcite{Glorot11}{8}
\bibcite{Rumelhart86}{9}
\bibcite{Behler15}{10}
\bibcite{Tuckerman92}{11}
\bibcite{Plimpton95}{12}
\bibcite{Abadi15}{13}
\bibcite{Abadi16}{14}
\bibcite{Qian99}{15}
\bibcite{Duchi11}{16}
\bibcite{Zeiler12}{17}
\bibcite{Kingma14}{18}
\bibcite{Ischtwan94}{19}
\bibcite{Raff05}{20}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  default-pyg-prefix.pygstyle}
