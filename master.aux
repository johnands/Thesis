\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Goals}{1}{section.1.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theory}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Molecular dynamics}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Potential energy surfaces}{3}{subsection.2.1.1}}
\newlabel{forcePES}{{2.2}{4}{Potential energy surfaces}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}LAMMPS}{5}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Installing LAMMPS}{5}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{LAMMPS input script}{6}{section*.3}}
\citation{ref11}
\@writefile{toc}{\contentsline {subsubsection}{LAMMPS structure}{10}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Extending LAMMPS}{10}{section*.5}}
\citation{ref1}
\citation{ref2}
\citation{ref3}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Machine learning}{11}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{11}{section.2.3}}
\citation{ref3}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Neural network\relax }}{12}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NN}{{2.1}{12}{Neural network\relax }{figure.caption.6}{}}
\citation{ref3}
\citation{ref4}
\newlabel{completeNN}{{2.7}{13}{Artificial Neural Networks}{equation.2.3.7}{}}
\newlabel{sigmoidActivationFunction}{{2.8}{13}{Artificial Neural Networks}{equation.2.3.8}{}}
\newlabel{tanhActivationFunction}{{2.9}{13}{Artificial Neural Networks}{equation.2.3.9}{}}
\citation{ref5}
\citation{ref6}
\citation{ref7}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation functions\relax }}{14}{figure.caption.7}}
\newlabel{fig:activations}{{2.2}{14}{Activation functions\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Training}{15}{subsection.2.3.1}}
\newlabel{quadraticCostFunction}{{2.12}{15}{Training}{equation.2.3.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation}{15}{section*.8}}
\newlabel{gradientDescent}{{2.13}{15}{Backpropagation}{equation.2.3.13}{}}
\newlabel{weightChange}{{2.14}{15}{Backpropagation}{equation.2.3.14}{}}
\citation{ref8}
\newlabel{neuronError}{{2.16}{16}{Backpropagation}{equation.2.3.16}{}}
\newlabel{forwardProp}{{2.23}{17}{Backpropagation}{equation.2.3.23}{}}
\newlabel{errorTerms}{{2.26}{17}{Backpropagation}{equation.2.3.26}{}}
\newlabel{backProp}{{2.29}{17}{Backpropagation}{equation.2.3.29}{}}
\citation{ref9}
\citation{ref10}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural networks in molecular dynamics}{19}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}High-dimensional NNPs}{20}{subsection.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax }}{21}{figure.caption.9}}
\newlabel{fig:cutOffNeighbours}{{2.3}{21}{\relax }{figure.caption.9}{}}
\citation{ref9}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Symmetry functions}{22}{subsection.2.4.2}}
\newlabel{cutoffFunction}{{2.41}{22}{Symmetry functions}{equation.2.4.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Plot of the cutoff function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {cutoffFunction}\unskip \@@italiccorr )}} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }}{23}{figure.caption.10}}
\newlabel{fig:cutoffFunction}{{2.4}{23}{Plot of the cutoff function \eqref {cutoffFunction} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }{figure.caption.10}{}}
\newlabel{G1}{{2.42}{24}{Symmetry functions}{equation.2.4.42}{}}
\newlabel{G2}{{2.43}{24}{Symmetry functions}{equation.2.4.43}{}}
\newlabel{G3}{{2.44}{24}{Symmetry functions}{equation.2.4.44}{}}
\newlabel{fig:radialSymmetriFunctions:a}{{\caption@xref {fig:radialSymmetriFunctions:a}{ on input line 937}}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{sub@fig:radialSymmetriFunctions:a}{{}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{fig:radialSymmetriFunctions:b}{{\caption@xref {fig:radialSymmetriFunctions:b}{ on input line 943}}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{sub@fig:radialSymmetriFunctions:b}{{}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{fig:radialSymmetriFunctions:c}{{\caption@xref {fig:radialSymmetriFunctions:c}{ on input line 949}}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{sub@fig:radialSymmetriFunctions:c}{{}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{fig:radialSymmetriFunctions:d}{{\caption@xref {fig:radialSymmetriFunctions:d}{ on input line 954}}{25}{Symmetry functions}{figure.caption.11}{}}
\newlabel{sub@fig:radialSymmetriFunctions:d}{{}{25}{Symmetry functions}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }}{25}{figure.caption.11}}
\newlabel{fig:radialSymmetriFunctions}{{2.5}{25}{Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }{figure.caption.11}{}}
\newlabel{G4}{{2.46}{25}{Symmetry functions}{equation.2.4.46}{}}
\newlabel{G5}{{2.48}{26}{Symmetry functions}{equation.2.4.48}{}}
\newlabel{fig:angularSymmetryFunctions:a}{{2.6a}{26}{\relax }{figure.caption.12}{}}
\newlabel{sub@fig:angularSymmetryFunctions:a}{{a}{26}{\relax }{figure.caption.12}{}}
\newlabel{fig:angularSymmetryFunctions:b}{{2.6b}{26}{\relax }{figure.caption.12}{}}
\newlabel{sub@fig:angularSymmetryFunctions:b}{{b}{26}{\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref  {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref  {fig:angularSymmetryFunctions:b}.\relax }}{26}{figure.caption.12}}
\newlabel{fig:angularSymmetryFunctions}{{2.6}{26}{Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref {fig:angularSymmetryFunctions:b}.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Symmetry functions and forces}{27}{subsection.2.4.3}}
\newlabel{pairForce}{{2.49}{27}{Symmetry functions and forces}{equation.2.4.49}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}TensorFlow}{28}{section.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Convolution}{28}{subsection.2.5.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Verifications and tests}{29}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Time usage}{29}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }}{30}{figure.caption.13}}
\newlabel{fig:timeComparisonEvaluateNetworkTotalScatter}{{3.1}{30}{Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }}{31}{figure.caption.14}}
\newlabel{fig:timeComparisonEvaluateNetwork2}{{3.2}{31}{Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training Lennard-Jones potential}{31}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }}{32}{figure.caption.15}}
\newlabel{fig:errorLJTest}{{3.3}{32}{Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }}{33}{figure.caption.16}}
\newlabel{fig:errorBackPropTest}{{3.4}{33}{Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Many-neighbour Lennard-Jones}{33}{subsection.3.2.1}}
\newlabel{zoomedOut}{{3.5a}{34}{Zoomed out\relax }{figure.caption.17}{}}
\newlabel{sub@zoomedOut}{{a}{34}{Zoomed out\relax }{figure.caption.17}{}}
\newlabel{zoomedIn}{{3.5b}{34}{Zoomed in\relax }{figure.caption.17}{}}
\newlabel{sub@zoomedIn}{{b}{34}{Zoomed in\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Quadratic cost \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticCostFunction}\unskip \@@italiccorr )}} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sigmoidActivationFunction}\unskip \@@italiccorr )}}. \autoref  {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref  {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }}{34}{figure.caption.17}}
\newlabel{fig:trainingManyNeighbourNN}{{3.5}{34}{Quadratic cost \eqref {quadraticCostFunction} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \eqref {sigmoidActivationFunction}. \autoref {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }}{37}{figure.caption.18}}
\newlabel{fig:errorManyNeighbourNN}{{3.6}{37}{Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {.1}Appendix}{39}{section.Alph0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.1}Symmetry functions derivatives}{39}{subsection.Alph0.1.1}}
\newlabel{app:appendixA1}{{.1.1}{39}{Symmetry functions derivatives}{subsection.Alph0.1.1}{}}
\newlabel{cutOffFunctionDerivative}{{4}{39}{Symmetry functions derivatives}{equation.Alph0.1.4}{}}
\newlabel{cutOffFunctionDerivative1}{{5}{39}{Symmetry functions derivatives}{equation.Alph0.1.5}{}}
\newlabel{cutOffFunctionDerivative1}{{6}{39}{Symmetry functions derivatives}{equation.Alph0.1.6}{}}
\newlabel{cutOffFunctionDerivative1}{{8}{39}{Symmetry functions derivatives}{equation.Alph0.1.8}{}}
\newlabel{cutOffFunctionDerivative1}{{9}{39}{Symmetry functions derivatives}{equation.Alph0.1.9}{}}
\newlabel{G4Derivative}{{17}{40}{Symmetry functions derivatives}{equation.Alph0.1.17}{}}
\bibcite{ref1}{1}
\bibcite{ref2}{2}
\bibcite{ref3}{3}
\bibcite{ref4}{4}
\bibcite{ref5}{5}
\bibcite{ref6}{6}
\bibcite{ref7}{7}
\bibcite{ref8}{8}
\bibcite{ref9}{9}
\bibcite{ref10}{10}
\bibcite{ref11}{11}
