\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Goals}{1}{section.1.1}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Theory}{3}{part.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Molecular dynamics}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Potential energy surfaces}{7}{section.2.1}}
\newlabel{sec:potentialEnergySurfaces}{{2.1}{7}{Potential energy surfaces}{section.2.1}{}}
\newlabel{forcePES}{{2.1}{7}{Potential energy surfaces}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Born-Oppenheimer approximation}{8}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Time integration}{9}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Empirical potentials}{9}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Lennard-Jones}{9}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Stillinger-Weber}{9}{subsection.2.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Vashishta}{9}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Advanced theory}{9}{section.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Verlet lists}{9}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Thermostats / ensembles}{9}{subsection.2.4.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine learning}{11}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{McCulloch43}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Machine learning, taken from from http://www.isaziconsulting.co.za/machinelearning.html.\relax }}{12}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:machineLearningDiagram}{{3.1}{12}{Machine learning, taken from from http://www.isaziconsulting.co.za/machinelearning.html.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Artificial neurons}{12}{section.3.1}}
\newlabel{sec:ANN}{{3.1}{12}{Artificial neurons}{section.3.1}{}}
\citation{Rojas96}
\citation{Kriesel07}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots  ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }}{13}{figure.caption.4}}
\newlabel{fig:neuronModel}{{3.2}{13}{Mathematical model of an artificial neuron. The neuron receives input signals $x_i,\dots ,x_n$ from $n$ other neurons. Each signal $x_i$ is associated with a weight $w_i$, and the neuron accumulates all input signals as a weighted sum $u$. This sum is then used as input to its activation function $f$, which serves as the neuron's output signal.\relax }{figure.caption.4}{}}
\newlabel{artificialNeuron}{{3.1}{13}{Artificial neurons}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Biological model of a neuron. Source: \href  {https://askabiologist.asu.edu/neuron-anatomy}{https://askabiologist.asu.edu/neuron-anatomy}\relax }}{14}{figure.caption.6}}
\newlabel{fig:neuronBiological}{{3.3}{14}{Biological model of a neuron. Source: \href {https://askabiologist.asu.edu/neuron-anatomy}{https://askabiologist.asu.edu/neuron-anatomy}\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Neural network types}{14}{section.3.2}}
\citation{LeCun99}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit  {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }}{15}{figure.caption.7}}
\newlabel{fig:networkGeneral}{{3.4}{15}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Each node in a layer is connected to \textit {all} nodes in the subsequent layer, and information only flows forward through the layers, hence the name.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Feed-forward neural networks}{15}{subsection.3.2.1}}
\citation{Rosenblatt58}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Recurrent neural networks}{16}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Other types of networks}{16}{subsection.3.2.3}}
\citation{Hornik89}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multilayer perceptron}{17}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Why multilayer perceptrons?}{17}{subsection.3.3.1}}
\newlabel{sec:whyMLP}{{3.3.1}{17}{Why multilayer perceptrons?}{subsection.3.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are listed.\relax }}{18}{figure.caption.8}}
\newlabel{fig:networkNotation}{{3.5}{18}{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, including their directions. Each connection has a weight $w$ (blue), where the notation explained in the text is applied. Every node is marked with its output $y$ and its associated bias $b$ (green), while all input nodes are labeled with an $x$. Only a few weights and biases are listed.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Mathematical model}{18}{subsection.3.3.2}}
\newlabel{sec:MLPmodel}{{3.3.2}{18}{Mathematical model}{subsection.3.3.2}{}}
\newlabel{artificialNeuron2}{{3.2}{18}{Mathematical model}{equation.3.3.2}{}}
\newlabel{outputLayer1}{{3.4}{19}{Mathematical model}{equation.3.3.4}{}}
\newlabel{generalLayer}{{3.5}{19}{Mathematical model}{equation.3.3.5}{}}
\newlabel{outputLayer2}{{3.7}{19}{Mathematical model}{equation.3.3.7}{}}
\newlabel{completeNN}{{3.10}{20}{Mathematical model}{equation.3.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Activation function of output neuron}{20}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix representation}{20}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively.\relax }}{21}{figure.caption.9}}
\newlabel{fig:activationsFlex}{{3.6}{21}{The mathematical expression for a MLP consists of nested terms of the form $h(x) = c_1 f(c_2 x + c_3) + c_4$, where $f$ is the activation function and $c_i$ are NN parameters. The flexibility of the MLP is shown by adjusting $c_1$, $c_2$, $c_3$ and $c_4$ such that $h(x)$ a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively.\relax }{figure.caption.9}{}}
\citation{Hornik89}
\citation{Rojas96}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Activation functions}{22}{section.3.4}}
\citation{Karlik11}
\citation{LeCun15}
\citation{Glorot11}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }}{23}{figure.caption.12}}
\newlabel{fig:SigmoidActivationFunctions}{{3.7}{23}{Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }{figure.caption.12}{}}
\newlabel{sigmoidActivationFunction}{{3.15}{23}{Activation functions}{equation.3.4.15}{}}
\newlabel{tanhActivationFunction}{{3.16}{23}{Activation functions}{equation.3.4.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }}{24}{figure.caption.13}}
\newlabel{fig:reluActivationFunction}{{3.8}{24}{Two of the most common activation functions for neural networks. Both meet the requirements of the universal approximation theorem.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training}{24}{section.3.5}}
\newlabel{sec:training}{{3.5}{24}{Training}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Cost functions}{25}{subsection.3.5.1}}
\newlabel{generalCost}{{3.18}{25}{Cost functions}{equation.3.5.18}{}}
\newlabel{quadraticCost}{{3.19}{25}{Cost functions}{equation.3.5.19}{}}
\citation{Ruder16}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Optimization}{26}{section.3.6}}
\newlabel{sec:optimization}{{3.6}{26}{Optimization}{section.3.6}{}}
\newlabel{gradientDescent}{{3.20}{26}{Optimization}{equation.3.6.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Gradient descent variants}{26}{subsection.3.6.1}}
\citation{Qian99}
\citation{Duchi11}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Optimization algorithms}{27}{subsection.3.6.2}}
\newlabel{sec:optimizationAlgorithms}{{3.6.2}{27}{Optimization algorithms}{subsection.3.6.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{27}{section*.14}}
\newlabel{Momentum}{{3.21}{27}{Momentum}{equation.3.6.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adagrad}{27}{section*.15}}
\citation{Zeiler12}
\newlabel{Adagrad}{{3.24}{28}{Adagrad}{equation.3.6.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{Adadelta}{28}{section*.16}}
\newlabel{decayingAverageVector}{{3.25}{28}{Adadelta}{equation.3.6.25}{}}
\newlabel{preliminiaryAdadelta}{{3.27}{28}{Adadelta}{equation.3.6.27}{}}
\citation{Kingma14}
\citation{Kingma14}
\citation{Kingma14}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{29}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Which optimizer to use?}{29}{section*.18}}
\citation{Rumelhart86}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Backpropagation}{30}{subsection.3.6.3}}
\newlabel{sec:backprop}{{3.6.3}{30}{Backpropagation}{subsection.3.6.3}{}}
\newlabel{partialDerivatives}{{3.34}{30}{Backpropagation}{equation.3.6.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{1. Forward propagation}{30}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{2. Backward propagation}{31}{section*.20}}
\newlabel{partialDerivativeExpanded}{{3.37}{31}{2. Backward propagation}{equation.3.6.37}{}}
\newlabel{neuronError}{{3.38}{31}{2. Backward propagation}{equation.3.6.38}{}}
\newlabel{derivativeSecondTerm}{{3.40}{31}{2. Backward propagation}{equation.3.6.40}{}}
\newlabel{weightGradient}{{3.41}{31}{2. Backward propagation}{equation.3.6.41}{}}
\newlabel{forwardProp}{{3.42}{31}{2. Backward propagation}{equation.3.6.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref  {fig:neuronModel}. A hidden neuron $j$ recieves a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t to its net input $u_j$.\relax }}{32}{figure.caption.21}}
\newlabel{fig:backprop}{{3.9}{32}{Illustration of the backpropagation algorithm. The direction of information flow is opposite of \autoref {fig:neuronModel}. A hidden neuron $j$ recieves a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate in the backwards direction: the activation function (output) $y_j$ of node $j$ is differentiated w.r.t to its net input $u_j$.\relax }{figure.caption.21}{}}
\newlabel{errorTerms}{{3.45}{32}{2. Backward propagation}{equation.3.6.45}{}}
\newlabel{backprop}{{3.48}{33}{2. Backward propagation}{equation.3.6.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{Matrix notation}{33}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{Training algorithm}{34}{section*.23}}
\citation{Behler15}
\citation{Behler11symmetry}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Neural networks in molecular dynamics}{37}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}High-dimensional NNPs}{38}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \relax }}{39}{figure.caption.24}}
\newlabel{fig:cutOffNeighbours}{{4.1}{39}{\relax }{figure.caption.24}{}}
\newlabel{systemEnergy}{{4.1}{39}{High-dimensional NNPs}{equation.4.1.1}{}}
\citation{Behler15}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Symmetry functions}{40}{subsection.4.1.1}}
\newlabel{sec:symmetryFunctions}{{4.1.1}{40}{Symmetry functions}{subsection.4.1.1}{}}
\newlabel{cutoffFunction}{{4.2}{40}{Symmetry functions}{equation.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Plot of the cutoff function \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {cutoffFunction}\unskip \@@italiccorr )}} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }}{41}{figure.caption.25}}
\newlabel{fig:cutoffFunction}{{4.2}{41}{Plot of the cutoff function \eqref {cutoffFunction} applied in this thesis. This function is used to define a chemical environment around a central atom: only the atoms within the cutoff radius $R_c$ contribute to its energy. These are called neighbouring atoms. The closer a neighbouring atom is, the larger the energy contribution, as is the case for most physical systems.\relax }{figure.caption.25}{}}
\newlabel{G1}{{4.3}{42}{Symmetry functions}{equation.4.1.3}{}}
\newlabel{G2}{{4.4}{42}{Symmetry functions}{equation.4.1.4}{}}
\newlabel{G3}{{4.5}{42}{Symmetry functions}{equation.4.1.5}{}}
\newlabel{fig:radialSymmetriFunctions:a}{{\caption@xref {fig:radialSymmetriFunctions:a}{ on input line 1209}}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{sub@fig:radialSymmetriFunctions:a}{{}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{fig:radialSymmetriFunctions:b}{{\caption@xref {fig:radialSymmetriFunctions:b}{ on input line 1215}}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{sub@fig:radialSymmetriFunctions:b}{{}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{fig:radialSymmetriFunctions:c}{{\caption@xref {fig:radialSymmetriFunctions:c}{ on input line 1221}}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{sub@fig:radialSymmetriFunctions:c}{{}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{fig:radialSymmetriFunctions:d}{{\caption@xref {fig:radialSymmetriFunctions:d}{ on input line 1226}}{43}{Symmetry functions}{figure.caption.26}{}}
\newlabel{sub@fig:radialSymmetriFunctions:d}{{}{43}{Symmetry functions}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }}{43}{figure.caption.26}}
\newlabel{fig:radialSymmetriFunctions}{{4.3}{43}{Radial symmetry functions $G^1$, $G^2$ and $G^3$ for an atom with one neighbour only. A set of such functions represents the radial distrubution of neighbours around a central atom placed at the origin. For $G^2$ and $G^3$ a cutoff $R_c = \SI {11.0}{\angstrom }$ has been used.\relax }{figure.caption.26}{}}
\newlabel{G4}{{4.7}{43}{Symmetry functions}{equation.4.1.7}{}}
\newlabel{G5}{{4.9}{44}{Symmetry functions}{equation.4.1.9}{}}
\newlabel{fig:angularSymmetryFunctions:a}{{4.4a}{44}{\relax }{figure.caption.27}{}}
\newlabel{sub@fig:angularSymmetryFunctions:a}{{a}{44}{\relax }{figure.caption.27}{}}
\newlabel{fig:angularSymmetryFunctions:b}{{4.4b}{44}{\relax }{figure.caption.27}{}}
\newlabel{sub@fig:angularSymmetryFunctions:b}{{b}{44}{\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref  {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref  {fig:angularSymmetryFunctions:b}.\relax }}{44}{figure.caption.27}}
\newlabel{fig:angularSymmetryFunctions}{{4.4}{44}{Angular part of symmetry functions $G^4$ and $G^5$ for an atom with one neighbour only. A set of such functions represents the angular distrubution of neighbours around a central atom placed at the origin. $\lambda = +1$ for \autoref {fig:angularSymmetryFunctions:a}, $\lambda =-1$ for \autoref {fig:angularSymmetryFunctions:b}.\relax }{figure.caption.27}{}}
\citation{Behler11symmetry}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Symmetry functions and forces}{45}{subsection.4.1.2}}
\newlabel{forcePES2}{{4.10}{45}{Symmetry functions and forces}{equation.4.1.10}{}}
\newlabel{forceAtomk}{{4.11}{45}{Symmetry functions and forces}{equation.4.1.11}{}}
\newlabel{forceAtomkChainRule}{{4.12}{45}{Symmetry functions and forces}{equation.4.1.12}{}}
\newlabel{changeOfCoordinates}{{4.13}{46}{Symmetry functions and forces}{equation.4.1.13}{}}
\newlabel{changeOfCoordsSymmetry1}{{4.17}{47}{Symmetry functions and forces}{equation.4.1.17}{}}
\newlabel{changeOfCoordsSymmetry2}{{4.18}{47}{Symmetry functions and forces}{equation.4.1.18}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Implementation and validation}{49}{part.2}}
\citation{Plimpton95}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}LAMMPS}{51}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Installing LAMMPS}{51}{subsection.5.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}LAMMPS input script}{52}{subsection.5.0.2}}
\citation{Tuckerman92}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}LAMMPS structure}{56}{subsection.5.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Extending LAMMPS}{56}{subsection.5.0.4}}
\newlabel{sec:extendingLammps}{{5.0.4}{56}{Extending LAMMPS}{subsection.5.0.4}{}}
\citation{Abadi15}
\citation{Abadi16}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}TensorFlow}{59}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{quadraticEquationSolution}{{6.2}{59}{TensorFlow}{equation.6.0.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Installing TensorFlow}{60}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}TensorFlow basic usage}{60}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Hello world}{61}{subsection.6.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Creating a neural network}{61}{subsection.6.2.2}}
\newlabel{sec:TensorFlowCreatingNN}{{6.2.2}{61}{Creating a neural network}{subsection.6.2.2}{}}
\newlabel{preActivationLayer}{{6.4}{61}{Creating a neural network}{equation.6.2.4}{}}
\newlabel{activationLayer}{{6.5}{61}{Creating a neural network}{equation.6.2.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{Scripts/TensorFlow/networkExample.py}{62}{lstlisting.6.-26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Visualizing the graph}{63}{subsection.6.2.3}}
\@writefile{lol}{\contentsline {lstlisting}{Scripts/TensorFlow/tensorBoardExample.py}{63}{lstlisting.6.-28}}
\newlabel{lst:tensorBoardExample}{{6.2.3}{64}{Visualizing the graph}{lstnumber.-28.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }}{65}{figure.caption.28}}
\newlabel{fig:graphExample}{{6.1}{65}{Example of a computational graph in TensorBoard of a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes.\relax }{figure.caption.28}{}}
\citation{Qian99}
\citation{Duchi11}
\citation{Zeiler12}
\citation{Kingma14}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Training a NN with TensorFlow}{66}{subsection.6.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt  {gradients} computes the gradient of all nodes in the graph, \texttt  {trainStep} implements the algorithm for updating the NN parameters, while \texttt  {beta1\_power} and \texttt  {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }}{68}{figure.caption.29}}
\newlabel{fig:tensorBoardTraining}{{6.2}{68}{Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes this error is included in the graph. The optimizer namespace is expanded to display its contents: \texttt {gradients} computes the gradient of all nodes in the graph, \texttt {trainStep} implements the algorithm for updating the NN parameters, while \texttt {beta1\_power} and \texttt {beta2\_power} are the parameters of the Adam optimizer algorithm.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Training procedure}{71}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Selecting the training data}{71}{section.7.1}}
\citation{Behler11general}
\citation{Behler15}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Iterative molecular dynamics sampling}{72}{subsection.7.1.1}}
\newlabel{sample}{{1}{72}{Iterative molecular dynamics sampling}{Item.19}{}}
\newlabel{refEnergies}{{2}{72}{Iterative molecular dynamics sampling}{Item.20}{}}
\newlabel{NNPalgorithm}{{7}{72}{Iterative molecular dynamics sampling}{Item.25}{}}
\citation{Ischtwan94}
\citation{Raff05}
\citation{Raff05}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Sampling algorithms}{73}{subsection.7.1.2}}
\citation{Raff12}
\citation{Pukrittayakamee09}
\@writefile{toc}{\contentsline {subsubsection}{Initial sampling}{74}{section*.30}}
\newlabel{variableIntervalSampling}{{7.1}{74}{Initial sampling}{equation.7.1.1}{}}
\citation{Pukrittayakamee09}
\newlabel{samplingAlgorithmModified}{{7.2}{75}{Initial sampling}{equation.7.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Distribution of forces for one Si atom in a quartz $\textrm  {SiO}_2$ crystal with and withouth the use of the sampling algorithm \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {samplingAlgorithmModified}\unskip \@@italiccorr )}}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm  {max} = 10$.\relax }}{77}{figure.caption.31}}
\newlabel{fig:forceDistSamplingAlgo}{{7.1}{77}{Distribution of forces for one Si atom in a quartz $\textrm {SiO}_2$ crystal with and withouth the use of the sampling algorithm \eqref {samplingAlgorithmModified}. The crystal consists of 576 atoms, with an initial temperature $T= \SI {1000}{\kelvin }$, run for $N = 10000$ time steps. The atoms are sampled from the microcanonical ensemble with periodic boundary conditions. Sampling algorithm parameters: $\alpha =3.0$ and $\tau _\textrm {max} = 10$.\relax }{figure.caption.31}{}}
\citation{Raff05}
\citation{Behler11general}
\@writefile{toc}{\contentsline {subsubsection}{Iterative sampling}{78}{section*.32}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{79}{section*.33}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Constructing the symmetry function sets}{80}{section.7.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Validation}{83}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Time usage}{83}{section.8.1}}
\newlabel{sec:timeUsage}{{8.1}{83}{Time usage}{section.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }}{84}{figure.caption.34}}
\newlabel{fig:timeComparisonEvaluateNetworkTotalScatter}{{8.1}{84}{Scatter plot of CPU time when evaluating untrained NNs with random weights and sigmoid activation functions using the TF Python API ($T_{TFP}$), the TF C++ API ($T_{TFC}$) and Armadillo ($T_{ARMA}$). $L$ is the number of layers, $N$ is the number of nodes in each hidden layer. All the NNs have one input and one output. The time has been found by averaging over 50 evaluations for each NN architecture. $T_{TFC}/T_{ARMA}$ is also shown in an uncut version to demonstrate how large the time difference is for very small NNs.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }}{85}{figure.caption.35}}
\newlabel{fig:timeComparisonEvaluateNetwork2}{{8.2}{85}{Left column: CPU time comparison for $N=100$. Right column: CPU time comparison for $L=30$.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Training Lennard-Jones potential}{85}{section.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }}{86}{figure.caption.36}}
\newlabel{fig:errorLJTest}{{8.3}{86}{Error of a network trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and the error on the training data interval is calculated, shown on the left. The NN is also written to file for evaluation in C++, shown on the right.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }}{87}{figure.caption.37}}
\newlabel{fig:errorBackPropTest}{{8.4}{87}{Error of the gradient of a NN trained to reproduce the shifted Lennard-Jones potential. The NN is trained in Python and evaluated and differentiated in C++ using the backpropagation algorithm. The result is compared to the analytical derivative of the LJ potential. Only a part of the training interval is shown, the graph is essentially flat after this point\relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Many-neighbour Lennard-Jones}{87}{subsection.8.2.1}}
\newlabel{zoomedOut}{{8.5a}{88}{Zoomed out\relax }{figure.caption.38}{}}
\newlabel{sub@zoomedOut}{{a}{88}{Zoomed out\relax }{figure.caption.38}{}}
\newlabel{zoomedIn}{{8.5b}{88}{Zoomed in\relax }{figure.caption.38}{}}
\newlabel{sub@zoomedIn}{{b}{88}{Zoomed in\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Quadratic cost \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {quadraticCostFunction}\unskip \@@italiccorr )}} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {sigmoidActivationFunction}\unskip \@@italiccorr )}}. \autoref  {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref  {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }}{88}{figure.caption.38}}
\newlabel{fig:trainingManyNeighbourNN}{{8.5}{88}{Quadratic cost \eqref {quadraticCostFunction} of training set and test set for NN trained to yield the sum of the shifted LJ-potential of 20 neighbouring atoms. The NN has 20 inputs, one hidden layer with 200 neurons and one output node. All nodes in the hidden layer have sigmoid activation functions \eqref {sigmoidActivationFunction}. \autoref {zoomedOut} shows the cost for the first 50 000 epochs, while \autoref {zoomedIn} displays the cost for all epochs, but on a smaller scale.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }}{91}{figure.caption.39}}
\newlabel{fig:errorManyNeighbourNN}{{8.6}{91}{Error of a network trained to yield the total energy and force on a central atom from 5 neighbouring atoms. The energy contribution from each neighbour is a shifted LJ potential. Trained for 1e6 epochs, 100 nodes in 2 layers. The energy error is shown on the left, force error on the right.\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Results and discussion}{93}{part.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}NN potential for Si}{95}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sec:SiPotentialResults}{{9}{95}{NN potential for Si}{chapter.9}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}NN potential for SiO2}{97}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendices}{99}{section*.40}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Symmetry functions derivatives}{101}{Appendix.1.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:appendixA1}{{A}{101}{Symmetry functions derivatives}{Appendix.1.A}{}}
\newlabel{cutOffFunctionDerivative}{{A.3}{101}{Symmetry functions derivatives}{equation.1.A.0.3}{}}
\newlabel{cutOffFunctionDerivative1}{{A.4}{101}{Symmetry functions derivatives}{equation.1.A.0.4}{}}
\newlabel{cutOffFunctionDerivative1}{{A.5}{101}{Symmetry functions derivatives}{equation.1.A.0.5}{}}
\newlabel{cutOffFunctionDerivative1}{{A.7}{101}{Symmetry functions derivatives}{equation.1.A.0.7}{}}
\newlabel{cutOffFunctionDerivative1}{{A.8}{102}{Symmetry functions derivatives}{equation.1.A.0.8}{}}
\newlabel{G4Derivative}{{A.16}{102}{Symmetry functions derivatives}{equation.1.A.0.16}{}}
\bibcite{Behler11general}{1}
\bibcite{Behler11symmetry}{2}
\bibcite{Dragly14}{3}
\bibcite{Hornik89}{4}
\bibcite{Rojas96}{5}
\bibcite{Karlik11}{6}
\bibcite{LeCun15}{7}
\bibcite{Glorot11}{8}
\bibcite{Rumelhart86}{9}
\bibcite{Behler15}{10}
\bibcite{Tuckerman92}{11}
\bibcite{Plimpton95}{12}
\bibcite{Abadi15}{13}
\bibcite{Abadi16}{14}
\bibcite{Qian99}{15}
\bibcite{Duchi11}{16}
\bibcite{Zeiler12}{17}
\bibcite{Kingma14}{18}
\bibcite{Ischtwan94}{19}
\bibcite{Raff05}{20}
\bibcite{Raff12}{21}
\bibcite{Pukrittayakamee09}{22}
\bibcite{McCulloch43}{23}
\bibcite{Kriesel07}{24}
\bibcite{LeCun99}{25}
\bibcite{Rosenblatt58}{26}
\bibcite{Ruder16}{27}
\gdef\minted@oldcachelist{,
  default.pygstyle,
  borland.pygstyle,
  default-pyg-prefix.pygstyle,
  B96A2F9C2C21DC9B6C0E9E12BC3292C5B1CFE32A756649A7B3EA379BE3962DF9.pygtex,
  44891C627509FD12201166A00012ABEDB1CFE32A756649A7B3EA379BE3962DF9.pygtex}
