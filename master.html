<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="Generator" content="Kate, the KDE Advanced Text Editor" />
<title>master.tex</title>
</head>
<body>
<pre style='color:#1f1c1b;background-color:#ffffff;'>
<b><span style='color:#800000;'>\documentclass</span></b>[twoside,english]{<span style='color:#0000d0;'>uiofysmaster</span>}

<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>float</span>}	<span style='color:#898887;'>% figurer der man plasserer dem</span>
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>caption</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>verbatim</span>}	<span style='color:#898887;'>% \begin{comment}</span>
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>subcaption</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>siunitx</span>}
<b><span style='color:#800000;'>\usepackage</span></b>[toc,page]{<span style='color:#0000d0;'>appendix</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>minted</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>xcolor</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>amssymb</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>mdframed</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>booktabs</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>enumitem</span>}
<b><span style='color:#800000;'>\usepackage</span></b>{<span style='color:#0000d0;'>upgreek</span>}

<span style='color:#800000;'>\sisetup</span>{range-phrase=-}

<span style='color:#800000;'>\usemintedstyle</span>{borland}

<span style='color:#898887;'>%\definecolor{box}{RGB}{218, 218, 218}</span>
<span style='color:#800000;'>\definecolor</span>{LightGray}{gray}{0.9}

<span style='color:#800000;'>\setminted</span>{ <span style='color:#898887;'>%</span>
 <span style='color:#898887;'>%frame=lines, </span>
 fontsize=<span style='color:#800000;'>\footnotesize</span> 
 <span style='color:#898887;'>%bgcolor=LightGray,</span>
}

<span style='color:#800000;'>\surroundwithmdframed</span>{minted}
<span style='color:#800000;'>\mdfsetup</span>{backgroundcolor=LightGray}

<span style='color:#898887;'>% define lammps code listing</span>
<span style='color:#800000;'>\lstdefinestyle</span>{lammps}
{
  frame=single,
  keywordstyle=<span style='color:#800000;'>\ttfamily</span>,
  keepspaces=true,
  columns=fixed,
  commentstyle=<span style='color:#800000;'>\color</span>{black},
  stringstyle=<span style='color:#800000;'>\color</span>{black},
  tabsize=4,
}

<span style='color:#800000;'>\lstset</span><span style='color:#898887;'>%</span>
{<span style='color:#898887;'>%</span>
 morekeywords={as},<span style='color:#898887;'>% </span>
 basicstyle=<span style='color:#800000;'>\footnotesize</span><span style='color:#898887;'>%</span>
}<span style='color:#898887;'>%</span>

<span style='color:#898887;'>%\bibliography{references}</span>

<span style='color:#800000;'>\author</span>{John-Anders Stende}
<span style='color:#800000;'>\title</span>{Constructing high-dimensional neural network potentials for molecular dynamics}
<span style='color:#800000;'>\date</span>{May 2017}

<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>document</span>}

<span style='color:#800000;'>\maketitle</span>

<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>abstract</span>}
 We implement a method for constructing analytic inter-atomic potentials by fitting artificial neural networks to 
 data from molecular dynamics simulations. The LAMMPS distribution is extended with an algorithm  
 for sampling atomic configurations and energies, and TensorFlow, in combination with the Behler-Parrinello method, 
 are employed to perform the fits. 
 Further, we add a custom potential to LAMMPS that apply trained neural networks in simulations. 
  
 We construct a neural network potential for Si, and its performance is evaluated by comparing the radial distribution function
 and various mechanical properties with Stillinger-Weber simulations and/or experiments (SHOULD I COMPARE WITH EXPERIMENTS?). 
 The crystal structure of Si is reproduced with small deviations, 
 and the bulk modulus, shear modulus and Poisson ratio are measured to be similar to Stillinger-Weber values. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>abstract</span>}

<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>acknowledgements</span>}
 I would like to thank my supervisors Anders Malthe-Sørenssen and Morten Hjorth-Jensen for everything they have taught 
 me during my two years as a computational physics student. You have created a great learning environment that motivates 
 students to <span style='color:#800000;'>\textit</span>{want} to work hard for learning's own sake, which is quite rare. I appreciate the numerous
 talks with Anders, which always left me reassured and encouraged. 
 
 Anders Hafreager has been like an additional supervisor for the whole duration of my thesis work. 
 Thanks for always being available when I asked for guidance. Your extensive knowledge of molecular dynamics 
 and LAMMPS have been of tremendous help. 
 
 I want to thank Morten Ledum and especially H<span style='color:#800000;'>\aa</span>{}kon Treider for all cooperation and discussions. 
 We have gone through a lot of ups and downs regarding symmetry functions and forces, and fixing these problems would have been 
 difficult on my own. I would also like to thank everybody else at the computational physics group for all the enjoyable hours
 playing Mario Kart, along with good conversations. 
 
 A big thanks to my mom, who have been very supportive through all my years of studies, and to my sister, for 
 all the nice dinners at Frederikke. <span style='color:#800000;'>\AA</span>{}KG also deserves appreciation for being a wanted distraction during 
 all my hours in front of the computer. 
 
 Lastly, a huge thanks to my dear Malene for all the patience and support you have granted me. I am very happy to have had you 
 waiting for me at home after a long day at Blindern. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>acknowledgements</span>}

<span style='color:#800000;'>\tableofcontents</span>

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Introduction</span></b>}
The field of artificial intelligence (AI) research has experienced a surge of interest in 
the new millennium, mainly due to the recent success of artificial neural networks (NNs). 
NNs are computer programs that try to mimic biological brains
by giving computers the ability to learn without being explicitly programmed. 
They have beaten previous records in typical AI tasks like image recognition <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Krizhevsky12</span>} and speech recognition
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Hinton12</span>}, and have also shown great promise in natural language understanding <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Collobert11</span>}. 
 
In later years, ANNs have also been employed in the natural sciences, where they have outperformed other machine learning
techniques at analyzing particle accelerator data <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Ciodaro12</span>} and reconstructing brain circuits
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Helmstaedter13</span>}. Further, physicists have developed NNs that can solve the quantum many-body problem
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Carleo17</span>} and even automatically search for new quantum physics experiments <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Krenn16</span>}. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Potentials in molecular dynamics</span></b>}
The main strength of NNs is their ability to learn features and find patterns in data sets. 
This has led to their application in the construction of inter-atomic potentials for molecular dynamics (MD) simulations, 
which is the topic of this thesis.

In MD, we are able to simulate thousands or even millions of atoms over considerable 
time scales (in a microscopic perspective). This is made possible by modelling the atomic interaction forces 
as the gradient of a potential energy surface (PES), and predicting new trajectories according 
to Newtonian dynamics. The PES thus determines the dynamics of the system, and is the 
key factor for a MD simulation to be physically plausible.

A PES can have very different forms and properties. The ideal potential<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{The word ''potential'' is frequently used in place of ''potential energy surface'' in this thesis.}
should be accurate, i.e.<span style='color:#800000;'>\ </span>produce thermodynamic properties that agree with experiments, and be computationally 
efficient to enable large-scale simulations. For many systems, a realistic PES must also be high-dimensional to 
incorporate all relevant degrees of freedom.

There is usually a trade-off between speed and accuracy when it comes to MD simulations. 
The most accurate scheme is to recalculate the PES on-the-fly with a quantum mechanical (QM) method like 
Hartree-Fock or Density Functional Theory. 
This <span style='color:#800000;'>\textit</span>{ab inito} approach to MD is however very computationally expensive, 
and severely restricts the feasible space and time scales of the simulation <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Sherrill10</span>}.   

In <span style='color:#800000;'>\textit</span>{classical} MD, the PES is instead approximated by a <span style='color:#800000;'>\textit</span>{pre-defined} mathematical function. 
The construction of such potentials has been one of the main focus areas in MD research, and there 
are two main strategies. The ''physical'' approach is to start out with a functional form 
that incorporates some of the known physics of the system under investigation. 
The function has a set of parameters that are then fitted to experimental data. 
These <span style='color:#800000;'>\textit</span>{empirical} potentials usually have a quite simple form containing only 
a few parameters, making them fast to evaluate, but also biased and possibly inaccurate. Also, the parameter fitting 
may be difficult and time consuming. Examples are the Lennard-Jones <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Jones24</span>} and the Stillinger-Weber 
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Stillinger85</span>} potentials. 

The ''mathematical'' approach is to employ a QM method to produce a data set of atomic configurations and 
their corresponding potential energies, called a <span style='color:#800000;'>\textit</span>{reference set}. 
Then, a general functional form containing many parameters are fitted to this data set with 
an interpolation algorithm of choice. The use of a very flexible, generic functional form makes this type of potential 
accurate and bias-free. If a small fitting error is achieved, only the error of the underlying QM method 
is present. This enables MD simulations with effective <span style='color:#800000;'>\textit</span>{ab inito} accuracy that are several orders of magnitude
faster than recomputing the PES on-the-fly. However, since no physics can be extracted from its functional form, great 
care must be taken during its construction. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Neural network potentials</span></b>}
Several methods have been proposed to fit data sets of atomic configurations and energies, 
like interpolating moving least squares <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Dawes08</span>}, 
and modified Sheppard interpolation <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Ischtwan94</span>}. Recently, also NNs have been employed for this purpose
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>}, resulting in so-called <span style='color:#800000;'>\textit</span>{neural network potentials} (NNPs). 

NNs have very flexible 
functional forms with an arbitrary number of inputs, outputs and parameters.
The various attempts at constructing NNPs vary significantly in scope and complexity, especially in the number of networks that are used.
Examples of NNPs where a single NN was employed are Raff et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Agrawal06</span>}, who studied the dissociation of 
the <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span> molecule, and Prudente et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Prudente98</span>}, who calculated the vibrational levels of the <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{H}_3^+$</span> ion. 

Using a single NN restricts the dimensionality of the system to a few atoms. Two high-dimensional approaches have 
been developed in parallel by Bholoa et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bholoa07</span>}, and Behler and Parrinello <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11symmetry</span>}. These 
methods are applicable to systems containing an arbitrary number of atoms. 
This thesis concerns itself with the Behler-Parrinello (BP) method for constructing NNPs, described by Raff et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff12</span>}
as the most powerful technique for executing dynamics of large systems using PESs obtained from <span style='color:#800000;'>\textit</span>{ab inito} 
calculations. The BP scheme has been applied to systems like copper <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Artrith12</span>}, sodium <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Eshet12</span>}, and solid-liquid 
interfaces <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Natarajan16</span>}. 

As mentioned above, a NN is a purely mathematical concept, which can pose problems in a physical context. 
There are certain physical symmetries any PES must possess, like translational and rotational invariance
of the energy of a system. Cartesian coordinates are not invariant with respect to these operations, and can therefore not 
be used. The BP method utilizes a set of symmetry functions that ensures the relevant symmetries are satisfied. 

In principle, the method can be used to construct potentials for any chemical system, since no initial biased functional form 
is required. Only the symmetry function set and the reference data differ from system to system, the interpolation procedure itself 
is automated. Further, the NNP can be fitted to arbitrary accuracy if performed correctly. 
Thus, this scheme can be viewed as a multi-scale physics tool that bridges the gap between the 
quantum realm and the classical realm, enabling large-scale MD simulations with <span style='color:#800000;'>\textit</span>{ab inito} accuracy.


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Goals</span></b>}
The main goal of this thesis is to use artificial neural networks and the Behler-Parrinello method to
construct high-dimensional potential energy surfaces and apply them in molecular dynamics simulations.
We will not attempt to construct a PES based on data from <span style='color:#800000;'>\textit</span>{ab inito} calculations, but instead 
perform a demonstration and validation of the complete methodology by reproducing the Stillinger-Weber potential for Si. 

<span style='color:#800000;'>\noindent</span> The main goal can be split into the following intermediate objectives:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}[label=<span style='color:#800000;'>\textbf</span>{<span style='color:#800000;'>\alph*</span>)}]
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textbf</span>{Sample data from MD simulations} <span style='color:#800000;'>\\</span>
 We want to sample atomic configurations and energies from MD simulations with known, empirical potentials. 
 The samples will be assembled to a data set, and neural networks applied to fit a functional form to this data set. 
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span>[<span style='color:#800000;'>\textbf</span>{1)}] <span style='color:#800000;'>\textbf</span>{Develop a method for sampling atom-centered configurations and energies from LAMMPS} <span style='color:#800000;'>\\</span>
 Specifically, we wish to dump the positions of all neighbours inside the cutoff sphere of a central atom,  
 along with the total potential energy of the central atom. LAMMPS does not contain this functionality, a goal is therefore
 to develop such a method an integrate it in the LAMMPS distribution. 
 <span style='color:#800000;'>\item</span>[<span style='color:#800000;'>\textbf</span>{2)}] <span style='color:#800000;'>\textbf</span>{Implement a sampling algorithm}
 Certain statistical properties of the data set is of great importance for a NNP to perform well. 
 Thus, we should add an adequate sampling algorithm to the LAMMPS extension above. 
 <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}

 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textbf</span>{Develop a neural network model for regression}. <span style='color:#800000;'>\\</span>
 We will use the machine learning software TensorFlow to implement feed-forward neural networks as a set of Python scripts. 
 The code will be organized like a library that can fit a functional form to any data set. 
 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textbf</span>{Implement the Behler-Parrinello method} <span style='color:#800000;'>\\</span>
 Develop Python scripts employing the BP symmetry functions to transform the cartesian coordinates of atomic configurations 
 to a set of symmetry values. The code should be embedded in the neural network regression model, and the combination of these 
 scripts should be able to create NNPs for systems with an arbitrary number of chemical elements. 
 We also wish to write code that can analyze and visualize data sets and symmetry function sets. 
 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textbf</span>{Apply neural network potentials in molecular dynamics simulations}. <span style='color:#800000;'>\\</span>
 We need to extend the LAMMPS library with our own neural network potential. This includes finding a way 
 to transfer a NN from Python to C<span style='color:#800000;'>\texttt</span>{++} and evaluate the NN as an analytic potential. 
 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textbf</span>{Develop, apply and validate a NNP for Si} <span style='color:#800000;'>\\</span>
 To demonstrate the methods described in this thesis, we will fit a NN to a data set consisting 
 of samples from Stillinger-Weber simulations of Si. The performance of the NNP will be tested 
 by measuring the radial distribution function, the bulk modulus, the shear modulus and the Poisson ratio, and 
 compare with Stillinger-Weber. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Our contributions</span></b>}
In this thesis, we have decided to employ LAMMPS to perform MD simulations and TensorFlow to train neural networks.
The author has previously written a basic molecular dynamics code<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://github.com/johnands/FYS4460-MD}{github.com/johnands/FYS4460-MD}}, 
which has been employed to do some preliminary testing on 
how to evaluate and differentiate a neural network in C<span style='color:#800000;'>\texttt</span>{++}, that has been trained in Python. However, LAMMPS has been used in 
the final implementation of the neural network potential for speed purposes.

We realized from the beginning that writing our own neural network code would not be worth the effort. 
We wanted to focus on the whole process of constructing a NNP, not just the fitting stage. 
Developing code from scratch would have given deep insights in the underlying concepts of neural networks, 
but this has nonetheless been obtained by careful study of the processes initiated with each TensorFlow command. 

<span style='color:#800000;'>\noindent</span> In this thesis we have:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>itemize</span>}
 <span style='color:#800000;'>\item</span> Extended the LAMMPS library with a custom <span style='color:#800000;'>\texttt</span>{compute} that samples atom-centered configurations and potential 
 energies. This also includes a modified version of a sampling algorithm. 
 <span style='color:#800000;'>\item</span> Developed a method to transfer a trained NN from Python to C<span style='color:#800000;'>\texttt</span>{++} by manually saving all relevant parameters to file. 
 This file is then read in C<span style='color:#800000;'>\texttt</span>{++} and the NN reconstructed as a set of matrices and vectors. 
 <span style='color:#800000;'>\item</span> Extended the LAMMPS library with a custom class to enable the use of NNPs in MD simulations. 
 This has been implemented both manually and with the TensorFlow C<span style='color:#800000;'>\texttt</span>{++} API<span style='color:#898887;'>% </span>
 <span style='color:#800000;'>\footnote</span>{Application Programming Interface.}. 
 To differentiate the network and obtain forces, we have implemented a modified version of the backpropagation algorithm.
 <span style='color:#800000;'>\item</span> Written a set of post-processing and analysis scripts.  
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>itemize</span>}

<span style='color:#800000;'>\noindent</span> All the source code can be found at GitHub<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://github.com/johnands}{github.com/johnands} (pinned repositories).}.




<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Structure of the thesis</span></b>}
This thesis has four parts: The first part consists of an introduction to the theories of 
molecular dynamics and neural networks, in addition to a description of the Behler-Parrinello method.  
The second part provides introductions to the LAMMPS and TensorFlow libraries with implementational details, 
advanced theory on the construction of NNPs, and results demonstrating the complete method. 
The fourth part concludes the text and contains a discussion of possible future work and applications. 




<span style='color:#f00000;'>\part</span>{<b><span style='color:#000000;'>Theory</span></b>}

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Molecular dynamics</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:MD</span>}
Molecular dynamics (MD) is a method to simulate the physical movements of atoms and molecules in gases, liquids and solids. 
It is thus a type of <span style='color:#00a000;'>$N$</span>-body simulation. The atoms are modelled as point-like particles with interactions described
by force fields. Their time-evolution is governed by Newton's equations of motion. MD allows us to study the microscopic
movement of thousands or millions of atoms, enabling the sampling of macroscopic properties such at temperature, pressure, 
diffusion, heat capacity and so on. 

The dynamics of an ensemble of particles is governed by their interactions. 
In MD, the interactions are described
by a force field <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}$</span>, which is defined as the negative gradient of a 
potential energy surface (PES) <span style='color:#00a000;'>$V(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r})$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F} = -</span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'> V(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forcePES}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The PES describes the potential energy of a system for any configuration of atoms 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r} = (</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_1, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_2, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_N)$</span>, and thereby the forces, thus
controlling the behaviour of the system. In other words, the PES is clearly the most important
factor for a MD simulation to be physically plausible. 

In this chapter, we will start out with a discussion on different ways to construct an adequate PES, followed 
by details about time-integration and functions that are commonly used to define a PES. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Potential energy surfaces</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:potentialEnergySurfaces</span>}
We know that atomic interactions are governed by the laws of quantum mechanics (QM). 
In principle, we can solve the time-dependent Schr<span style='color:#800000;'>\&quot;</span>{o}dinger equation to obtain the trajectories of 
an ensemble of atoms. This is however a very computationally expensive task, and is not feasible except for
small systems and time scales. Numerical methods for this purpose scales non-linearly with system size, while
MD simulations scale linearly, if implemented in a certain way. 

An alternative is to solve the time-<span style='color:#800000;'>\textit</span>{independent} Schr<span style='color:#800000;'>\&quot;</span>{o}dinger equation for each time step in a simulation. 
The resulting PES is differentiated to obtain the forces on all atoms, and the new positions and velocities are calculated
by employing Newton's laws. This method is called <span style='color:#800000;'>\textit</span>{ab inito} molecular dynamics. Although this is the most accurate
approach to MD, it also suffers from bad scalability when the system size grows. 

In <span style='color:#800000;'>\textit</span>{classical} MD, the PES is represented by a predefined, mathematical function, i.e.<span style='color:#800000;'>\ </span>we do not have to 
update the PES during the simulation, resulting in better performance. The cost is a loss of accuracy, but for many 
systems it is fully possible to construct a predefined PES that yields the correct thermodynamic properties. 
In this thesis, we present a method to construct potential energy surfaces for classical MD.

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>From quantum mechanics to classical potentials</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:bornOppenheimer</span>}
It it not obvious how an ensemble of atoms, which is a quantum mechanical system, can be simulated using 
laws from classical mechanics. To be able to do this,
we must make the assumption that the motion of atomic nuclei and electrons in a molecule
can be seperated, known as the Born-Oppenheimer approximation. 
Under this assumption, we can solve the electronic Schr<span style='color:#800000;'>\&quot;</span>{o}dinger equation with the nucleonic degrees of freedom as parameters.
The resulting wavefunction serves as a potential in a <span style='color:#800000;'>\textit</span>{nucleonic} Schr<span style='color:#800000;'>\&quot;</span>{o}dinger equation, yielding a PES 
for the nuclear motion. In this way, the positions and velocities of the electrons are baked into the PES, 
allowing us to model the atomic nuclei as point particles that follow Newtonian dynamics. 


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Constructing potential energy surfaces</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:constructingPES</span>}
As mentioned above, we represent the PES by a predefined functional form in classical MD. We assume that this
function can be constructed as a sum of <span style='color:#00a000;'>$n$</span>-body terms <span style='color:#00a000;'>$V_n$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}) </span><span style='color:#606000;'>\approx</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_i^N V_1(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i) + </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i,j}^N V_2(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j) + </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i,j,k}^N V_3(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_k) + </span><span style='color:#606000;'>\dots</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{generalPotential}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where the sums are over all single atoms <span style='color:#00a000;'>$i$</span>, all pairs <span style='color:#00a000;'>$(i,j)$</span>, all triplets <span style='color:#00a000;'>$(i,j,k)$</span> and so on. 
Each term is determined by empirical knowledge of the system, by quantum mechanical calculations or a combination of both. 
The resulting PES is called an empirical, quantum mechanical or semi-empirical potential respectively. 

When constructing empirical potentials, we typically start out with a functional form that captures some of the known, basic properties
of the system under investigation. The function has a set of parameters that are fitted to experimental data, hence the name.
Empirical potentials are often quite simple and only have a few parameters, making their computational performance superior. 
However, their
accuracy is limited by the choice of functional form; there is always a risk that not all the physics of a system
is captured. Further, the parameters need to be fitted specifically for a given system and are typically not transferable 
from one system to another. Also, the fitting process can be difficult and time-consuming. 

For QM potentials, electronic structure calculations are performed for a number of atomic configurations, yielding a data set
of configurations and corresponding potential energies. A very flexible functional form, often containing many parameters, 
are then fitted to this data set. Potentials of this type can in principle be fitted to arbitrary accuracy, and may thus 
be treated as <span style='color:#800000;'>\textit</span>{ab inito} potentials. QM potentials are purely mathematical,
in the sense that no physics can be extracted from their functional form. Therefore, the construction of the potential
has to be done with great care, i.e.<span style='color:#800000;'>\ </span>the data set must contain all relevant configurations for the intended application
of the PES. However, the unbiased nature of QM potentials make them more general than their empirical counterparts,
and the methodology can be applied to any system. 

Semi-empirical potentials are constructed as a combination of the two schemes above. Their general functional form
are based on methods from computational QM, but many approximations are made and some parameters are obtained from empirical data. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Truncation and configuration space</span></b>}
Determining how many terms in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>generalPotential</span>} that is sufficient to 
describe a system adequately is not trivial. Noble gases like argon (Ar) have weak interactions and 
may be well described by a two-body potential <span style='color:#00a000;'>$V_2$</span>, while molecules with strong angular dependencies on their bond, like
silicon (Si), will need more terms. We note that the one-body term <span style='color:#00a000;'>$V_1$</span> is only included if the system 
is experiencing an external force. 

Another challenge is to identify the
configuration space of the system. For three-body potentials and more the set of probable configurations is large, and 
running electronic structure calculations for each configuration is expensive. The problem of identifying the configuration
space is described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:selectingTrainingData</span>}. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Fitting procedure</span></b>}
In this thesis we present a method to construct many-body QM potentials for MD simulations.
An important step in this process is the fitting of the data set of atomic configurations and energies by 
a suitable functional form. Several methods have been proposed, like different types of splines, 
interpolating moving least squares <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Dawes08</span>} and modified Sheppard interpolation <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Ischtwan94</span>}. 
We will use <span style='color:#800000;'>\textit</span>{artificial neural networks} (NNs) for this purpose. 
The basic theory of NNs is presented in chapter 2, while chapter 3 deales with physical symmetries that has to be considered
when constructing neural network potentials (NNPs). 
Finally, in the implementation part, the details of constructing and applying a NNP in MD simulations are described. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Common empirical potentials</span></b>}
Although the method described in this thesis is for the construction of QM potentials, empirical potentials
are useful as test cases and validations of the method. We can define two types of empirical potentials based on how they 
relate to groups of atoms. <span style='color:#800000;'>\textit</span>{Bonded} potentials compute the energy of each (covalent) bond for a predefined set 
of atoms or molecules, while <span style='color:#800000;'>\textit</span>{non-bonded} potentials depend on the spatial configuration of <span style='color:#800000;'>\textit</span>{each possible set}
of particles of a specific kind, e.g.<span style='color:#800000;'>\ </span>all pairs of Ar atoms or all triplets of Si atoms. The total potential <span style='color:#00a000;'>$V_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{tot}$</span>
is the sum of the contributions from these two types of potentials,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{tot}} = V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{bonded}} + V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{non-bonded}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We will only work with non-bonded potentials in this thesis. In the following we will discuss three non-bonded potentials
that are common in the literature, two of which we have employed to validate our PES construction method. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Lennard-Jones</span></b>}
The Lennard-Jones (LJ) potential <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Jones24</span>} is one of the simplest and most widely used potentials. 
It has a simple functional form which is inexpensive to compute, and is able to reproduce weakly interacting systems 
like noble gases well. It is a two-body potential that only depends on the distance <span style='color:#00a000;'>$r_{ij}$</span> between pairs of atoms <span style='color:#00a000;'>$i$</span> and <span style='color:#00a000;'>$j$</span>. 
The general form is (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:LJ</span>})
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}(r_{ij}) = 4</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}{r_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)^{12} - </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}{r_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)^6</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{Lennard-Jones}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where the two parameters <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>$</span> are the depth of the potential well and the zero-point of the potential respectively.
The Lennard-Jones potential captures a basic, physical property of many atomic systems: Atoms repulse each other at short distances
and are attracted to each other at long distances. The first term describes the Pauli repulsion due to overlapping electron orbitals, 
while the second term is the attraction due to the so-called van der Waals force. 
The functional form of the repulsive term has, in contrast to the attractive one, no theoretical justification other than that
it approximates the Pauli repulsion well. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/LJ.pdf</span>}
 <span style='color:#800000;'>\caption</span>{The Lennard-Jones potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>Lennard-Jones</span>} as a function of inter-atomic distance for two different parameter sets. 
 The depth of the potential well (potential
 strength) is given by <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>$</span>, while <span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>$</span> is the inter-atomic distance for which the potential is zero.}
 <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:LJ</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}




<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Calculating total potential energy</span></b>}
Before we move on to three-body potentials, we will briefly explain how the total potential energy of a system is calculated. 
For a two-body potential like LJ, each unique pair of atoms is only counted once, so that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{tot}} = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i&lt;j} V_2(r_{ij}) = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{2}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i</span><span style='color:#606000;'>\neq</span><span style='color:#00a000;'> j} V_2(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{twoBodyPotentialEnergy}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
In the second expression we count all pairs twice, thus a factor <span style='color:#00a000;'>$1/2$</span> is required. 
For the three-body case, the counting is a bit more complicated. An atomic triplet is defined by the three
variables <span style='color:#00a000;'>$(r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}$</span>), where <span style='color:#00a000;'>$r_{ij}$</span> and <span style='color:#00a000;'>$r_{ik}$</span> are the 
two inter-atomic distances to the central atom <span style='color:#00a000;'>$i$</span>, and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}$</span> is the angle between the lines from <span style='color:#00a000;'>$i$</span> to <span style='color:#00a000;'>$j$</span> and 
from <span style='color:#00a000;'>$i$</span> to <span style='color:#00a000;'>$k$</span>. Note that there are only two <span style='color:#800000;'>\textit</span>{independent} variables, since
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij} </span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ik}}{r_{ij}^2r_{ik}^2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:triplets</span>} illustrates that three atoms <span style='color:#00a000;'>$(i, j, k)$</span> define three different triplets depending
on which atom that is the central one. All three triplets have different energies, and must therefore be tallied individually.
If we assume that the potential form is symmetric with respect to exchange of the <span style='color:#00a000;'>$j$</span> and <span style='color:#00a000;'>$k$</span>
indicies (which is not always the case for multielemental systems), we have
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i&lt;j&lt;k}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[V_3^{(1)}(r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}) + V_3^{(2)}(r_{ij}, r_{jk}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{ijk}) + </span>
<span style='color:#00a000;'> V_3^{(3)}(r_{ik}, r_{jk}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{ikj})</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{threeBodyPotentialEnergy1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/triplets.pdf</span>}
 <span style='color:#800000;'>\caption</span>{Three atoms <span style='color:#00a000;'>$(i,j,k)$</span> define three different triplets that generally have different energies. They must therefore 
          be tallied individually when computing the total energy of a system.}
 <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:triplets</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
where <span style='color:#00a000;'>$V_3^{(i)}$</span> is one of the triplets in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:triplets</span>}. Alternatively, we can calculate the energy of one triplet 
at a time and instead sum over all <span style='color:#00a000;'>$j$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j</span><span style='color:#606000;'>\neq</span><span style='color:#00a000;'> i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k&gt;j} V_3 (r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}) </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{threeBodyPotentialEnergy2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This is the convention we are going to adopt in the following. 
The total energy is now written,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{tot}} = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_i</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j&gt;i} V_2(r_{ij}) + </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j</span><span style='color:#606000;'>\neq</span><span style='color:#00a000;'> i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k&gt;j} V_3 (r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}) </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{totalPotentialEnergy}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Stillinger-Weber</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:stillingerWeber</span>}
The Stillinger-Weber (SW) potential was proposed in 1985 <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Stillinger85</span>} for simulating Si atoms. It has a two-body
and a three-body term, and is thus written on the form <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>totalPotentialEnergy</span>}. 
The two-body part consists of a Lennard-Jones factor with variable exponents, together with an exponential decay factor
that ensures zeroing out of the potential at a <span style='color:#800000;'>\textit</span>{cutoff} distance <span style='color:#00a000;'>$r_c = a</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_2(r_{ij}) = A</span><span style='color:#606000;'>\epsilon\left</span><span style='color:#00a000;'>[B</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}{r_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)^p - </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}{r_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)^q</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] </span><span style='color:#606000;'>\exp\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}{r_{ij} - a</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{StillingerWeber2Body}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The significance of the cutoff <span style='color:#00a000;'>$r_c$</span> is explained in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:forceCutoff</span>}. 
Silicon forms 4-coordinated
tetrahedral bonded structures, and the purpose of the three-body component is to enforce the tetrahedral bond angle
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_0 </span><span style='color:#606000;'>\approx</span><span style='color:#00a000;'> 109^</span><span style='color:#606000;'>\circ</span><span style='color:#00a000;'>$</span>, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_3(r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}) = </span><span style='color:#606000;'>\lambda\epsilon</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik} - </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_0]^2</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\exp\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma\sigma</span><span style='color:#00a000;'>}{r_{ij} - a</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span><span style='color:#606000;'>\exp\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma\sigma</span><span style='color:#00a000;'>}{r_{ik} - a</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{StillingerWeber3Body}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We observe that for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik} = </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_0$</span>, the three-body term is zeroed out, making this configuration energetically 
favourable. The expression also contains two exponential decay factors to ensure that the potential goes 
to zero once <span style='color:#00a000;'>$r_{ij}$</span> and/or <span style='color:#00a000;'>$r_{ik}$</span> approach the cutoff. 

SW simulations of solid and liquid silicon agree reasonably well with experimental data <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Stillinger85</span>}.
The potential has also been used on other systems than silicon, for instance in monatomic water models <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Molinero08</span>}. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Vashishta</span></b>}
In 1990, Vashishta et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Vashishta90</span>} suggested a three-body interaction potential for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span>. 
The three-body term is of similar form to that of Stillinger-Weber, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> V_3(r_{ij}, r_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}) </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\:</span>
<span style='color:#00a000;'> &amp;B_{jik}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{[</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik} - </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{0jik}]^2}{1 + C[</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik} - </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{0jik}]^2} </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;</span><span style='color:#606000;'>\exp\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>_{ij}}{r_{ij} - r_{0ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\exp\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>_{ik}}{r_{ik} - r_{0ik}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We note that indicies are included for the parameters, as they are generally different for each pair and triplet combination
of elements; the parameter set must be fitted to e.g.<span style='color:#800000;'>\ </span>(Si,Si,Si) and (Si,O,O) seperately. The angle <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{0jik}$</span> has 
the same purpose as for SW, while <span style='color:#00a000;'>$r_{0ij}$</span> is the cutoff distance for each pair of elements. 

<span style='color:#800000;'>\noindent</span> The two-body part is more complex,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_2(r_{ij}) = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{H_{ij}}{r^{</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>_{ij}}} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{W_{ij}}{r^6} + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{Z_iZ_j}{r}</span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>(-r/</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>_{1,ij}) - </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{D_{ij}}{r^4}</span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>(-r/</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>_{4,ij}) </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where we recognize the two first terms as typical LJ repulsive and attractive components. The other two describe
electrostatic interactions, the third being a so-called screened Coulomb potential for charges <span style='color:#00a000;'>$Z_i$</span> and <span style='color:#00a000;'>$Z_j$</span>. 
We will not expand upon the physical justification of these terms here. 

The Vashishta potential has also been succesfully employed on other multi-component systems like SiC <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Vashishta07</span>}. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Time integration</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:timeIntegration</span>}
To evolve the system forward in time, we need a numerical method to integrate Newton's equations of motion,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t) = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(0) + </span><span style='color:#606000;'>\int</span><span style='color:#00a000;'>_0^t</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}(t)dt </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{equationsOfMotion1} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}(t) = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}(0) + </span><span style='color:#606000;'>\int</span><span style='color:#00a000;'>_0^t</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t)dt</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{equationsOfMotion2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
Various finite different schemes can be used for this purpose, and the choice of algorithm is 
of vital importance to the quality of the simulation. The most important property of an integrator
for use in MD, is long-term energy conservation. MD simulations are often run for millions of time steps, 
and a significant energy drift may cause useless results. Further, all integration schemes approximate 
derivatives as finite differences, introducing truncation errors. Finally, numerical stability is
important to prevent unphysical behaviour on large time scales. 

A good choice is the Velocity-Verlet algorithm, which is
a <span style='color:#800000;'>\textit</span>{symplectic} integrator. Put simply, this means that it preserves the form of Hamilton's equations, thereby
conserving energy<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{Strictly speaking, Velocity-Verlet only conserves a pseudo-Hamiltonian
approaching the true Hamiltonian in the limit of infinitesimal time steps <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Frenkel01</span>}.}.
Employing Velocity-Verlet
allows us to sample the phase space of the microcanonical (NVE) ensemble, and collect 
thermodynamic properties like temperature and pressure.
The steps of the algorithm are,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t/2) &amp;= </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t) + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}(t)}{m}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t}{2} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t) &amp;= </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}(t) + </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t/2)</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t) &amp;= </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t) + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t)}{m}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t}{2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
First, the velocity at half the time step forward <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t/2)$</span> is computed. 
This half-velocity is used to obtain the new positions <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t)$</span>. 
Then we calculate the new accelerations <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}(t+</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t)/m$</span> based on the new positions, which is used
to compute the velocities at a whole time step forward <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}(t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t)$</span>. 

In addition to being symplectic, Velocity-Verlet is time-reversible <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Frenkel01</span>}, which it should be considering that we 
are solving time-reversible equations (<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>equationsOfMotion1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>equationsOfMotion2</span>}).  
It is also computationally inexpensive; the only 
additional calculation compared to the primitive Euler method is the half-velocity. 
The global error in position and velocity is <span style='color:#00a000;'>$O(</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t^2)$</span>, which may not seem very impressive. However, 
this is sufficient for collecting statistical properties, 
our goal is not to reproduce the exact trajectories of the particles. 

Velocity-Verlet is the integrator of choice in LAMMPS (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:lammps</span>}), and thus the 
only scheme used in this work. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Force calculations and cutoff radius</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:forceCutoff</span>}
We need the force on each atom to compute the acceleration that is integrated in the equations of motion
(<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>equationsOfMotion1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>equationsOfMotion2</span>}). 
Using the Lennard-Jones potential as an example, 
the force on atom <span style='color:#00a000;'>$i$</span> from atom <span style='color:#00a000;'>$j$</span> is (<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>forcePES</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>Lennard-Jones</span>}),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}}(r_{ij}) = -</span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'> V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}}(r_{ij}) = </span>
<span style='color:#00a000;'> -24</span><span style='color:#606000;'>\epsilon\left</span><span style='color:#00a000;'>[2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>^{12}}{r_{ij}^{13}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) - </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>^6}{r_{ij}^7}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The force is of the same shape 
as the potential in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:LJ</span>}, which means it rapidly goes to zero. If we choose units so that
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>=</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>=1.0$</span>, the force at a distance <span style='color:#00a000;'>$r_{ij} = 3</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>$</span> is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}}(r_{ij} = 3.0</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\approx</span><span style='color:#00a000;'> 0.01</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This is a general property of systems where only van der Waal-interactions are present: Atoms that 
are far apart do not effectively interact. This can be exploited in MD simulations by introducing 
a cutoff radius <span style='color:#00a000;'>$r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c}$</span>, and only compute forces for atoms displaced by a distance smaller than this cutoff. 
In principle, we have to sum over all pairs of atoms in the system to compute the energy and forces, which 
scales like <span style='color:#00a000;'>$O(N^2)$</span>. With a cutoff, this is reduced to <span style='color:#00a000;'>$O(N)$</span>, and the force is now written,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}}(r_{ij}) = </span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'>  -24</span><span style='color:#606000;'>\epsilon\left</span><span style='color:#00a000;'>[2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>^{12}}{r_{ij}^{13}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) - </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>^6}{r_{ij}^7}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] ,</span>
<span style='color:#00a000;'>  &amp; r_{ij} </span><span style='color:#606000;'>\leq</span><span style='color:#00a000;'> r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  0, &amp; r_{ij} &gt; r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c} </span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forceCutoff}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
which greatly increases the computational performance of MD simulations. In fact, simulations with non-bonded potentials
not including a cutoff is unfeasible except for small systems.

The potential itself <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>Lennard-Jones</span>} is also truncated at <span style='color:#00a000;'>$r_{ij} = r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c}$</span>, creating a discontinuity 
at the cutoff. To avoid this problem, we simply shift the LJ potential upward a little, so that the energy 
is excactly zero at this point. The truncated and shifted form of the potential is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> V_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}}^</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{trunc}(r_{ij}) = </span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'>  V_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}(r_{ij}) - V_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ}(r_c), &amp; r_{ij} </span><span style='color:#606000;'>\leq</span><span style='color:#00a000;'> r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  0, &amp; r_{ij} &gt; r_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{c} </span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{shiftedLJ}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}






<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Machine learning</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:machineLearning</span>}
Machine learning is the science of giving computers the ability to learn without being explicitly programmed. 
The idea is that there exist generic algorithms which can be used to find patterns in a broad class of data sets without 
having to write code specifically for each problem. The algorithm will build its own logic based on the data.  

Machine learning is a subfield of computer science, and is closely related to computational statistics. 
It evolved from the study of pattern recognition in artificial intelligence (AI) research, and has made contributions to
AI tasks like computer vision <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Krizhevsky12</span>}, natural language processing <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Collobert11</span>}
and speech recognition <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Hinton12</span>}. It has also, especially in later years, 
found applications in a wide variety of other areas, including bioinformatics, economy, physics, finance and marketing
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun15</span>}. 

The approaches to machine learning are many, but are often split into two main categories. 
In <span style='color:#800000;'>\textit</span>{supervised learning} we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, <span style='color:#800000;'>\textit</span>{unsupervised learning}
is a method for finding patterns and relationship in data sets withouth any prior knowledge of the system.
Some authours also operate with a third category, namely <span style='color:#800000;'>\textit</span>{reinforcement learning}. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.

Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>itemize</span>}
 <span style='color:#800000;'>\item</span> Classification: Outputs are divided into two or more classes. The goal is to 
 produce a model that assigns inputs into one of these classes. An example is to identify
 digits based on pictures of hand-written ones. Classification is typically supervised learning.  
 <span style='color:#800000;'>\item</span> Regression: Finding a functional relationship between an input data set and a reference data set. 
 The goal is to construct a function that maps input data to continous output values. 
 <span style='color:#800000;'>\item</span> Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.
 It is thus a form of unsupervised learning.  
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>itemize</span>} 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:machineLearningDiagram</span>} gives an (incomplete) overview of machine learning methods with some selected applications.
We will not go into further detail on all these approaches to machine learning, as this work is only concerned with
one specific area: artificial neural networks. The rest of this chapter is devoted to the ideas behind this machine learning
approach. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/MachineLearningDiagram.png</span>}
 <span style='color:#800000;'>\caption</span>{Overview of various approaches to machine learning and their applications.
 Source: <span style='color:#800000;'>\href</span>{http://www.isaziconsulting.co.za/machinelearning.html}{isaziconsulting.co.za}.}
 <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:machineLearningDiagram</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Artificial neurons</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:ANN</span>}
The field of artificial neural networks has a long history of development, and is closely connected with 
the advancement of computer science and computers in general. A model of artificial neurons 
was first developed by McCulloch and Pitts in 1943 <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>McCulloch43</span>} to study signal processing in the brain and 
has later been refined by others. The general idea is to mimic neural networks in the human brain, which
is composed of billions of neurons that communicate with each other by sending electrical signals. 
Each neuron accumulates its incoming signals, 
which must exceed an activation threshold to yield an output. If the threshold is not overcome, the neuron
remains inactive, i.e.<span style='color:#800000;'>\ </span>has zero output.  

This behaviour has inspired a simple mathematical model for an artificial neuron <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Rojas96</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y = f</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^n w_ix_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) = f(u)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{artificialNeuron}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Here, the output <span style='color:#00a000;'>$y$</span> of the neuron is the value of its activation function, which have as input
a weighted sum of signals <span style='color:#00a000;'>$x_i, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> ,x_n$</span> received by <span style='color:#00a000;'>$n$</span> other neurons.
The model is illustrated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:neuronModel</span>}. We will now take a closer look at if and how 
this mathematical model is biologically plausible.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/neuron.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Mathematical model of an artificial neuron. The neuron receives input signals <span style='color:#00a000;'>$x_i,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>,x_n$</span> from
	   <span style='color:#00a000;'>$n$</span> other neurons. Each signal <span style='color:#00a000;'>$x_i$</span> is associated with a weight <span style='color:#00a000;'>$w_i$</span>, and the neuron accumulates
	   all input signals as a weighted sum <span style='color:#00a000;'>$u$</span>. This sum is then used as input to its activation function
	   <span style='color:#00a000;'>$f$</span>, which serves as the neuron's output signal.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:neuronModel</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsection*</span>{<b><span style='color:#000000;'>Biological model</span></b>}
Artificial neurons are designed to mimic certain aspects of their biological counterparts <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kriesel07</span>}. 
A schematic model of a biological nervous cell is depicted in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:neuronBiological</span>}.
The <span style='color:#800000;'>\textit</span>{dendrites} in a neuron acts as the input vector, and allow the cell to receive signals from a large number
of neighbouring neurons. As in the mathematical model, each dendrite is associated with the multiplication of a
''weight value'', which is achieved by an increase or decrease of chemical neurotransmitters that amplifies or
weakens the input signals. The cell can also transmit signal inhibitors (oppositely charged ions) to 
accomplish ''negative'' weight values. 

The positive and negative ions arriving from the dendrites are mixed together in the solution inside the <span style='color:#800000;'>\textit</span>{soma}, 
which corresponds to the summation in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>artificialNeuron</span>}. Finally, the <span style='color:#800000;'>\textit</span>{axon} serves
as the activation function by sampling the electrical potential of this solution. If the potential 
reaches a certain strength, the axon transmits a signal pulse down its own length. The axon is connected 
to another neuron's dendrites (or other parts of the cell), enabling the neurons to communicate. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.9<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/neuron_anatomy.jpg</span>}
  <span style='color:#800000;'>\caption</span>{Biological model of a nervous cell. The various parts of the cell are briefly explained in the text. 
	   Source: <span style='color:#800000;'>\href</span>{https://askabiologist.asu.edu/neuron-anatomy}{askabiologist.asu.edu}.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:neuronBiological</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

This discussion of nervous cells helps, to a certain degree, to validate the mathematical form <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>artificialNeuron</span>}
of artifical neurons. However, we note that the above description is a highly simplified picture of the great complexity
of biological neurons, which is not yet fully understood. Our mathematical neurons are thus only caricatures of nature. 
Next, we will look at how networks of these mathematical objects can be formed to emulate the neural networks
in the brain.

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Neural network types</span></b>}
An artificial neural network (ANN), is a computational model that consists of layers of connected neurons, or <span style='color:#800000;'>\textit</span>{nodes}. 
It is supposed to mimic a biological nervous system by letting each neuron interact with other neurons
by sending signals in the form of mathematical functions between layers. 
Each node is modelled according to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>artificialNeuron</span>}. 
A wide variety of different ANNs have
been developed, but most of them consist of an input layer, an output layer and eventual layers in-between, called
<span style='color:#800000;'>\textit</span>{hidden layers}. All layers can contain an arbitrary number of nodes, and each connection between two nodes
is associated with a weight variable. 

The main factor that seperates the different types are how the neurons are <span style='color:#800000;'>\textit</span>{connected}. 
This section contains a short presentation of some of the most common types of ANNs, before we move on to 
a more detailed description of the ANN architecture used in this thesis. In the following we will refer to artificial 
neural networks simply as neural networks (NNs), i.e.<span style='color:#800000;'>\ </span>not referring to its biological counterpart.  

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Feed-forward neural networks</span></b>}
The feed-forward neural network (FFNN) was the first and simplest type of NN devised. In this network, 
the information moves in only one direction: forward through the layers. An example FFNN is shown
in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkGeneral</span>}, consisting of an input layer, two hidden layers and an output layer. 
Nodes are represented by circles, while the arrows display the connections between the nodes, including the 
direction of information flow. Additionaly, each arrow corresponds to a weight variable, not displayed here. 
We observe that each node in a layer is connected to <span style='color:#800000;'>\textit</span>{all} nodes in the subsequent layer, 
making this a so-called <span style='color:#800000;'>\textit</span>{fully-connected} FFNN. 
This is the type of NN that is used in the present work, 
and will be further investigated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLP</span>}.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/networkGeneral.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers
	   and an output layer. Each node in a layer is connected to <span style='color:#800000;'>\textit</span>{all} nodes in the subsequent layer, 
	   and information only flows forward through the layers, hence the name.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:networkGeneral</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

A different variant of FFNNs are <span style='color:#800000;'>\textit</span>{convolutional neural networks} (CNNs) <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun99</span>}, which have a connectivity pattern
inspired by the animal visual cortex. Individual neurons in the visual cortex only respond to stimuli from
small sub-regions of the visual field, called a receptive field. This makes the neurons well-suited to exploit the strong
spatially local correlation present in natural images. The response of each neuron can be approximated mathematically 
as a convolution operation. 

CNNs emulate the behaviour of neurons in the visual cortex by enforcing a <span style='color:#800000;'>\textit</span>{local} connectivity pattern
between nodes of adjacent layers: Each node
in a convolutional layer is connected only to a subset of the nodes in the previous layer, 
in contrast to the fully-connected FFNN in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkGeneral</span>}.
Often, CNNs 
consist of several convolutional layers that learn local features of the input, with a fully-connected layer at the end, 
which gathers all the local data and produces the outputs. They have wide applications in image and video recognition
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun15</span>}. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Recurrent neural networks</span></b>}
So far we have only mentioned NNs where information flows in one direction: forward. <span style='color:#800000;'>\textit</span>{Recurrent neural networks} on
the other hand, have connections between nodes that form directed <span style='color:#800000;'>\textit</span>{cycles}. This creates a form of 
internal memory which are able to capture information on what has been calculated before; the output is dependent 
on the previous computations. Recurrent NNs make use of sequential information by performing the same task for 
every element in a sequence, where each element depends on previous elements. An example of such information is 
sentences, making recurrent NNs especially well-suited for handwriting and speech recognition.

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Other types of networks</span></b>}
There are many other kinds of NNs that have been developed. One type that is specifically designed for interpolation
in multidimensional space is the radial basis function (RBF) network. RBFs are typically made up of three layers: 
an input layer, a hidden layer with non-linear radial symmetric activation functions and a linear output layer (''linear'' here
means that each node in the output layer has a linear activation function). The layers are normally fully-connected and 
there are no cycles, thus RBFs can be viewed as a type of fully-connected FFNN. They are however usually treated as
a seperate type of NN due the unusual activation functions (common activation functions are presented in
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:activationFunctions</span>}).

Other types of NNs could also be mentioned, but are outside the scope of this work. We will now move on to a detailed description
of how a fully-connected FFNN works, and how it can be used to interpolate data sets. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Multilayer perceptron</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:MLP</span>}
In this thesis we use fully-connected feed-forward neural networks with three
or more layers (an input layer, one or more hidden layers and an output layer), mainly
consisting of neurons that have non-linear activation functions.
Such networks are often called <span style='color:#800000;'>\textit</span>{multilayer perceptrons} (MLPs)<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{The terms ''feed-forward neural network'' and ''multilayer perceptron'' are used interchangeably in the 
literature, although the MLP is just one type of FFNN, namely a fully-connected one with mainly non-linear neurons.}.
The <span style='color:#800000;'>\textit</span>{perceptron} was first introduced by Rosenblatt in 1958 <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Rosenblatt58</span>}, and was a FFNN made up of an input layer
and an ouput layer only. It served as a <span style='color:#800000;'>\textit</span>{binary classifier}, i.e.<span style='color:#800000;'>\ </span>a function that maps a real-valued vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x}$</span> 
to a single binary value <span style='color:#00a000;'>$f(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x})$</span>, employing a linear activation function. The addition of hidden layers and 
non-linear activation functions led to the MLP, which could produce an arbitrary number of continous output values. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Why multilayer perceptrons?</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:whyMLP</span>}
We have chosen to use MLPs to interpolate data sets for the construction of inter-atomic potentials. 
Other NN types could also have been used, but our choice is well justified. According to the
<span style='color:#800000;'>\textit</span>{Universal approximation theorem} <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Hornik89</span>}, a feed-forward neural network with just a single hidden layer containing 
a finite number of neurons can approximate a continous multidimensional function to arbitrary accuracy, 
assuming the activation function for the hidden layer is a ''non-constant, bounded and monotonically-increasing continous function''.
Note that the requirements on the activation function only applies to the hidden layer, the output nodes are always
assumed to be linear, so as to not restrict the range of output values. 

The only requirement on the NN information flow is that it must be feed-forward, which is not satisfied by recurrent NNs.
Anyhow, computing energies and forces in MD simulations at a given time step do not require any knowledge of 
the earlier states of the system, i.e.<span style='color:#800000;'>\ </span>no memory or recurrency is involved.
Further, it is not obvious how local connectivity should be helpful in a MD context, thus
we might as well use the simplest multilayer FFNN available, the MLP. 

We note that this theorem is only applicable to a NN with <span style='color:#800000;'>\textit</span>{one} hidden layer. 
Therefore, we can easily construct a NN 
that employs activation functions which do not satisfy the above requirements, as long as we have at least one layer
with activation functions that <span style='color:#800000;'>\textit</span>{do}. Furthermore, although the universal approximation theorem
lays the theoretical foundation for regression with neural networks, it does not say anything about how things work in practice: 
A NN can still be able to approximate a given function reasonably well without having the flexibility to fit <span style='color:#800000;'>\textit</span>{all other}
functions. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Mathematical model</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}
In an MLP, each node (neuron) is modelled according to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>artificialNeuron</span>}. To increase the flexibility of the NN,
we also add a <span style='color:#800000;'>\textit</span>{bias} <span style='color:#00a000;'>$b$</span> to this model, resulting in 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y = f</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^n w_ix_i + b_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) = f(u)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{artificialNeuron2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
In a FFNN of such neurons, the <span style='color:#800000;'>\textit</span>{inputs} <span style='color:#00a000;'>$x_i$</span>
are the <span style='color:#800000;'>\textit</span>{outputs} of the neurons in the preceding layer. Furthermore, a MLP is fully-connected, 
which means that each neuron receives a weighted sum of the outputs of <span style='color:#800000;'>\textit</span>{all} neurons in the previous layer. 

<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkNotation</span>} displays the same NN as in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkGeneral</span>}, now with all nodes labeled by their
output <span style='color:#00a000;'>$y$</span>, except for the input nodes, which are labeled with <span style='color:#00a000;'>$x$</span>. The input nodes are strictly speaking not 
artifical neurons, as they have no activation function. Their only purpose is to forward the NN inputs to the 
first hidden layer. A few weights <span style='color:#00a000;'>$w$</span> (blue) and biases <span style='color:#00a000;'>$b$</span> (green) are also included, next to the connections and nodes
which they belong to respectively. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/networkWithNotation.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of a fully-connected feed-forward neural network with an input layer, two hidden layers
	   and an output layer. Nodes are depicted as circles, while the arrows shows the connections between neurons, 
	   including their directions. Each connection has a weight <span style='color:#00a000;'>$w$</span> (blue), where the notation explained in the
	   text is applied. Every node is marked with its output <span style='color:#00a000;'>$y$</span> and its associated bias <span style='color:#00a000;'>$b$</span> (green), while all input nodes
	   are labeled with an <span style='color:#00a000;'>$x$</span>. Only a few weights and biases are displayed.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:networkNotation</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
The following notation is introduced: <span style='color:#00a000;'>$y_i^l$</span> is the output of the <span style='color:#00a000;'>$i$</span>-th node in layer <span style='color:#00a000;'>$l$</span>, where <span style='color:#00a000;'>$l=0$</span> is the input layer, 
<span style='color:#00a000;'>$l=1$</span> is the first hidden layer and so on. The same notation applies to the biases. For the weights, we have that
<span style='color:#00a000;'>$w_{ij}^l$</span> is the weight connecting node <span style='color:#00a000;'>$j$</span> in layer <span style='color:#00a000;'>$l-1$</span> with node <span style='color:#00a000;'>$i$</span> in layer <span style='color:#00a000;'>$l$</span><span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{This order of indicies <span style='color:#00a000;'>$i$</span> and <span style='color:#00a000;'>$j$</span> may seem illogical, but will become useful when converting
to matrix equations below.}.
All weights and biases are real-valued numbers.

The MLP in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkNotation</span>} maps the inputs <span style='color:#00a000;'>$x_i$</span> to the output <span style='color:#00a000;'>$y_1^3$</span> via two hidden layers
with an (arbitrary) number of hidden nodes. The hidden neurons have no physical meaning, but have the purpose
of defining the functional form of the NN. In the following we describe how to calculate the output of this MLP. The description
is also valid for a NN with several outputs; the result applies to each ouput node individually. 

First, for each node <span style='color:#00a000;'>$i$</span> in the first hidden layer, we calculate a weighted sum <span style='color:#00a000;'>$u_i^1$</span> of the input coordinates <span style='color:#00a000;'>$x_j$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> u_i^1 = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^2 w_{ij}^1 x_j  + b_i^1 </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This value is the argument to the activation function <span style='color:#00a000;'>$f_1$</span> of each neuron <span style='color:#00a000;'>$i$</span>,
producing the output <span style='color:#00a000;'>$y_i^1$</span> of all neurons in layer 1,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y_i^1 = f_1(u_i^1) = f_1</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^2 w_{ij}^1 x_j  + b_i^1</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{outputLayer1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where we assume that all nodes in the same layer have identical activation functions, hence the notation <span style='color:#00a000;'>$f_l$</span><span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{Note that for the activation functions, the layer indicies are subscripts rather than superscripts, 
as having a single superscript can easily be confused with a square.}.
For an arbitrary node <span style='color:#00a000;'>$i$</span> in layer <span style='color:#00a000;'>$l$</span> this generalizes to
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y_i^l = f_l(u_i^l) = f_l</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^{N_{l-1}} w_{ij}^l y_j^{l-1} + b_i^l</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{generalLayer}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$N_l$</span> is the number of nodes in layer <span style='color:#00a000;'>$l$</span>. When the output of all the nodes in the first hidden layer are computed,
the values of the subsequent layer can be calculated and so forth until the output is obtained. 
The output of neuron <span style='color:#00a000;'>$i$</span> in layer 2 is thus,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> y_i^2 &amp;= f_2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^3 w_{ij}^2 y_j^1 + b_i^2</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= f_2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^3 w_{ij}^2f_1</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k=1}^2 w_{jk}^1 x_k + b_j^1</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) + b_i^2</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{outputLayer2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
where we have substituted <span style='color:#00a000;'>$y_m^1$</span> with <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>outputLayer1</span>}. Finally, the NN output yields,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> y_1^3 &amp;= f_3</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^3 w_{1m}^3 y_j^2 + b_1^3</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= f_3</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^3 w_{1j}^3 f_2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k=1}^3 w_{jk}^2 f_1</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{m=1}^2 w_{km}^1 x_m + b_k^1</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) + b_j^2</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'>  + b_1^3</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
We can generalize this expression to a MLP with <span style='color:#00a000;'>$l$</span> hidden layers. The complete functional form
is,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>flalign</span>}
<span style='color:#00a000;'>&amp;y^{l+1}_1</span><span style='color:#606000;'>\!</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>f_{l+1}</span><span style='color:#606000;'>\!\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\!\sum</span><span style='color:#00a000;'>_{j=1}^{N_l}</span><span style='color:#606000;'>\!</span><span style='color:#00a000;'> w_{1j}^3 f_l</span><span style='color:#606000;'>\!\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\!\sum</span><span style='color:#00a000;'>_{k=1}^{N_{l-1}}</span><span style='color:#606000;'>\!</span><span style='color:#00a000;'> w_{jk}^2 f_{l-1}</span><span style='color:#606000;'>\!\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\!</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>f_1</span><span style='color:#606000;'>\!\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\!\sum</span><span style='color:#00a000;'>_{n=1}^{N_0} </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>w_{mn}^1 x_n</span><span style='color:#606000;'>\!</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>b_m^1</span><span style='color:#606000;'>\!\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\!\dots</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\!\right</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>+ </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>b_k^2</span><span style='color:#606000;'>\!\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>+ </span><span style='color:#606000;'>\!</span><span style='color:#00a000;'>b_1^3</span><span style='color:#606000;'>\!\right</span><span style='color:#00a000;'>] &amp;&amp;</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{completeNN}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>flalign</span>}
which illustrates a basic property of MLPs: The only independent variables are the input values <span style='color:#00a000;'>$x_n$</span>. 
This confirms that a MLP,
despite its quite convoluted mathematical form, is nothing more than an analytic function, specifically a 
mapping of real-valued vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^n </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^m$</span>. 
In our example (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkNotation</span>}), <span style='color:#00a000;'>$n=2$</span> and <span style='color:#00a000;'>$m=1$</span>. Consequentially, 
the number of input and output values of the function we want to fit must be equal to the number of inputs and outputs of our MLP.  

Furthermore, the flexibility and universality of a MLP can be illustrated by realizing that 
the expression <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>completeNN</span>} is essentially a nested sum of scaled activation functions of the form
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> h(x) = c_1 f(c_2 x + c_3) + c_4</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where the parameters <span style='color:#00a000;'>$c_i$</span> are weights and biases. By adjusting these parameters, the activation functions
can be shifted up and down or left and right, change slope or be rescaled (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:activationsFlex</span>}), 
which is the key to the flexibility of a NN. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/activationFlex.pdf</span>}
  <span style='color:#800000;'>\caption</span>{The mathematical expression for a MLP consists of nested terms of the form <span style='color:#00a000;'>$h(x) = c_1 f(c_2 x + c_3) + c_4$</span>
           <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>completeNN</span>}, where <span style='color:#00a000;'>$f$</span> is the activation function
	    and <span style='color:#00a000;'>$c_i$</span> are NN parameters. The flexibility of the MLP is shown by adjusting <span style='color:#00a000;'>$c_1$</span>, <span style='color:#00a000;'>$c_2$</span>, <span style='color:#00a000;'>$c_3$</span> and <span style='color:#00a000;'>$c_4$</span> such that
	    <span style='color:#00a000;'>$h(x)$</span> a) is scaled, b) has a change of slope, c) is shifted left and right, d) is shifted up and down, respectively. 
	    Here <span style='color:#00a000;'>$f$</span> is the hyperbolic tangent <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>tanhActivationFunction</span>}. Reproduced from Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>}.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:activationsFlex</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Activation function of output neuron</span></b>}
We stated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:whyMLP</span>} that the output neuron always have a linear activation function for regression to avoid
restricting the output values. This is a requirement for a MLP to operate as a mapping 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^n </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^m$</span>. In this thesis, we will exclusively use a
specific linear activation function for the output neuron <span style='color:#00a000;'>$o$</span>, namely the identity function <span style='color:#00a000;'>$f(x) = x$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f_o = f(u_o) = u_o</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{outputActivation}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}



<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Matrix-vector notation</span></b>}
We can introduce a more convenient notation for the activations in a NN. 
If we look at the structure of the NN in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkNotation</span>}, we realize that all the weights connecting 
two adjacent layers can be represented as a matrix <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l$</span>, where the <span style='color:#00a000;'>$(i,j)$</span>-th element is 
is the weight <span style='color:#00a000;'>$w^l_{ij}$</span> connecting connecting node <span style='color:#00a000;'>$j$</span>
in layer <span style='color:#00a000;'>$l-1$</span> with node <span style='color:#00a000;'>$i$</span> in layer <span style='color:#00a000;'>$l$</span>, as above. 

Additionaly, we can represent the biases and activations
as layer-wise column vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l$</span>, so that the <span style='color:#00a000;'>$i$</span>-th element of each vector 
is the bias <span style='color:#00a000;'>$b_i^l$</span> and activation <span style='color:#00a000;'>$y_i^l$</span> of node <span style='color:#00a000;'>$i$</span> in layer <span style='color:#00a000;'>$l$</span> respectively. 

We have that <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l$</span> is a <span style='color:#00a000;'>$N_{l-1} </span><span style='color:#606000;'>\times</span><span style='color:#00a000;'> N_l$</span> matrix, while <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l$</span> are <span style='color:#00a000;'>$N_l </span><span style='color:#606000;'>\times</span><span style='color:#00a000;'> 1$</span> column vectors. 
With this notation, the sum in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>outputLayer2</span>} becomes a matrix-vector multiplication, and we can write
the equation for the activations of hidden layer 2 in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:networkNotation</span>} as<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{For the same reasons as the activation functions above, the layer indicies are changed from superscripts to subscripts
          with this matrix-vector notation.}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_2 = f_2(</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_2 </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{1} + </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_{2}) = </span>
<span style='color:#00a000;'> f_2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>array</span>}<span style='color:#00a000;'>{ccc}</span>
<span style='color:#00a000;'>    w^2_{11} &amp;w^2_{12} &amp;w^2_{13} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>    w^2_{21} &amp;w^2_{22} &amp;w^2_{23} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>    w^2_{31} &amp;w^2_{32} &amp;w^2_{33} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>    </span><span style='color:#f00000;'>\end</span>{<span style='color:#00a000;'>array} </span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] </span><span style='color:#606000;'>\cdot</span>
<span style='color:#00a000;'>    </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>array</span>}<span style='color:#00a000;'>{c}</span>
<span style='color:#00a000;'>           y^1_1 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>           y^1_2 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>           y^1_3 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>          </span><span style='color:#f00000;'>\end</span>{<span style='color:#00a000;'>array}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] + </span>
<span style='color:#00a000;'>    </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>array</span>}<span style='color:#00a000;'>{c}</span>
<span style='color:#00a000;'>           b^2_1 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>           b^2_2 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>           b^2_3 </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>          </span><span style='color:#f00000;'>\end</span>{<span style='color:#00a000;'>array}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and we see that the activation of node <span style='color:#00a000;'>$i$</span> in layer 2 is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y^2_i = f_2</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>(w^2_{i1}y^1_1 + w^2_{i2}y^1_2 + w^2_{i3}y^1_3 + b^2_i</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>) = </span>
<span style='color:#00a000;'> f_2</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^3 w^2_{ij} y_j^1 + b^2_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>} 
which is in accordance with <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>outputLayer2</span>}. Note that
This is not just a convenient and compact notation, but also 
a useful and intuitive way to think about MLPs: The output is calculated by a series of matrix-vector multiplications
and vector additions that are used as input to the activation functions. For each operation 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{l-1}$</span> we move forward one layer. Furthermore, it is the easiest way to implement NNs with 
TensorFlow, which is discussed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:TensorFlowCreatingNN</span>}. 



<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Activation functions</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:activationFunctions</span>}
A property that characterizes a NN, other than its connectivity, is the choice of activation function(s). 
As described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:whyMLP</span>}, the following restrictions are imposed on an activation function for a FFNN
to fulfill the universal approximation theorem <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Hornik89</span>}:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Non-constant
 <span style='color:#800000;'>\item</span> Bounded
 <span style='color:#800000;'>\item</span> Monotonically-increasing
 <span style='color:#800000;'>\item</span> Continous
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
We realize that the second requirement excludes all linear functions. Furthermore, in a MLP with only linear activation functions, each 
layer simply performs a linear transformation of its inputs. Consequentially, regardless of the number of layers, 
the output of the NN will be nothing but a linear function of the inputs. Thus we need to introduce some kind of 
non-linearity to the NN to be able to fit non-linear functions. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/activationFunctions.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Two of the most common activation functions for neural networks, the sigmoid <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>sigmoidActivationFunction</span>} and 
           the hyperbolic tangent <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>tanhActivationFunction</span>}. Both meet the requirements
	   of the universal approximation theorem.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:SigmoidActivationFunctions</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

A group of non-linear functions that meet the above requirements are the <span style='color:#800000;'>\textit</span>{sigmoid} functions. 
The two sigmoid functions that are most commonly used as activation functions <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Rojas96</span>} in 
neural networks are the logistic function
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f(x) = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{1 + e^{-x}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{sigmoidActivationFunction}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and the hyperbolic tangent
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f(x) = </span><span style='color:#606000;'>\tanh</span><span style='color:#00a000;'>(x)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{tanhActivationFunction}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The logistic functon is often referred to as ''the sigmoid'', which is the naming convention we will use 
in the following. Both functions are depicted in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:SigmoidActivationFunctions</span>}. We observe that they
are bounded above by 1, while the lower bound is -1 and 0 for the hyperbolic tangent and sigmoid respectively. This property 
keeps the activations from diverging. 

The sigmoid are more biologically plausible because 
the output of inactive neurons are zero. Such activation function are called <span style='color:#800000;'>\textit</span>{one-sided}. However,
it has been shown <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Karlik11</span>} that the hyperbolic tangent 
performs better than the sigmoid for training MLPs. This will be further discussed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:hyperParamsActFunctions</span>}. 
Nevertheless, one should assess the problem 
at hand when deciding what activation function to use; the performance can vary from problem to problem. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/reluActivation.pdf</span>}
  <span style='color:#800000;'>\caption</span>{The rectifier <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>reluActivationFunction</span>} has become the most popular activation function for 
           deep convolutional NNs.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:reluActivationFunction</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#800000;'>\noindent</span> In later years, the rectifier function (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:reluActivationFunction</span>})
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f(x) = </span><span style='color:#606000;'>\max</span><span style='color:#00a000;'>(0,x)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{reluActivationFunction}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
has become the most popular <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun15</span>} for <span style='color:#800000;'>\textit</span>{deep neural networks}, i.e.<span style='color:#800000;'>\ </span>NNs with several hidden layers<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{There is no consensus in the literature on how many hidden layers a NN must possess in order to qualify as ''deep'', 
but there seems to be a tendency towards setting the minimum number of layers to three.}.
It has been argued to be even more biologically plausible than the sigmoid and also perform better
than the hyperbolic tangent, especially for deep, convolutional NNs <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Glorot11</span>}. Even though the rectifier is a
piecewise <span style='color:#800000;'>\textit</span>{linear} function, it effectively behaves as a non-linearity. However, it does not have an upper bound, and
therefore does not satisfy the universal approximator requirements. Further, the function values can potentially blow up. 
These problems are solved in practice by employing various teqhniques, illustrating that the universal approximation theorem
should not be used as an absolute guideline for the construction of activation functions. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Training</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:training</span>}
In the beginning of this chapter we defined machine learning as the ''science of giving computers the ability to learn without
being explicitly programmed.'' In the neural network case, this learning is achieved by iteratively feeding the network with data. 
With the help of certain learning algorithms, the network will then automatically adjust its parameters, i.e.<span style='color:#800000;'>\ </span>the weights and biases, 
to find patterns in this data<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{in the following, ''weights'' will often be equivalent to all the NN parameters, including the biases.}.
The procedure depends on what learning paradigm we work with
(unsupervised or supervised learning), and the type of NN we use. 

In this thesis we employ NNs for regression: We want to interpolate data sets og atomic coordinates and energies.
The goal is to adjust the weights of a MLP so that it accurately represents a function that maps
atomic coordinates to the corresponding potential energies. Interpolation with NNs is essentially an optimization problem:
The weights are typically initialized as random numbers, and then modified iteratively to minimize the error
to a set of expected output values, in our case energies. 
This procedure is called <span style='color:#800000;'>\textit</span>{training}. 

Thus, the input values to the function that we want to fit are atomic configurations,
while the output values are the total energies of these configurations. 
Together they form what we refer to as a <span style='color:#800000;'>\textit</span>{data set} or a <span style='color:#800000;'>\textit</span>{reference set}. 
The output values are also known as the <span style='color:#800000;'>\textit</span>{target values}. 
The input data <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{X}$</span> and output data <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Y}$</span> are represented as matrices, where the <span style='color:#00a000;'>$i$</span>-th row of X and Y, 
denoted <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{X}_{i*}$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Y}_{i*}$</span> respectively, 
together form a <span style='color:#800000;'>\textit</span>{training example}. The combined data set consists of <span style='color:#00a000;'>$N$</span> such training examples. 

Note that the number of columns of X and Y are equal to the number of inputs and outputs of our MLP respectively. 
This is a restatement of the observation made in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}: The number of inputs and outputs of our MLP must be equal 
to the number of input and output values of the function we are trying to fit. In the following we assume that the MLP only has one 
output, thus each row of Y is a single number, denoted <span style='color:#00a000;'>$Y_{i*} = Y_i$</span>. 


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Cost functions</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>costFunctions</span>}
To minimize the error, we need a way to <span style='color:#800000;'>\textit</span>{define} it. 
As in mathematical optimization, the error is represented by an objective function, also called <span style='color:#800000;'>\textit</span>{loss} function
or <span style='color:#800000;'>\textit</span>{cost} function. Training a NN therefore amounts to the minimization of this function, which can be written in the 
following general way,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\Gamma\bigr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\{\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>, </span><span style='color:#606000;'>\{\vec</span><span style='color:#00a000;'>{b}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>, </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{X}, </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Y}</span><span style='color:#606000;'>\bigr</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{generalCost}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\vec</span><span style='color:#00a000;'>{b}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> are all the weights and biases of the NN respectively. 
The value of this function is a measure of how well the NN is able to map <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{X} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Y}$</span>. 
By adjusting <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\vec</span><span style='color:#00a000;'>{b}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span>, we try to minimize the value of this function.  

The standard cost function used in regression with NNs is the mean-square error,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{2N}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^N (Y_i - y_i)^2</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{quadraticCost}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$y_i = y_i(W, B, X_{i*})$</span> is the value predicted by the NN for training example <span style='color:#00a000;'>$X_{i*}$</span>. 
For a MLP with more than one output, <span style='color:#00a000;'>$Y_i$</span> and <span style='color:#00a000;'>$y_i$</span> are vectors. 
The constant <span style='color:#00a000;'>$1/2$</span> is included to cancel out the exponent when this function is differentiated at a later stage
(<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:backprop</span>}).

A variety of other cost functions are used for other machine learning tasks. In classification the output is often binary, 
hence the mean-square error function is inadequate. We will not go into further detail on these cost functions here, but 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:hyperParamsCostFunction</span>} provides a discussion on an extension of <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>} that is relevant for 
the construction of NNPs. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Optimization</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:optimization</span>}
There are a large number of algorithms that can be used to determine the set of weights minimizing the 
cost function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>}. 
Different kinds of gradient descent-based methods are widely used, while higher order methods like
the conjugate gradient algorithm or Newton's method are rarely seen in the literature because they are 
too computationally expensive for large NNs and data sets. Therefore, only first-order methods will be discussed here. 

The idea behind gradient descent methods is to minimize a function by 
iteratively taking steps in the direction of steepest descent towards a minimum in parameter space. 
This direction is defined as the negative gradient of the function with respect to 
all its parameters. If we define <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^d$</span> as a vector containing all the weights and biases
of a MLP, we get the following iterative scheme,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k+1} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k} - </span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'>_{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k} </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{gradientDescent}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span> is a step size, called the <span style='color:#800000;'>\textit</span>{learning rate} in the context of machine learning. The process is started
by initializing the parameters as random numbers. The value of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span> is of great importance for the algorithm to converge,
and is allowed to change at every iteration. Note that convergence to 
a minimum is not guaranteed withouth certain assumptions on the function <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>$</span> and the particular choices of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span>.
Also, the obtained minimum is generally local, not global. 
Different ways to update <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span> is discussed below.


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Gradient descent variants</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:gradientDescentVariants</span>}
There are three versions of gradient descent <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Ruder16</span>}, which differ in 
the number of training examples we present to the NN before updating the parameters. 
According to neural network terminology, the process of adjusting the parameters based on all the training examples,
either in batches or all at once, is called an <span style='color:#800000;'>\textit</span>{epoch}. 

In <span style='color:#800000;'>\textit</span>{batch} gradient descent, 
we compute the gradient of the cost function for the <span style='color:#800000;'>\textit</span>{whole} data set before updating, also called 
<span style='color:#800000;'>\textit</span>{offline learning}. 
This approach can be very slow and is intractable for datasets that do not fit in memory. Furthermore, there is a risk
of performing redundant computations if many similar examples are present in the data set.

In contrast, <span style='color:#800000;'>\textit</span>{stochastic} gradient descent (SDG) performs a parameter update for <span style='color:#800000;'>\textit</span>{each} training example. 
SDG avoids redundant gradient calculations and is therefore faster than batch gradient descent. However, the accuracy 
of each update is lower compared to offline learning, which can lead to quite large error oscillations. The concept of having  
more than one parameter update per epoch is called <span style='color:#800000;'>\textit</span>{online learning}. 

Finally, we have <span style='color:#800000;'>\textit</span>{mini-batch} gradient descent, which is a mix of the two other approaches. The reference set 
is divided into <span style='color:#00a000;'>$n$</span> equally sized mini-batches and a parameter update is performed for each mini-batch. This is usually the
algorithm of choice, as it has the optimal trade-off between speed and accuracy. Normal mini-batch sizes range
between 50 and 256, but should in some degree be tailored to each problem. 

We realize that SDG can be seen as 
a variant of mini-batch gradient descent with a mini-batch size of 1. In the literature, these two methods 
are often collectively referred to as SDG.


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Optimization algorithms</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:optimizationAlgorithms</span>}
In the following we will outline different optimization algorithms that are widely used in neural network research. 
They are all variations on the update rule <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>gradientDescent</span>}. The main focus of the methods is to find
a proper learning rate. A learning rate that is too small may lead to very slow convergence, while 
a learning rate that is too large can cause the loss function to fluctuate around the minimum or even diverge. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Momentum</span></b>}
Near local minima, the surface area of the cost function (in parameter space) often curve much more steeply in one 
direction than in another, forming ravines. SDG will have slow converge in such regions, as it will oscillate across the slopes
while having slow progress along the bottom of the ravine. The Momentum method <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Qian99</span>} avoids this problem
by accelerating SDG in the downwards direction, while damping the oscillations. The update rule is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_{k} &amp;= </span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_{k-1} + </span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'>_k </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k+1} &amp;= </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_k</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#606000;'>\label</span><span style='color:#00a000;'>{Momentum}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> is the momentum term, which is usually set to 0.9 or a similar value. The new update vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_k$</span> 
is formed by adding a fraction <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> of the previous update vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_{k-1}$</span> to the gradient. In this way, the magnitude
of the update decreases for parameters (dimensions) whose gradients change direction, while it increases for parameters
whose gradients points in the same direction as in the previous step. This helps to push SDG downhill towards the minima. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Adagrad</span></b>}
In SDG and Momentum, we have one learning rate <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span> that is applied to all the parameters. 
Ideally, the learning rate should be adapted to each individual parameter to perform larger or smaller updates
depending on their importance. Adagrad <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Duchi11</span>} does just that. For brevity, we set
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> g_{k,i} = </span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'>_{</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_i} </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> (</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
to be the gradient of the loss function w.r.t. parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_i$</span> at step <span style='color:#00a000;'>$k$</span>. Adagrad adjusts the general learning rate <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span>
at each step <span style='color:#00a000;'>$k$</span> for every parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_i$</span> based on the past gradients for that parameter,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{k+1,i} = </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{k,i} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{G_{k,ii} + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>}} </span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'> g_{k,i}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_k </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathbb</span><span style='color:#00a000;'>{R}^{d </span><span style='color:#606000;'>\times</span><span style='color:#00a000;'> d}$</span> is a diagonal matrix where each diagonal element <span style='color:#00a000;'>$G_{k,ii}$</span> is the sum of squares
of the gradient with respect to <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_i$</span> up to step <span style='color:#00a000;'>$k$</span>. The smoothing constant <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\sim</span><span style='color:#00a000;'> 10^{-8}$</span> is present to avoid
division by zero. The vectorized version of this equation yields,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k+1} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{diag}(</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_k) + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>}} </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{Adagrad}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'>$</span> stands for element-wise multiplication. In other words, we do not have to manually tune the learning rate, 
Adagrad does that for us. A weakness of this method is that the learning rates inevitably shrinks for each step, 
resulting in arbitrary small values, at which point the learning stops.

For the rest of <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:optimizationAlgorithms</span>}, we assume for brevity that all vector operations are element-wise, i.e.<span style='color:#800000;'>\</span>
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}</span><span style='color:#606000;'>\,\vec</span><span style='color:#00a000;'>{h} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g} </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{h}$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2 = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g} </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}$</span>.

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Adadelta</span></b>}
Adadelta <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Zeiler12</span>} is an extension of Adagrad that reduces the rate at which the learning rate decreases. 
The sum of all past squared gradients is replaced by a <span style='color:#800000;'>\textit</span>{exponentially decaying average} of all the previous squared gradients.
We introduce the vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k$</span> containing the decaying averages of the gradient
with respect to all the parameters at step <span style='color:#00a000;'>$k$</span>. 
This vector is defined recursively
as a weighted average of the previous averages <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_{k-1}$</span> and the current gradient <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k = </span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_{k-1} + (1 - </span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2_k</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{decayingAverageVector}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> is a decay constant similar to that in the Momentum method, and is usually set to the same value (0.9). 
We also define a new parameter update vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k = -</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_k$</span> so that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k+1} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k  + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Replacing the vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{diag}(</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_k)$</span> in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>Adagrad</span>} with the decaying average vector <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>decayingAverageVector</span>}
yields the following parameter update vector,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k = -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>}} </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{preliminiaryAdadelta}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Additionally, the learning rate <span style='color:#00a000;'>$</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>$</span> is replaced by a decaying average of previous squared parameter updates
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\Delta\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}^2]$</span> up to step <span style='color:#00a000;'>$k-1$</span>, yielding the Adadelta update rule,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{v}_k = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\Delta\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}^2]_{k-1} + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>}}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>}} </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The final replacement is done to obtain correct units for the update vector and to eliminate the learning rate from the equation. 
Thus, we do not even need to set a default learning rate with Adadelta. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Adam</span></b>}
The final algorithm we are going to discuss is Adapte Moment Estimation (Adam) <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kingma14</span>}. This method
computes adaptive learning rates for each parameter by storing 
exponentially decaying averages of both the gradients <span style='color:#800000;'>\textit</span>{and} 
the squared gradients <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>decayingAverageVector</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}]_k &amp;= </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1 </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}]_{k-1} + (1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1) </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k &amp;= </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_{k-1} + (1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2) </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2_k</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We set <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_k = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}]_k$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_k = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{E}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2]_k$</span> so that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_k &amp;= </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1 </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_{k-1} + (1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1) </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_k &amp;= </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2 </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_{k-1} + (1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2) </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}^2_k</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_k$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_k$</span> are estimates of the first moment (the mean) and the second moment (the uncentered variance)
of the gradients respectively, hence the name of the method. The elements of these two vectors are initialized as zeros, 
which make them biased towards zero, especially during the first time steps. This bias is counteracted by
computing bias-corrected version of the vectors (derived in <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kingma14</span>}),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\hat</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}}_k &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_k}{1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1^k} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\hat</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}}_k &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_k}{1 - </span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2^k}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The parameter update vector for Adam is obtained by replacing the gradient <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{g}_k$</span> with <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}_k$</span> in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>preliminiaryAdadelta</span>},
yielding the update rule,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_{k+1} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\uptheta</span><span style='color:#00a000;'>}_k - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\hat</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}}_k} + </span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>} </span><span style='color:#606000;'>\hat</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{a}}_k</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{adamUpdateRule}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The default values of the hyperparameters are <span style='color:#00a000;'>$</span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_1 = 0.9$</span>, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>_2 = 0.999$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'> = 10^{-8}$</span>. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Which optimizer to use?</span></b>}
There is no general answer to the question stated above. The adaptive learning rate methods
like Adagrad, Adadelta and Adam are generally more robust because they do not rely on manual fine-tuning of the learning rate. 
Also, an adaptive learning rate generally handles <span style='color:#800000;'>\textit</span>{sparse} data better, i.e.<span style='color:#800000;'>\ </span>data where some learning features
are poorly represented. 

Adam has been proven to perform favourably over the other optimization methods for certain applications like classification
with MLPs and convolutional NNs <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kingma14</span>}. However, the performance of various optimizers should be tested on 
the problem at hand to determine which method that works best. 




<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Backpropagation</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:backprop</span>}
In the preceding sections we have discussed different optimizers that implements various update rules for 
the parameters of a loss function. In our case, these parameters are the weights and biases of a neural network
and the cost function <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>$</span> is the mean-square-error <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{2} (Y_i - y_i)^2</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
here written as a function of only <span style='color:#800000;'>\textit</span>{one} training example <span style='color:#00a000;'>$i$</span>. 
What we have not discussed yet, is
how we calculate the gradient of the cost function, i.e.<span style='color:#800000;'>\ </span>how we obtain the partial derivatives <span style='color:#00a000;'>$g_{ij}$</span> and <span style='color:#00a000;'>$h_i$</span> 
with respect to all the weights and biases respectively,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> g_{ij} &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> w_{ij}} </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{weightDerivative} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> h_i &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> b_i} </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{biasDerivative}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
where we have dropped layer indicies for clarity. 
A common method to obtain these derivatives is <span style='color:#800000;'>\textit</span>{backpropagation} <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Rumelhart86</span>}. 
In backpropagation, a training example is propagated forward through the NN to produce an output. 
This output is compared to the desired output (target values), and the error is then propagated <span style='color:#800000;'>\textit</span>{backwards}
through the layers to obtain the amount of which each parameter should be adjusted, 
hence the name.
The method is essentially an implementation of the chain rule, and will allow us to calculate
the partial derivatives of the cost with respect to all the parameters, thereby obtaining the gradient of the network. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>1. Forward propagation</span></b>}
The first stage of the algorithm is forward propagation, producing an output as described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{We assume that the MLP only has one output. As before, the results are valid for each individual output in 
a many-output MLP.}.
In the following, we will use the same notation as in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}. We also define
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> A_i = </span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>j:w_{ij}</span><span style='color:#606000;'>\}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
as the set <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>j</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> of nodes anterior to node <span style='color:#00a000;'>$i$</span> and connected to node <span style='color:#00a000;'>$i$</span> with weights <span style='color:#00a000;'>$w_{ij}$</span>, in addition to
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> P_j = </span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>i:w_{ij}</span><span style='color:#606000;'>\}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
as the set <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>i</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> of nodes posterior to node <span style='color:#00a000;'>$j$</span> and connected to node <span style='color:#00a000;'>$j$</span> with weights <span style='color:#00a000;'>$w_{ij}$</span>. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>2. Backward propagation</span></b>}
The second step is to backpropagate the error of the NN output. The weight derivative <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>weightDerivative</span>}
can be expanded into two factors by use of the chain rule,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> g_{ij} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> w_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{weightDerivativeExpanded}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Now we move in the opposite direction of the feed-forward stage: First we differentiate the cost w.r.t 
the input of neuron <span style='color:#00a000;'>$i$</span>, then we differentiate the input w.r.t. weight <span style='color:#00a000;'>$w_{ij}$</span> connecting neurons <span style='color:#00a000;'>$j$</span> (in the preceding layer)
and <span style='color:#00a000;'>$i$</span>. 
The first term on the r.h.s. is defined as the the error <span style='color:#00a000;'>$</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i$</span> of node <span style='color:#00a000;'>$i$</span>, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{neuronError}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This definition can be justified by the following argument.
To change the value of a cost function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>generalCost</span>}, we need to change the outputs of the neurons in the network. 
Changing the input <span style='color:#00a000;'>$u_j$</span> to neuron <span style='color:#00a000;'>$j$</span> by a small amount <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> u_j$</span> results in the output
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y_j = f_j(u_j + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> u_j)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This change will propagate through subsequent layers in the network, finally causing the overall cost to change
by an amount <span style='color:#00a000;'>$</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j}</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> u_j$</span>. If <span style='color:#00a000;'>$</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> / </span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j$</span> is close to zero,
we are not able improve the cost much by perturbing the weighted input <span style='color:#00a000;'>$u_j$</span>; the neuron is already quite near the optimal value.
This is a heuristic argument for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> / </span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j$</span> to be a measure of the error of the neuron. 

<span style='color:#800000;'>\noindent</span> The second term on the r.h.s. of <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>weightDerivativeExpanded</span>} is 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> w_{ij}} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> w_{ij}} </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{m</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> A_i} w_{im}y_m = y_j</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{derivativeSecondTerm}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Combining the the two terms <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>neuronError</span>} and <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>derivativeSecondTerm</span>} yields
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> g_{ij} = </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i y_j</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{weightGradient}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
To compute this quantity, we thus need to know the outputs and the errors of all nodes in the network. 
The outputs are generated during the feed-forward stage <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>generalLayer</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y_i = f_i(u_i) = f_i</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> A_i} w_{ij}y_j + b_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forwardProp}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and need to be stored for all nodes. 
The errors are obtained by backpropagating the error of the output neuron <span style='color:#00a000;'>$o$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_o = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_o} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_o}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_o}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_o}</span>
<span style='color:#00a000;'>          = (Y - y_o) </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_o}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_o}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We remember from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>} that the output neuron has the identity activation function <span style='color:#00a000;'>$y_o = f_o(u_o) = u_o$</span>, 
which reduces the error to
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_o = Y  - y_o</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This error is then propagated backwards through the network, layer by layer. 
The error of each neuron in layer <span style='color:#00a000;'>$l$</span> thus depends on the errors of all nerons in layer <span style='color:#00a000;'>$l+1$</span>.  
Consequentially, the error of an arbitrary hidden neuron can be written as a recursive equation (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:backprop</span>}),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_j = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j} = </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> P_j} </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{errorTerms}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.9<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/backprop.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Illustration of the backpropagation algorithm. The direction of information flow is opposite of <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:neuronModel</span>}. 
	   A hidden neuron <span style='color:#00a000;'>$j$</span> receives a weighted sum of errors of all nodes in the posterior layer. Then, we differentiate
	   in the backwards direction: the activation function (output) <span style='color:#00a000;'>$y_j$</span> of node <span style='color:#00a000;'>$j$</span> is differentiated w.r.t to its 
	   net input <span style='color:#00a000;'>$u_j$</span>.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:backprop</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
We observe that also the inputs <span style='color:#00a000;'>$u_i$</span> to all nodes need to be stored during forward propagation for 
this scheme to work. 
The first term on the r.h.s. of <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>errorTerms</span>} is the error <span style='color:#00a000;'>$</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i$</span> of node <span style='color:#00a000;'>$i$</span>, which is assumed to be known. 
As long as there are no cycles in the network, there is an ordering
of nodes from the output back to the input that respects this condition. It is
therefore valid only for feed-forward NNs.

The second term is the derivative of the net input of all posterior
nodes <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>i</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> w.r.t. the output of node <span style='color:#00a000;'>$j$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'>}{y_j}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{m</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> A_i} w_{im}y_m + b_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) = w_{ij}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
while the third is the derivative of node <span style='color:#00a000;'>$j$</span>'s activation function w.r.t. its net input:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_j(u_j)}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j} </span><span style='color:#606000;'>\equiv</span><span style='color:#00a000;'> y^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_j</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Putting all the pieces together we obtain
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_j = y^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_j</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> P_j}w_{ij} </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{backprop}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This expression is illustrated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:backprop</span>}.  
Each hidden neuron <span style='color:#00a000;'>$j$</span> recieves a weighted sum
of the errors of all nodes in the posterior layer. Then we differentiate in the backwards direction (compare with 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:neuronModel</span>}): 
the output <span style='color:#00a000;'>$y_j$</span> of node <span style='color:#00a000;'>$j$</span> is differentiated w.r.t. its input <span style='color:#00a000;'>$u_j$</span>. 

The errors are propagated backwards through the whole NN until we reach the weights connecting the input layer
and the first hidden layer. 
By propagating the error
of only one ouput neuron, we thus obtain the errors of all the neurons at once. This is the main strength of the backpropagation
algorithm, and the reason for its popularity in neural network research. 

<span style='color:#800000;'>\noindent</span> For the biases, we have <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>biasDerivative</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> h_j = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> b_j} = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> P_j} </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}</span>
<span style='color:#00a000;'>	      </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j} </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> y_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j} </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> b_j}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The only new term here is the last one:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> u_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> b_j} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> b_j}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{m</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> A_j} w_{jm}y_m + b_j</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) = 1</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Consequentially, the bias gradient is simply the error of each neuron:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> h_j = </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_j</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Matrix notation</span></b>}
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>} we rewrote the forward propagation framework as matrix-vector equations. 
This is easily done also for the backpropagation case. From <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>} we have the
set of vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l$</span> for <span style='color:#00a000;'>$l = 1,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> ,L+1$</span>, 
where <span style='color:#00a000;'>$L$</span> is the number of hidden layers. We now extend this set with the vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l$</span> 
and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l$</span>, i.e.<span style='color:#800000;'>\ </span>the errors and inputs (or <span style='color:#800000;'>\textit</span>{preactivations}) of layer <span style='color:#00a000;'>$l$</span>
respectively. These are all column vectors of size <span style='color:#00a000;'>$N_l </span><span style='color:#606000;'>\times</span><span style='color:#00a000;'> 1$</span>. 

<span style='color:#800000;'>\noindent</span> Comparing the sums in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forwardProp</span>} and <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>backprop</span>}, we realize that since 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_j w_{ij} y_j </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W} </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
we have
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_i w_{ij} </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'>  </span><span style='color:#606000;'>\Rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}^T </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
i.e.<span style='color:#800000;'>\ </span>the weight matrices employed in backpropagation are the transpose of the matrices used in forward activation.
Thus, for nodes <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>j</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> in layer <span style='color:#00a000;'>$l$</span> and nodes <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>i</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> in layer <span style='color:#00a000;'>$l+1$</span>, the vectorized equation for the error of each hidden neuron is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_j = y^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_j</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i</span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> P_j}w_{ij} </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i </span><span style='color:#606000;'>\:</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\Rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\:</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_l </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> (W_{l+1} </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_{l+1})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'>$</span> signifies element-wise multiplication. The expression for the weight gradients <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>weightGradient</span>} is converted
into an outer product of the errors of layer <span style='color:#00a000;'>$l+1$</span> and the outputs of layer <span style='color:#00a000;'>$l$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> g_{ij} = </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>_i y_j </span><span style='color:#606000;'>\Rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_l = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>}_{l+1} </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}^T_l</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_l$</span> is a matrix containing all gradients for the weights connecting layer <span style='color:#00a000;'>$l$</span> and <span style='color:#00a000;'>$l+1$</span>. 
The corresponding matrix for the biases is a <span style='color:#00a000;'>$N_l </span><span style='color:#606000;'>\times</span><span style='color:#00a000;'> 1$</span> column vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_l$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_l = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The set <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\mathrm</span><span style='color:#00a000;'>{G}_l, </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_l</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> for <span style='color:#00a000;'>$l=1,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>,L+1$</span> thus make up the total gradient of the cost function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>}. 


<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Training algorithm</span></b>}
We are now ready to express the complete training algorithm for a MLP with backpropagation using
matrix-vector notation. We introduce <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_{l,i}$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_{l,i}$</span> as the weight gradient matrix and 
the bias gradient vector for training example <span style='color:#00a000;'>$i$</span>. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Input a set of <span style='color:#00a000;'>$n$</span> training examples.
 <span style='color:#800000;'>\item</span> For each training example <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x} = X_{i*}$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Initialize the input layer:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_0 = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{x}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
 <span style='color:#800000;'>\item</span> Propagate the activity forward. For <span style='color:#00a000;'>$l = 1,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>,L+1$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l &amp;= </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{l-1} + </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l &amp;= f_l(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l)</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forwardPropMatrix}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
 Store all vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l$</span>.
 <span style='color:#800000;'>\item</span> Calculate and store the error in the output layer:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_{L+1} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{Y} - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{L+1}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
 <span style='color:#800000;'>\item</span> Backpropagate the error. For <span style='color:#00a000;'>$l = L, L-1, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> ,1$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_l </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> (</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}^T_{l+1}</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_{l+1}) </span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'> </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{backPropMatrix}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
 Store all errors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l$</span>. 
 <span style='color:#800000;'>\item</span> Compute and store the weight and bias gradients. <span style='color:#800000;'>\\</span> For <span style='color:#00a000;'>$l = L+1, L, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> ,1$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_{l,i} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{l-1}^T, </span><span style='color:#606000;'>\quad</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_{l,i} = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{weightUpdate}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
 <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Calculate the total gradients of all training examples <span style='color:#00a000;'>$i$</span>. <span style='color:#800000;'>\\</span>
 For <span style='color:#00a000;'>$l = L+1, L, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'> ,1$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_l = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_i^n </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{G}_{l,i} </span><span style='color:#606000;'>\quad</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{and} </span><span style='color:#606000;'>\quad</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_l = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_i^n </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{H}_{l,i}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
 <span style='color:#800000;'>\item</span> Update weights and biases with an update rule of choice. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
The value of <span style='color:#00a000;'>$n$</span> depends on the learning paradigm that is used, described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:gradientDescentVariants</span>}. 



<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Neural network potentials</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:NNPs</span>}
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:constructingPES</span>} we discussed various ways to construct a PES for use in molecular dynamics, and
stated that this thesis deales with neural network potentials (NNP). Then, in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:machineLearning</span>}
we outlined the basic theory of MLPs, which is our choice of neural network type. In this chapter, 
we will discuss the symmetry problems that arise when constructing NNPs, and introduce the Behler-Parrinello method for 
solving these symmetry issues. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Neural network potentials</span></b>}
NNPs are constructed by fitting an analytic function to a set of reference data
obtained by quantum mechanical calculations.
The main goal of the NN approach to constructing a PES, is to create a potential that has the accuracy of <span style='color:#800000;'>\textit</span>{ab inito}
methods and the speed of empirical potentials (or at least of the same order).

Several different methods have been developed to construct NNPs. A comprehensive review of many of these 
has been written by Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>}. They mainly differ in the number of NNs that are employed 
to represent the total energy of the system, the dimensionality of each NN and the choice of input coordinates. 
By dimensionality we mean the number of input nodes, as the number of output nodes is assumed to be 1 if not specified otherwise. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Potentials using single neural network</span></b>}
In the literature, the dominant part of NN potentials employ a single feed-forward NN to calculate the total energy of the system.
This is the most straightforward approach and is the easiest to implement.
For instance, Agrawal et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Agrawal06</span>} have used a single, three-dimensional NN to study the dissociation of the 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span> molecule. However, these types of NNPs suffer from a number of limitations. Most importantly, 
they have a fixed dimensionality. This is a general property of MLPs where the atomic coordinates are fed directly 
to the network without any transformations: Once fitted, the NNP can only be applied to systems with the same number of atoms
as the number of inputs. Further, a large number of input nodes requires many hidden neurons, and there comes a point 
where the number of parameters of the NN makes training unfeasible. Thus, the number of degrees of freedom of the system 
must be small. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Potentials using multiple neural networks</span></b>}
For NNPs to be recognized as competitive potentials in molecular dynamics, they should be applicable to large systems
of various sizes, containing thousands of atoms. This can not be achieved by employing only one NN to 
represent the total energy of the system. A solution is to replace the single NN by a set of <span style='color:#800000;'>\textit</span>{atomic} NNs, where 
each NN provides the contribution <span style='color:#00a000;'>$E_i$</span> of <span style='color:#800000;'>\textit</span>{one} atom to the total energy <span style='color:#00a000;'>$E$</span> of the system, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> E = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^N E_i</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{systemEnergy}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Two methods using this approach have been developed in parallel by Bholoa et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bholoa07</span>}, and Behler and Parrinello 
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler07</span>}.
Both methods compute <span style='color:#00a000;'>$E_i$</span> based on the chemical environment of atom <span style='color:#00a000;'>$i$</span>, but they differ in how 
the environment is represented and also how the atomic NNs are structured. 

Bholoa et al.<span style='color:#800000;'>\ </span>compute each atomic energy <span style='color:#00a000;'>$E_i$</span> by a NN of variable size, using a set of 
vectors as input to the NN. Each vector specifies the geometry of a four atom chain <span style='color:#00a000;'>$i-j-k-l$</span> describing the chemical 
environment. The input layer and first hidden layer are replicated once for each vector, thus the problem 
of fixed dimensionality is solved. 

Behler and Parrinello have developed a different approach to constructing a NNP. 
We have chosen to use the Behler-Parrinello (BP) method in this thesis, which will be the topic of the 
rest of the chapter. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>The Behler-Parrinello method</span></b>}
In the Behler-Parrinello method, the architecture of each NN is fixed, and
the atomic energies <span style='color:#00a000;'>$E_i$</span> depend on the local chemical environments up to a cutoff radius <span style='color:#00a000;'>$R_c$</span>. 
This is analogous to the introduction of a cutoff radius in MD simulations
(<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:forceCutoff</span>}). Thus, the NNP can be applied to MD simulations in excactly the same way 
as ordinary, empirical potentials: It is simply an analytic function that yields the potential energy of a central atom
in the presence of neighbouring atoms contained in a cutoff sphere of radius <span style='color:#00a000;'>$R_c$</span>. 
In other words, the BP method employs an <span style='color:#800000;'>\textit</span>{atom-centered} approach. 
In the following we will refer to all atoms inside the cutoff sphere of an atom <span style='color:#00a000;'>$i$</span> as <span style='color:#800000;'>\textit</span>{neighbours}.

The cutoff is introduced by a set of so-called symmetry functions. 
These functions
describe the local chemical environment of an atom <span style='color:#00a000;'>$i$</span> by transforming the cartesian coordinates of <span style='color:#800000;'>\textit</span>{all} its neighbours to 
a predefined number of function values, forming a symmetry vector. 
The symmetry vector is used as input to the atomic NN of atom <span style='color:#00a000;'>$i$</span>, that in turn calculates its potential energy <span style='color:#00a000;'>$E_i$</span>.
The length of this vector must therefore be equal to the number of NN inputs. 

Each atom has an individual atomic NN along with a fixed set of symmetry functions, but
all atoms of the same element have identical 
atomic NNs and symmetry functions sets. Thus, for a diatomic system, only two unique NNs need to be trained. 

As an example, let us look at a system containing two different elements: Si and O. 
All Si atoms have identical atomic NNs and symmetry function sets, the former represented by the function <span style='color:#00a000;'>$F_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}}$</span>. 
A given Si atom <span style='color:#00a000;'>$i$</span> has at a given time <span style='color:#00a000;'>$n$</span> neighbours, including both Si and O atoms.  
We define <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij} = (r_{i1}, r_{i2}, </span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>, r_{in})$</span> as the vector containing the distances from
atom <span style='color:#00a000;'>$i$</span> to all neighbours <span style='color:#00a000;'>$j = 1,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>,n$</span>, where <span style='color:#00a000;'>$r_{ij} = |</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i|$</span>. 
Further, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}) = </span><span style='color:#606000;'>\{</span><span style='color:#00a000;'>G_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si},s}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij})</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span> is the vector of Si symmetry functions
<span style='color:#00a000;'>$s = 1,</span><span style='color:#606000;'>\dots</span><span style='color:#00a000;'>,M_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}$</span><span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{This notation signifies that each symmetry function, i.e.<span style='color:#800000;'>\ </span>each element of 
the symmetry vector, depends on the coordinates of all atoms in the cutoff sphere. However, it does not say anything about
the form of the dependence, e.g.<span style='color:#800000;'>\ </span>if it involves triplets or just pairs of atoms etc.}.
The process of calculating the potential energy <span style='color:#00a000;'>$E_i$</span> described above can then be written,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}) </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> F_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij})] </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> E_i</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{BPatomicEnergySi}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
To obtain the corresponding expression for an O atom, we simply substitute <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Si} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{O}$</span>. 
The expression is valid for any element e, and for a system with an arbitrary number of elements,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}) </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> F_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij})] </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> E_i</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{BPatomicEnergy}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
While the symmetry function vector of an element e is of a fixed size equal to the number of inputs to <span style='color:#00a000;'>$F_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}$</span>,
the length of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}$</span> can be arbitrary. This is enabled by the functional form of the symmetry functions, 
which is the topic of the next section. 

We note that although the NNs and the symmetry vectors
are identical for all atoms of the same element, the resulting energies <span style='color:#00a000;'>$E_i$</span> are not equal since 
they depend on the unique atomic environments <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}$</span> of each atom. 




<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Symmetry functions</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>}
We have mentioned two requirements that the symmetry function set have to meet. First, they must contain a cutoff <span style='color:#00a000;'>$r_c$</span>
that defines a chemical environment. Secondly, since we employ atomic NNs with a fixed number of inputs,
they have to be able to produce a constant 
number of function values independent of the number of neighbours, which can change during MD simulations. 

In addition to this, the NN has symmetry problems. It does not take into account that exchanging two or more atoms
can lead to an energetically equivalent configuration. For example, exchanging the positions of both hydrogen atoms
in a water molecule will not alter the energy. However, noting that all weights have numerically different values,
changing the order of the input coordinates to the NN will result in a different energy value. This problem can be solved
by employing symmetry functions that are invariant with respect to the order of the atoms. 

Further, the energy of a system is invariant under a translation or rotation. Cartesian coordinates can therefore not be used 
directly as input to the NN, as their numerical values change under such operations, resulting in a different NN output. 

We start with defining the chemical environments by introducing a cutoff function <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11symmetry</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f_c(r_{ij}) = </span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'>0.5 </span><span style='color:#606000;'>\!\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\cos\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\pi</span><span style='color:#00a000;'> r_{ij}}{r_c}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) + 1 </span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>], &amp; r_{ij} </span><span style='color:#606000;'>\leq</span><span style='color:#00a000;'> r_c </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>0, &amp; r_{ij} &gt; r_c</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutoffFunction}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
which is the monotonically decreasing part of a cosine function on <span style='color:#00a000;'>$r_{ij} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [0,r_c]$</span> (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:cutoffFunction</span>}). 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/cutoffFunction.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Plot of the cutoff function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>cutoffFunction</span>} applied in this thesis. 
	   This function is used to define a chemical environment around a central atom:
           only the atoms within the cutoff radius <span style='color:#00a000;'>$r_c$</span> contribute to its energy. 
           These are called neighbouring atoms. The closer a neighbouring atom is, the larger
           the energy contribution, as is the case for most physical systems.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:cutoffFunction</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
This function have the desirable property that it decreases with increasing distance <span style='color:#00a000;'>$r_{ij}$</span> between the central
atom <span style='color:#00a000;'>$i$</span> and its neighbour <span style='color:#00a000;'>$j$</span>. At the cutoff radius <span style='color:#00a000;'>$r_c$</span>, is has zero value and slope, which is important
to avoid discontinuities when computing energies and forces. Atoms beyond the cutoff radius are not a part of the central
atom's chemical environment and therefore do not contribute to its energy. 

Several types of many-body symmetry functions can be
constructed based on the cutoff function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>cutoffFunction</span>}. 
They can be divided into two classes: <span style='color:#800000;'>\textit</span>{Radial} symmetry functions, describing 
the radial distribution of neighbours up to the cutoff radius, and <span style='color:#800000;'>\textit</span>{angular} symmetry functions, specifying their
angular arrangement. The symmetry functions discussed in this section are all suggested by Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11symmetry</span>}. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Radial symmetry functions</span></b>}
The most basic radial symmetry function is simply the sum of the cutoff functions for all the neighbours <span style='color:#00a000;'>$j$</span> of 
atom <span style='color:#00a000;'>$i$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^1 = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^N f_c(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We immidiately realize that this simple function meets the symmetry requirement stated above: The value 
of the sum is independent of the order of neighbours <span style='color:#00a000;'>$j$</span>. This is the case for each of the following symmetry functions, as they 
are all defined as sums over neighbours. We also note that all symmetry functions, including the cutoff function, 
are unitless. 

Physically, <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G1</span>} can be thought of as a ''coordination number'' on <span style='color:#00a000;'>$r_{ij} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [0,r_c]$</span>.
A set of such functions with different cutoff radii <span style='color:#00a000;'>$r_c$</span> can be used to describe the radial arrangement of the neighbouring atoms by
yielding coordination numbers on distance intervals of varying length. Thus, a symmetry function is defined by 
its parameter values, 
and a set of symmetry functions with different parameters form a symmetry vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}$</span>. 

A better alternative is to use a sum of products of Gaussians and the cutoff function, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^2 = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^N </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>(r_{ij}-r_s)^2] </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'>f_c(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We now have two parameters that can be adjusted to probe different radii. The width parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> determines the 
radial extension of the symmetry functions, while the shifting parameter <span style='color:#00a000;'>$r_s$</span> displaces the Gaussians to improve
the sensitivity at specific radii. A third option is 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^3 = </span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^N </span><span style='color:#606000;'>\cos</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\kappa</span><span style='color:#00a000;'> r_{ij}) </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> f_c(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G3}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
which are damped cosine functions with a period length adjusted by parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\kappa</span><span style='color:#00a000;'>$</span>. We will however not use 
this function because of the existence of negative function values, which can lead to atoms canceling each other's
contribution to the sum. 

In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions</span>} we see the radial symmetry functions for several
different parameters. It is clear that a set of such functions can exhibit a large flexibility by tuning the parameters,
enabling us to adequately represent the radial distribution of neighbours around a central atom. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G1.pdf</span>} 
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions:a</span>} 
    <span style='color:#898887;'>%\vspace{1ex}</span>
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>}<span style='color:#898887;'>%% </span>
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G2_1.pdf</span>} 
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions:b</span>} 
    <span style='color:#898887;'>%\vspace{1ex}</span>
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>} 
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G2_2.pdf</span>} 
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions:c</span>} 
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>}<span style='color:#898887;'>%%</span>
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G3.pdf</span>} 
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions:d</span>} 
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>} 
  <span style='color:#800000;'>\vspace</span>{-1.5ex}
  <span style='color:#800000;'>\caption</span>{Radial symmetry functions <span style='color:#00a000;'>$G^1$</span>, <span style='color:#00a000;'>$G^2$</span> and <span style='color:#00a000;'>$G^3$</span> for an atom with one neighbour only. A set of such 
	   functions represents the radial distribution of neighbours around a central atom placed at the origin.
	   For <span style='color:#00a000;'>$G^2$</span> and <span style='color:#00a000;'>$G^3$</span> a cutoff <span style='color:#00a000;'>$r_c = </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{11.0}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span> has been used. <span style='color:#00a000;'>$r_{ij}$</span>, <span style='color:#00a000;'>$r_c$</span> and <span style='color:#00a000;'>$r_s$</span> are in 
	   units of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>, while <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\kappa</span><span style='color:#00a000;'>$</span> have units <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-2}$</span> and 
	   <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-1}$</span> respectively.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialSymmetriFunctions</span>} 
  <span style='color:#800000;'>\vspace</span>{-1.5ex}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Angular symmetry functions</span></b>}
To obtain a suitable structural fingerprint of the atomic environments, we also need the angular distribution
of neighbouring atoms. This can be achieved by using functions depending on <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}$</span>, which
is the angle formed by the central atom <span style='color:#00a000;'>$i$</span> and the two interatomic distances <span style='color:#00a000;'>$r_{ij}$</span> and <span style='color:#00a000;'>$r_{ik}$</span>. 
The potential is periodic with respect to this angle, so we can use the cosine of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}$</span> instead. 
We thus define an angular symmetry function as a sum over all cosines with respect to any possible triplet
(<span style='color:#00a000;'>$i$</span>, <span style='color:#00a000;'>$j$</span>, <span style='color:#00a000;'>$k$</span>), multiplied by Gaussians of the three interatomic distances and the respective cutoff functions,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> G_i^4 = 2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j</span><span style='color:#606000;'>\neq</span><span style='color:#00a000;'> i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k&gt;j} &amp;</span><span style='color:#606000;'>\Big</span><span style='color:#00a000;'>[(1 + </span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik})^</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\,</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> (r_{ij}^2 + r_{ik}^2 + r_{jk}^2))  </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;f_c(r_{ij}) f_c(r_{ik}) f_c (r_{jk})</span><span style='color:#606000;'>\Big</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G4}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This function is zeroed out if any of the interatomic distances is larger than <span style='color:#00a000;'>$r_c$</span>. The parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> takes here
into account that the angular contribution depends on the atomic seperations. The angular arrangement can be 
investigated by using different values for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> while the normalization factor <span style='color:#00a000;'>$2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}$</span> ensures that the range of 
values is independent of the choice of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span>. The parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> {-1,1}$</span> can be used to invert the shape of the 
cosine function. For <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> = +1$</span> the maxima of the cosine terms are at <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik} = 0^</span><span style='color:#606000;'>\circ</span><span style='color:#00a000;'>$</span>, 
while for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>=-1$</span> they are located at <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik} = 180^</span><span style='color:#606000;'>\circ</span><span style='color:#00a000;'>$</span>. The presence of the cutoff function <span style='color:#00a000;'>$r_{jk}$</span>
ensures that only triplets where all three inter-atomic distances are within the cutoff radius are included. 
Another function can be defined that has no constraint on <span style='color:#00a000;'>$r_{jk}$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  G_i^5 = 2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j</span><span style='color:#606000;'>\neq</span><span style='color:#00a000;'> i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{k&gt;j} </span><span style='color:#606000;'>\Big</span><span style='color:#00a000;'>[(1 + </span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik})^</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\,</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> (r_{ij}^2 + r_{ik}^2)) </span><span style='color:#606000;'>\,</span>
<span style='color:#00a000;'> &amp;f_c(r_{ij}) f_c(r_{ik})</span><span style='color:#606000;'>\Big</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G5}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
which will generally lead to larger function values than <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G4</span>} because the lack of constraint on <span style='color:#00a000;'>$r_{jk}$</span> results in 
a larger number of non-zero terms in the summation. The angular parts of <span style='color:#00a000;'>$G_i^4$</span> and <span style='color:#00a000;'>$G_i^5$</span> are identical, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'> = 2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}(1 + </span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{jik})^</span><span style='color:#606000;'>\zeta</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and is shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions</span>} for different values of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span>. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G4G5angular1.pdf</span>}
    <span style='color:#800000;'>\subcaption</span>{}
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions:a</span>} 
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>}<span style='color:#898887;'>%% </span>
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subfigure</span>}{0.5<span style='color:#800000;'>\linewidth</span>}
    <span style='color:#800000;'>\centering</span>
    <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Theory/G4G5angular2.pdf</span>} 
    <span style='color:#800000;'>\subcaption</span>{}
    <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions:b</span>} 
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subfigure</span>} 
  <span style='color:#800000;'>\caption</span>{Angular part of symmetry functions <span style='color:#00a000;'>$G_i^4$</span> and <span style='color:#00a000;'>$G_i^5$</span> for an atom with one neighbour only. A set of such 
	   functions represents the angular distribution of neighbours around a central atom placed at the origin.
	   <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> = +1$</span> for <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions:a</span>}, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>=-1$</span> for <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions:b</span>}.}
 <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:angularSymmetryFunctions</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Determining the symmetry parameters</span></b>}
The parameter values <span style='color:#00a000;'>$r_c, </span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>, r_s, </span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span> are not automatically optimized, like the weights. 
They need to be predetermined, and remain fixed during the training. In the context of machine learning, such variables are called
<span style='color:#800000;'>\textit</span>{hyperparameters}, while the variables that are optimized during training are the <span style='color:#800000;'>\textit</span>{model parameters}, i.e.<span style='color:#800000;'>\ </span>
the weights and biases. Other examples of hyperparameters are the learning rate (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:optimization</span>}) and 
the number of NN nodes and layers. Various techniques for deciding on satisfactory hyperparameters are
discussed in  <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:settingHyperParams</span>}. 

They symmetry function set is
an integral part of the NN, and needs to be evaluated together with the NN itself when applied in simulations. 
The symmetry parameters must be customized for the system under cosideration; different strategies regarding this
is described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:constructingSymmetry</span>}.


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Symmetry functions and forces</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>}
To integrate Newton's laws in MD simulations we need the forces on all the atoms. The force field associated with a PES
was introduced in section <span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>sec:potentialEnergySurfaces</span>} and is restated here for reference:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F} = -</span><span style='color:#606000;'>\nabla</span><span style='color:#00a000;'> E</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forcePES2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
In the present case, the total energy <span style='color:#00a000;'>$E$</span> of the system is defined as a sum over all the atomic energies <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>systemEnergy</span>}. 
Further, we have applied a cutoff so that each atom only gets energy contributions from its atomic environment up 
to a certain distance <span style='color:#00a000;'>$r_c$</span>.
The force component <span style='color:#00a000;'>$F_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}$</span>, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'> = (x,y,z)$</span> acting on atom <span style='color:#00a000;'>$k$</span> with respect to coordinate <span style='color:#00a000;'>$r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}$</span>,
can therefore be written <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11symmetry</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>} = -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}} = -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^N</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}}</span>
<span style='color:#00a000;'> = -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^{N_k+1}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forceAtomk}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>$N$</span> is as before the total number of atoms, while <span style='color:#00a000;'>$N_k$</span> is the number of neighbours of atom <span style='color:#00a000;'>$k$</span>. 
Note that <span style='color:#00a000;'>$E_k$</span> is included in the above sum - atom <span style='color:#00a000;'>$k$</span> itself contributes to the total energy of its atomic
environment. We therefore have <span style='color:#00a000;'>$N_k+1$</span> contributions. 

NNs have as we have seen well-defined functional forms, and analytic derivatives are therefore readily available.
However, when symmetry transformations is applied, there is only an indirect relation between the energy and the cartesian coordinates
of the atoms. To calculate the force <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomk</span>} we therefore need to apply the chain rule,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>} = -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^{N_k+1}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}} = </span>
<span style='color:#00a000;'> -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^{N_k+1}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{s=1}^{M_i}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{i,s}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{i,s}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forceAtomkChainRule}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$M_i$</span> is the number of symmetry functions of atom <span style='color:#00a000;'>$i$</span>. In other words, we need to sum over all the symmetry functions
of all the neighbours <span style='color:#00a000;'>$i$</span> of atom <span style='color:#00a000;'>$k$</span> and <span style='color:#00a000;'>$k$</span> itself. This is the reverse process of <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>BPatomicEnergy</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> E_i </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> F_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}[</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij})] </span><span style='color:#606000;'>\rightarrow</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G}_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{e}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}) </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_{ij}   </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{BPatomicForce}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where the differentiation is performed in the direction of the arrows. 
We realize that the first term
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i/</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{i,s}$</span> is the derivative of the output of the atomic NN of atom <span style='color:#00a000;'>$i$</span> with respect to the inputs,
i.e.<span style='color:#800000;'>\ </span>the gradient of the NN. This is an analytic gradient
which is given by the architecture of the NN, including the values of the weights and biases.
We recall from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:backprop</span>} that the backpropagation algorithm calculates the derivatives of all neurons in a NN to obtain
the amount of which each weight and bias should be adjusted during training. We can thus use a slightly modified version
of this algorithm to compute the gradient of a NN,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Instead of backpropagating the derivative of the cost function, 
 we backpropagate the derivative of the output neuron itself.
 For output neurons with the identity activation function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>outputActivation</span>}, 
 this derivative is 1. 
 <span style='color:#800000;'>\item</span> The derivative is propagated all the way back to the input nodes. During training, the propagation
 stops at the weights connecting the input layer and the first hidden layer. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
This procedure clarifies how a fully-connected feed-forward NN is built: To get the derivative of the output neuron
w.r.t. the input neurons, we have to also compute the derivative of all the neurons in-between because
all nodes in each layer are connected to all nodes in the following layer. 

The second term <span style='color:#00a000;'>$</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{i,s}/</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> R_{k,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}$</span> in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomkChainRule</span>} is given by the 
definition of the employed symmetry functions. 
The derivatives of the Behler symmetry functions in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>} are 
listed in Appendix <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>app:appendixA1</span>}.

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Change of coordinates</span></b>}
In a MD simulation we only work with relative or inter-atomic coordinates and potentials. 
The training of neural networks is also based on an atom-centered approach; the cartesian coordinates
that is given as input to the symmetry functions are always atom-centered. This means that the atom
in question is placed at the origin and the positions of all neighbouring atoms are measured relative to 
the origin. Effectively, we are making a change of coordinates to the neighbours <span style='color:#00a000;'>$j$</span> of the central atom <span style='color:#00a000;'>$i$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoordinates}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This is in accordance with the Behler symmetry functions, which only operate with relative coordinates. 
It is however not trivial how this affects the above equations for the forces, which are not written in 
an atom-centered way. The process of obtaining the forces can be illustrated with an example system
consisting of three atoms of the same element with positions <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i$</span> where <span style='color:#00a000;'>$i = 0,1,2$</span>, shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:threeAtoms</span>}.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>center</span>}
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.4<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/3atoms.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example system of three atoms of the same type. Only atom 2 is inside the cutoff sphere of atom 1, thus we only need to 
	   take the energy of atom 1 and 2 into account to find the force on atom 1. Since the atoms are of the same type,
	   they have identical atomic NNs and symmetry function sets.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:threeAtoms</span>}
  <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>center</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
We want to find the force on atom 1 in the <span style='color:#00a000;'>$x$</span>-direction. Atom 0 is outside the cutoff sphere of atom 1, 
so according to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomk</span>} the force is,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{1,x} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_1}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forceOnAtom1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
For simplicity, we assume that each atom only have two different symmetry functions describing their atomic environment;
one <span style='color:#00a000;'>$G_{i,0}^2$</span> function describing the radial arrangement of neighbours and one <span style='color:#00a000;'>$G_{i,1}^5$</span> dealing with the angular distribution.
Each atomic NN thus has two inputs and one output. 
Since the three atoms are all of the same type, they will have identical NNs <span style='color:#00a000;'>$F$</span> and 
symmetry vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{G} = (G_{i,0}^2, G_{i,1}^5)$</span>. 
We will therefore denote the the two symmetry functions simply as <span style='color:#00a000;'>$G_2$</span> and <span style='color:#00a000;'>$G_5$</span>. 
In the following we apply the notation <span style='color:#00a000;'>$r_{ij} = |</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_j - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{r}_i|$</span>. 
The expressions for <span style='color:#00a000;'>$E_1$</span> and <span style='color:#00a000;'>$E_2$</span> are,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> E_1 = F</span><span style='color:#606000;'>\bigr</span><span style='color:#00a000;'>[G_2(r_{12})</span><span style='color:#606000;'>\bigr</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{energyAtom1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>$G_5$</span> is zero because atom 1 only has one neighbour. Further, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> E_2 = F</span><span style='color:#606000;'>\bigr</span><span style='color:#00a000;'>[G_2(r_{20}, r_{21}) + G_5(r_{20},r_{21},</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{021})</span><span style='color:#606000;'>\bigr</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{energyAtom2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We note that atom 2 receives an energy contribution from both atom 0 and 1, so that 
its <span style='color:#00a000;'>$G_2$</span> function is a sum containing two terms. 
To compute the derivatives of these energies with respect to the <span style='color:#00a000;'>$x$</span>-coordinate of atom 1, we
must take into account the change of coordinates <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>changeOfCoordinates</span>}. We have that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_j} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_j}</span>
<span style='color:#00a000;'> = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> (x_j-x_i)}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_j} = </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoordsSymmetry1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_i} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> (x_j-x_i)}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_i} =</span>
<span style='color:#00a000;'> -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(x_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoordsSymmetry2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
which is also valid for the other radial symmetry functions <span style='color:#00a000;'>$G_1$</span> and <span style='color:#00a000;'>$G_3$</span>.   
This symmetry, which is a manifestation of Newton's third law,
enable us to calculate the derivatives in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceOnAtom1</span>}. For radial symmetry functions
dependent on several atom pairs, <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry1</span>} and <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry2</span>} 
apply to each term in the sum seperately. Further, the symmetry is also present for each atom in a triplet,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_i} = </span>
<span style='color:#00a000;'> -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} - </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoords3body1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_j} &amp;= </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoords3body2} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_k} &amp;= </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(x_{ij}, x_{ik}, </span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{changeOfCoords3body3}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
i.e.<span style='color:#800000;'>\ </span>the derivative w.r.t. <span style='color:#00a000;'>$i$</span> is the negative sum of the derivatives w.r.t. <span style='color:#00a000;'>$j$</span> and <span style='color:#00a000;'>$k$</span>. This is a symmetry we
have exploited in our code, see <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:extendingLammps</span>}. 

To obtain the derivatives of the energies <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>energyAtom1</span>} and <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>energyAtom2</span>}, we apply the chain rule
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_1}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{12})}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{12})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} = </span>
<span style='color:#00a000;'> -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{12})}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{12})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{12}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where we have applied the above change of coordinates <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry2</span>}.
Using <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry1</span>}, the derivative of <span style='color:#00a000;'>$E_2$</span> can be calculated correspondingly,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} &amp;= </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{20}, r_{21})}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{20}, r_{21})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} + </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(r_{20},r_{21},</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{021})}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{G_5(r_{20},r_{21},</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{021})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_1} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{21})}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_2(r_{21})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{21}} + </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_5(r_{20},r_{21},</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{021})}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{G_5(r_{20},r_{21},</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{021})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{21}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>} 
From the definition of <span style='color:#00a000;'>$G_2$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G2</span>} we see that the sum contains two terms
in the case of atom 2, one for each pair interaction between atom 2 and its two neighbours. 
The interaction term between atom 0 and 2
cancels when differentiated with respect to the coordinates of atom 1, and is thus not included in the force on atom 1. 
 
The sum in <span style='color:#00a000;'>$G_5$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G5</span>} however, consists of only one term corresponding to the only unique triplet with atom 2
as the central atom. This term is directly dependent on the coordinates of atom 0, which means that 
an atom which is outside the cutoff sphere of atom 1 still contributes to the force. This is an artecfact of many-body potentials
like the NNPs employed in this thesis: The force acting on an atom <span style='color:#00a000;'>$i$</span> depends on the positions of the atoms as 
far as <span style='color:#00a000;'>$2r_c$</span> away because <span style='color:#800000;'>\textit</span>{neighbours of neighbours} are included in the
energies that are differentiated to obtain the force. 
This large effective range of atomic interactions is however not of great importance, taking into account
that the interaction strength falls rapidly with distance for most systems where Coulomb forces are not included. 


<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Summary</span></b>}
In this chapter we have introduced the Behler-Parrinello method for constructing NNPs to be used in MD simulations. 
This scheme employs certain symmetry functions that have the following desirable properties,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> They are atom-centered and have an inherent cutoff <span style='color:#00a000;'>$r_c$</span> that defines a chemical environment of tractable size. 
 <span style='color:#800000;'>\item</span> They yield the same number of outputs independent of the number of inputs. 
 <span style='color:#800000;'>\item</span> The order of the inputs is irrelevant. 
 <span style='color:#800000;'>\item</span> They are invariant with respect to translation and rotation.
 <span style='color:#800000;'>\item</span> They have analytic derivatives.
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
1. and 2. solves the technical issues of the limited and fixed dimensionality of MLPs respectively.
3. and 4. deals with physical symmetries, namely that the energy of an atomic system does not change
if two identical atoms are exchanged, or if the system undergoes a translation or a rotation. 
5. is a wanted property because the symmetry functions are differentiated to obtain forces. 

In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:constructingNNP</span>} we present the complete workflow for constructing a NNP, 
including a description of how to assemble a symmetry function set, found in 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:constructingSymmetry</span>}.




<span style='color:#f00000;'>\part</span>{<b><span style='color:#000000;'>Implementation and results</span></b>}

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>LAMMPS</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:lammps</span>}
LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator)
is a classical molecular dynamics simulation package 
developed at Sandia National Laboratories <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Plimpton95</span>}. It is written
in highly portable and extendable objected-oriented C<span style='color:#800000;'>\texttt</span>{++}. 
The package contains functionality encompassing a wide variety of 
potentials, atom types, thermostats and 
ensembles, and is able to simulate a large number
of different systems, including atoms, molecules, 
coarse-grained particles, polymers, granular materials
and metals. LAMMPS can be run on a single processor
or in parallel. 

There exist several other molecular dynamics packages, 
like GROMACS, OpenMD, Aber and NAMD that could also
have served the purpose of this thesis. We have chosen
to work with LAMMPS because it is well documented<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{http://lammps.sandia.gov/doc/Manual.html}{lammps.sandia.gov/doc/Manual.html}},
easy to expand upon<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{http://lammps.sandia.gov/doc/Developer.pdf}{lammps.sandia.gov/doc/Developer.pdf}},
and because the Computational
physics group at UiO has a lot of experience with using
it for molecular dynamics simulations. 

In this chapter we will outline the basic usage of LAMMPS, including
a walkthrough of an example input script. We also describe its
class hierarchy and how we can add our own functionality
to the package.

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Installing LAMMPS</span></b>}
We have chosen to install LAMMPS by cloning
the Github repository and compile the source
code by running <span style='color:#800000;'>\texttt</span>{make} in the source directory. 
To compile a serial version of the software, we simply run
the command
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> make serial</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
while the corresponding command for the parallel
MPI version is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> make mpi</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
New updates can be downloaded by setting an
upstream git remote to point to the LAMMPS
GitHub repository and doing a <span style='color:#800000;'>\texttt</span>{git fetch upstream}. 
LAMMPS also have several additional packages that
can be installed. This can be done by running
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> make yes-&lt;package name&gt;</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
e.g.<span style='color:#800000;'>\ \texttt</span>{manybody} to install many-body potentials
like Stillinger-Weber and Vashishta. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>LAMMPS input script</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:lammpsInputScript</span>}
LAMMPS is run by providing an input script as 
argument to the executable. This input script
is read line by line and has its own syntax. 
A good way to show the basics an input file
is to look at a simple example. 
We have chosen to have a look at a script
to measure the radial distribution function of a Si crystal with the Stillinger Weber (SW) potential (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:stillingerWeber</span>}),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'># initialization</span>
<span style='color:#a08000;'>units 			metal		</span>
<span style='color:#a08000;'>dimension 		3</span>
<span style='color:#a08000;'>boundary		p p p		</span>
<span style='color:#a08000;'>atom_style		atomic		</span>

<span style='color:#a08000;'># create geometry and atoms</span>
<span style='color:#a08000;'>lattice 		diamond 5.431</span>
<span style='color:#a08000;'>variable		L equal 10</span>
<span style='color:#a08000;'>region 			simBox block 0 ${L} 0 ${L} 0 ${L}</span>
<span style='color:#a08000;'>create_box		1 simBox</span>
<span style='color:#a08000;'>create_atoms	1 box</span>

<span style='color:#a08000;'># set mass and initial temperature</span>
<span style='color:#a08000;'>mass			1 28.06</span>
<span style='color:#a08000;'>variable		temp equal 300</span>
<span style='color:#a08000;'>velocity		all create ${temp} 87287</span>

<span style='color:#a08000;'># compute radial distribution function</span>
<span style='color:#a08000;'>compute 		radialDist all rdf 100</span>

<span style='color:#a08000;'># potential </span>
<span style='color:#a08000;'>pair_style		sw</span>
<span style='color:#a08000;'>pair_coeff		* * Si.sw Si</span>
<span style='color:#a08000;'>neighbor		0.5 bin</span>
<span style='color:#a08000;'>neigh_modify	every 20 delay 0 check no</span>

<span style='color:#a08000;'># integration</span>
<span style='color:#a08000;'>timestep 		0.01</span>
<span style='color:#a08000;'>run_style 		verlet</span>
<span style='color:#a08000;'>fix 			integration all nve</span>

<span style='color:#a08000;'># output</span>
<span style='color:#a08000;'>thermo			50</span>
<span style='color:#a08000;'>thermo_style 	custom step temp density press ke pe etotal </span>
<span style='color:#a08000;'>thermo_modify 	norm yes</span>

<span style='color:#a08000;'># thermalise before sampling rdf</span>
<span style='color:#a08000;'>run				10000</span>
<span style='color:#a08000;'>fix radial all ave/time 100 10 1000 c_radialDist[*] file &amp;</span>
<span style='color:#a08000;'>    rdf.txt mode vector</span>
<span style='color:#a08000;'>run				10000</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
We will in the following briefly explain what
processes that are evoked in LAMMPS when the 
above commands are read. We start with the initialization of 
three basic properties of the simulation: Units, boundary conditions and particle types.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> units 		metal</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
This command defines the units that are used 
in the simulation and the output. LAMMPS have eight
different unit sets. The <span style='color:#800000;'>\texttt</span>{metal} set measures
distance in <span style='color:#800000;'>\AA</span>{}ngstr<span style='color:#800000;'>\o</span>{}m, energy in eV and temperature
in Kelvin. The choice of units depends on the system
that is investigated and the scale we are looking at.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> boundary 	p p p</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
We wish to measure the radial distribution function in a <span style='color:#800000;'>\textit</span>{bulk}
Si crystal, thus we want to have periodic 
boundary conditions in all three dimensions, 
annotated by a <span style='color:#800000;'>\texttt</span>{p}. LAMMPS can also handle stiff <span style='color:#800000;'>\texttt</span>{f}
and adaptive <span style='color:#800000;'>\texttt</span>{s} non-periodic boundaries. Adaptive
means that the position of the face is set so as to
encompass the atoms in that dimension. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> atom_style atomic</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
Different systems need different information
to be stored for each atom. For style <span style='color:#800000;'>\texttt</span>{atomic}, only
the default attributes are associated with each atom, 
namely coordinates, velocities, atom IDs and types. 
This is sufficient for non-bonded potentials like Stillinger-Weber.

<span style='color:#800000;'>\noindent</span> Next, we create the geometry of the simulation domain.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>lattice 	diamond 5.431</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
This defines the initial configuration of the atoms. 
Here, a diamond cubic lattice with a lattice constant of
5.431 is used, which is the crystal structure of Si. Other types of lattices based on
cubic or square unit cells are also available. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>variable	L equal 10</span>
<span style='color:#a08000;'>region 		simBox block 0 ${L} 0 ${L} 0 ${L}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The geometry of the system is defined with the <span style='color:#800000;'>\texttt</span>{region}
command. The <span style='color:#800000;'>\texttt</span>{block} style is simply a 3-dimensional
straight-faced box with a size of <span style='color:#00a000;'>$L = 10$</span> unit
cells in each dimension. We have labelled the 
region <span style='color:#800000;'>\texttt</span>{simBox}. 
The number of unit cells
<span style='color:#800000;'>\texttt</span>{L} is defined as a LAMMPS <span style='color:#800000;'>\texttt</span>{variable}. Variables
can be referenced elsewhere in the script by
writing <span style='color:#800000;'>\texttt</span>{<span style='color:#800000;'>\$\{</span>variable<span style='color:#800000;'>\_</span>name<span style='color:#800000;'>\}</span>} to become part of a new input
command like above. LAMMPS enables many styles
of variables to be defined. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>create_box		1 simBox</span>
<span style='color:#a08000;'>create_atoms	1 box</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The command <span style='color:#800000;'>\texttt</span>{create<span style='color:#800000;'>\_</span>box} creates a simulation box 
based on the specified region, in our case the 
<span style='color:#800000;'>\texttt</span>{block} region defined above. The argument specifies
the number of atom types that will be used in the
simulation. Next, <span style='color:#800000;'>\texttt</span>{create<span style='color:#800000;'>\_</span>atoms} with the argument
<span style='color:#800000;'>\texttt</span>{box} fills the simulation
domain with atoms of type 1 on the lattice. 
LAMMPS also lets you create a random collection
of atoms or single atoms at specified coordinates.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>mass			1 28.06</span>
<span style='color:#a08000;'>variable		temp equal 300</span>
<span style='color:#a08000;'>velocity		all create ${temp} 87287 </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
We need to assign mass to the atoms. For <span style='color:#800000;'>\texttt</span>{metal} units, 
mass is measured in grams/mole. The atoms are also
given an initial velocity that corresponds to 
the given initial temperature. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>pair_style		sw</span>
<span style='color:#a08000;'>pair_coeff		* * Si.sw Si</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The choice of potential is made with the 
<span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>style} command. We want to simulate
interactions using the Stillinger-Weber potential <span style='color:#800000;'>\texttt</span>{sw}. The SW parameters
are set with <span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>coeff}, where <span style='color:#800000;'>\texttt</span>{Si.sw} is a text file from which all parameters are read.  
Further, asterisks signifies that the potential file contains parameters for all elements, in this case for 
Si only. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>neighbor		0.5 bin</span>
<span style='color:#a08000;'>neigh_modify	every 20 check yes</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
These commands sets parameters that affect the 
building of neighbor lists. The first argument
to <span style='color:#800000;'>\texttt</span>{neighbor} is the skin size, while the 
second selects what algorithm is used
to build the lists. The <span style='color:#800000;'>\texttt</span>{bin} style creates 
the lists by binning, which in most cases
(including ours) is the fastest method. 
Further, we can control how often the lists
are built with <span style='color:#800000;'>\texttt</span>{neigh<span style='color:#800000;'>\_</span>modify}. The above arguments
specifies that new neighbour lists are built 
every 20 steps, but every step LAMMPS checks
if any atom has moved more than half the skin distance.
If that is the case, new lists are built. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>timestep 		0.01</span>
<span style='color:#a08000;'>run_style 		verlet</span>
<span style='color:#a08000;'>fix 			integration all nve</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
LAMMPS integrates Newtons' equations of motion
with the velocity-Verlet algorithm by default. 
This is the integrator of choice for most MD
applications due to its simplicity and 
symplectic nature (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:timeIntegration</span>}). The 
rRESPA integrator <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Tuckerman92</span>} scheme is also available.
LAMMPS does however not integrate and update
the positions and velocities of the particles
if not explicitly told so. This is done with
a <span style='color:#800000;'>\texttt</span>{fix}, which is any operation that is applied to the 
system during timestepping. The above <span style='color:#800000;'>\texttt</span>{fix} 
tells LAMMPS to integrate all atoms in the system 
so that they follow trajectories consistent with 
the microcanonical ensemble. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>thermo			50</span>
<span style='color:#a08000;'>thermo_style 	custom step temp press ke pe etotal </span>
<span style='color:#a08000;'>thermo_modify 	norm yes</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
We can control what thermodynamic properties 
to calculate and output with <span style='color:#800000;'>\texttt</span>{thermo<span style='color:#800000;'>\_</span>style}, while 
<span style='color:#800000;'>\texttt</span>{thermo} decides how often they should be computed. 
We want to output the time step, temperature, 
pressure, kinetic energy, potential energy and total
energy. Also, we want to normalize the extensive
quantities (the energies) by the number of atoms. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>compute 		radialDist all rdf 100</span>
<span style='color:#a08000;'>fix radial all ave/time 100 10 1000 c_radialDist[*] file &amp;</span>
<span style='color:#a08000;'>    rdf.txt mode vector</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The radial distribution function are measured with a LAMMPS <span style='color:#800000;'>\texttt</span>{compute}, 
which defines a computation that is performed on a group of atoms, in this case <span style='color:#800000;'>\texttt</span>{all} atoms. 
For the <span style='color:#800000;'>\texttt</span>{compute} to actually be executed, it needs
to be evoked by other LAMMPS commands, in this case the fix <span style='color:#800000;'>\texttt</span>{ave<span style='color:#800000;'>\slash</span> time}. 
The three numerical arguments specify how the distribution is averaged over time. 
Note that the <span style='color:#800000;'>\texttt</span>{compute} is referenced by <span style='color:#800000;'>\texttt</span>{c<span style='color:#800000;'>\_</span>} followed by its label. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>run		5000</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
Lastly, we run the simulation for 5000 time steps. 
LAMMPS allowss several run commands to be issued
after one another. This comes in handy if we want
to thermalise the system before measuring thermodynamic quantities,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> run 	5000</span>
<span style='color:#a08000;'> fix radial all ave/time 100 10 1000 c_radialDist[*] file &amp;</span>
<span style='color:#a08000;'>    rdf.txt mode vector</span>
<span style='color:#a08000;'> run 	10000</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
When parsing the second run command, LAMMPS will
continue the dynamics while computing eventual
new fixes, computes, dumps etc.<span style='color:#800000;'>\ </span>defined since
the last run command. 

This section has illuminated some of the basic
functionality of LAMMPS through several comments
on a simple input script. Now we move on to
how class hierarchy of LAMMPS is structured. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>LAMMPS structure</span></b>}
LAMMPS is written in C<span style='color:#800000;'>\texttt</span>{++} in an object-oriented
fashion. The class hierarchy is shown schematically
in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:lammpsClasses</span>}. 
The top-level classes (blue)
are visible anywhere in LAMMPS.
Some selected sub-classes (green and yellow) are displayed. We recognize many 
of the input script commands among these sub-classes; a rule of thumb is that every input script
command has a corresponding class and a
corresponding file name in the source directory. 
For instance, we observe that the Stillinger-Weber potential ''PairSW'' is a sub-class of Pair.  
This potential was evoked by the command
<span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>style sw} above and the source files are named
<span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>sw.cpp} and <span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>sw.h}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#800000;'>\centering</span>
 <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/lammpsClasses.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Class hierarchy of LAMMPS. The inheritance is from left to right. 
	   The top-level classes (blue) are visible everywhere in the code. Only a selected set 
	   of sub-classes (green and yellow) is displayed, where we recognize many of the input script
	   commands.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:lammpsClasses</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

We will not go into detail about the functionality
and communication of these classes. The one
that is the most relevant for this work is the <span style='color:#800000;'>\texttt</span>{Pair} class. 
This is the parent class of non-bonded potentials, which
in LAMMPS includes many-body potentials like Stillinger-Weber and 
Vashishta. The NN potential in this thesis falls 
under this category, and thus inherits from <span style='color:#800000;'>\texttt</span>{Pair}. 
In the following section we will describe how 
LAMMPS can be extended with such a new <span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>style}.

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Extending LAMMPS</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:extendingLammps</span>}
To extend LAMMPS with a new component, we 
simply add a new <span style='color:#800000;'>\texttt</span>{*.cpp} and <span style='color:#800000;'>\texttt</span>{*.h} file to the 
source directory and re-compile. The new class
will (in theory) work with all other LAMMPS 
classes without breaking any functionality. 
A good strategy is to start with an existing potential
source file that is somewhat similar to the one 
we want to make, instead of programming from scratch.
We aim to construct a many-body non-bonded neural network potential (NNP), and 
have chosen to base our implementation on the 
Stillinger-Weber source files.

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Potential initialization</span></b>}
The new potential class is added to LAMMPS with the following function call in the header file,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=c++]
 PairStyle(nn/manybody,PairNNManyBody)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The first argument is the name the potential will have in the input script, while 
the second is the name of the class. 
To use our new NN potential in a simulation we simply write,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> pair_style 	nn/manybody</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
in the input script. This command evokes two functions in the source files that need to be implemented: <span style='color:#800000;'>\texttt</span>{settings}
and <span style='color:#800000;'>\texttt</span>{init<span style='color:#800000;'>\_</span>style}. The former controls the number of arguments to 
<span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>style}, which in our case is zero, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{C++}
<span style='color:#0057ae;'>void</span> PairNNManyBody::settings(<span style='color:#0057ae;'>int</span> narg, <span style='color:#0057ae;'>char</span> **arg) {
  <b>if</b> (narg != <span style='color:#b08000;'>0</span>) error-&gt;all(FLERR,<span style='color:#bf0303;'>&quot;Illegal pair_style command&quot;</span>);
}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
while the latter decides what kind of neighbour lists LAMMPS should provide to the potential, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{C++}
<span style='color:#0057ae;'>void</span> PairNNAngular2::init_style() {
  <span style='color:#898887;'>// ... </span>
  <b>if</b> (force-&gt;newton_pair == <span style='color:#b08000;'>0</span>)
    error-&gt;all(FLERR,<span style='color:#bf0303;'>&quot;Pair style NN requires newton pair on&quot;</span>);

  <span style='color:#898887;'>// need a full neighbor list</span>
  <span style='color:#0057ae;'>int</span> irequest = neighbor-&gt;request(<b>this</b>);
  neighbor-&gt;requests[irequest]-&gt;half = <span style='color:#b08000;'>0</span>;
  neighbor-&gt;requests[irequest]-&gt;full = <span style='color:#b08000;'>1</span>;
}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
Our NN potential needs full neighbour lists to be able to compute three-body terms. Further, 
we ensure that <span style='color:#800000;'>\texttt</span>{newton<span style='color:#800000;'>\_</span>pair} is set to 1, which tells LAMMPS to communicate forces attributed to ghost 
atoms to the original atoms. 

The parent class <span style='color:#800000;'>\texttt</span>{Pair} has three <span style='color:#800000;'>\textit</span>{pure virtual member functions}, i.e.<span style='color:#800000;'>\ </span>functions that are required
to be implemented by a derived class: <span style='color:#800000;'>\texttt</span>{compute}, <span style='color:#800000;'>\texttt</span>{coeff} and the aforementioned <span style='color:#800000;'>\texttt</span>{settings}. 
The <span style='color:#800000;'>\texttt</span>{compute} method is where the forces and energies are calculated. This function is called
once each time step by the Verlet algorithm and the resulting forces are used to integrate Newton's equations
of motion. 

The coefficients of the employed potential are set in <span style='color:#800000;'>\texttt</span>{coeff}, which in the case of a NN potential
is the weights, biases, symmetry function parameters and other hyperparameters. 
These are read from file, and based on information on the architecture of the NN, 
the network is reconstructed as a set of matrices and 
vectors, according to the description in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}. This is achieved with the input script command,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> pair_coeff		* * &lt;path-to-saved-NN&gt; &lt;cutoff&gt; </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
where the wildcard asterisks signify that the coefficients span all atom types and are read from file. 
If there are more than one element, a seperate NN file is read for each one. 
An example is <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'> pair_coeff 	* * &lt;path-1&gt; &lt;path-2&gt; Si O &lt;cutoff&gt;</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Computing energies and forces</span></b>}
Now that the NNP is set up, we are ready to employ it in simulations. 
The process of computing energies and forces with the NNP is described as follows,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Initialize various variables like the vector of atom coordinates and the neighbour lists of each atom.
 <span style='color:#800000;'>\item</span> Loop through all atoms. For each atom <span style='color:#00a000;'>$i$</span>:
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
  <span style='color:#800000;'>\item</span> Construct the input vector to the NN by evaluating the symmetry functions for all 
        atomic pairs and triplets below the cutoff radius. 
	The coordinates, types, angles etc.<span style='color:#800000;'>\ </span>of all pairs and triplets must be stored for later force calculations. 
  <span style='color:#800000;'>\item</span> Evaluate the NN corresponding to the element of atom <span style='color:#00a000;'>$i$</span> to obtain its total potential energy.
  <span style='color:#800000;'>\item</span> Calculate the gradient of the energy w.r.t. the symmetry function values by backpropagation.
  <span style='color:#800000;'>\item</span> Differentiate the symmetry functions w.r.t. to the coordinates of all neighbours to obtain the total force on atom <span style='color:#00a000;'>$i$</span>.
 <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
This process determines the structure of the <span style='color:#800000;'>\texttt</span>{nn/manybody} <span style='color:#800000;'>\texttt</span>{compute} function. 
We start out by accessing various variables needed to compute the forces,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
<span style='color:#0057ae;'>double</span> **x = atom-&gt;x;      <span style='color:#898887;'>// atom coordinates</span>
<span style='color:#0057ae;'>double</span> **f = atom-&gt;f;      <span style='color:#898887;'>// forces</span>
<span style='color:#0057ae;'>int</span> *type = atom-&gt;type;    <span style='color:#898887;'>// type of each atom	</span>
<span style='color:#0057ae;'>int</span> nlocal = atom-&gt;nlocal; <span style='color:#898887;'>// no. of atoms belonging to current processor  </span>

<span style='color:#0057ae;'>int</span> inum = list-&gt;inum;     <span style='color:#898887;'>// length of list containing all neighbour lists</span>
<span style='color:#0057ae;'>int</span> *ilist = list-&gt;ilist;  <span style='color:#898887;'>// local indicies of all local atoms</span>
<span style='color:#0057ae;'>int</span> *numneigh = list-&gt;numneigh; <span style='color:#898887;'>// lengths of each neighbour list</span>
<span style='color:#0057ae;'>int</span> **firstneigh = list-&gt;firstneigh; <span style='color:#898887;'>// all neighbour lists</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
Next, we loop through all atoms in <span style='color:#800000;'>\texttt</span>{ilist}. For each atom, we evaluate 
the two-body and three-body symmetry functions with the symmetry parameters read from file. 
We also store certain information on all neighbours. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
<b>for</b> (<span style='color:#0057ae;'>int</span> ii = <span style='color:#b08000;'>0</span>; ii &lt; inum; ii++) {
  <span style='color:#0057ae;'>int</span> i = ilist[ii];
  <span style='color:#0057ae;'>int</span> itype = map[type[i]];

  <span style='color:#0057ae;'>double</span> xtmp = x[i][<span style='color:#b08000;'>0</span>];
  <span style='color:#0057ae;'>double</span> ytmp = x[i][<span style='color:#b08000;'>1</span>];
  <span style='color:#0057ae;'>double</span> ztmp = x[i][<span style='color:#b08000;'>2</span>];

  <span style='color:#898887;'>// neighbour list of atom i</span>
  <span style='color:#0057ae;'>int</span> *jlist = firstneigh[i];
  <span style='color:#0057ae;'>int</span> jnum = numneigh[i];
  
  <span style='color:#898887;'>// collect all pairs</span>
  <b>for</b> (<span style='color:#0057ae;'>int</span> jj = <span style='color:#b08000;'>0</span>; jj &lt; jnum; jj++) {
    <span style='color:#0057ae;'>int</span> j = jlist[jj];
    j &amp;= NEIGHMASK;
    tagint jtag = tag[j];
    <span style='color:#0057ae;'>int</span> jtype = map[type[j]];

    <span style='color:#0057ae;'>double</span> delxj = x[j][<span style='color:#b08000;'>0</span>] - xtmp;
    <span style='color:#0057ae;'>double</span> delyj = x[j][<span style='color:#b08000;'>1</span>] - ytmp;
    <span style='color:#0057ae;'>double</span> delzj = x[j][<span style='color:#b08000;'>2</span>] - ztmp;
    <span style='color:#0057ae;'>double</span> rsq1 = delxj*delxj + delyj*delyj + delzj*delzj;

    <b>if</b> (rsq1 &gt;= cutoffSquared) <b>continue</b>;   
    <span style='color:#0057ae;'>double</span> rij = sqrt(rsq1);
    
    <span style='color:#898887;'>// store relative coordinates (delxj, delyj, delzj), distance rij, </span>
    <span style='color:#898887;'>// index j and type jtype. Initialize input vector. </span>
    
    <span style='color:#898887;'>// apply two-body symmetry</span>
    <b>for</b> (<span style='color:#0057ae;'>int</span> s=<span style='color:#b08000;'>0</span>; s &lt; m_numberOfSymmFunc; s++)
      <b>if</b> ( m_parameters[s].size() == <span style='color:#b08000;'>3</span> )
          inputVector(<span style='color:#b08000;'>0</span>,s) += G2(rij, m_parameters[s][<span style='color:#b08000;'>0</span>],
                                 m_parameters[s][<span style='color:#b08000;'>1</span>], m_parameters[s][<span style='color:#b08000;'>2</span>]);
    
    <span style='color:#898887;'>// collect all triplets</span>
    <b>for</b> (<span style='color:#0057ae;'>int</span> kk = jj<span style='color:#b08000;'>+1</span>; kk &lt; jnum; kk++) {
      <span style='color:#898887;'>// apply three-body symmetry and </span>
      <span style='color:#898887;'>// store configurations of all triplets...</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We are now ready to evaluate the NN with the input vector of symmetry values. 
The NN is implemented as a function, shown here in the monatomic case for brevity,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
<span style='color:#0057ae;'>double</span> PairNNManyBody::network(arma::mat inputVector) {

  <span style='color:#898887;'>// no activation for input layer</span>
  m_preActivations[<span style='color:#b08000;'>0</span>] = inputVector;
  m_activations[<span style='color:#b08000;'>0</span>] = m_preActivations[<span style='color:#b08000;'>0</span>];

  <span style='color:#898887;'>// hidden layers</span>
  <b>for</b> (<span style='color:#0057ae;'>int</span> i=<span style='color:#b08000;'>0</span>; i &lt; nLayers; i++) {
    m_preActivations[i<span style='color:#b08000;'>+1</span>] = m_activations[i]*m_weights[i] + m_biases[i];
    m_activations[i<span style='color:#b08000;'>+1</span>] = sigmoid(m_preActivations[i<span style='color:#b08000;'>+1</span>]);
  }

  <span style='color:#898887;'>// identity activation for output layer</span>
  m_preActivations[nLayers<span style='color:#b08000;'>+1</span>] = m_activations[nLayers]*m_weights[nLayers] + 
				m_biases[nLayers];
  m_activations[nLayers<span style='color:#b08000;'>+1</span>] = m_preActivations[nLayers<span style='color:#b08000;'>+1</span>];

  <span style='color:#898887;'>// return activation of single output neuron</span>
  <b>return</b> m_activations[nLayers<span style='color:#b08000;'>+1</span>](<span style='color:#b08000;'>0</span>,<span style='color:#b08000;'>0</span>);
}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
where <span style='color:#800000;'>\texttt</span>{sigmoid(A)} is a function that returns the sigmoid <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>sigmoidActivationFunction</span>} 
of each element of a matrix <span style='color:#800000;'>\texttt</span>{A}. 
Each variable with the <span style='color:#800000;'>\texttt</span>{m<span style='color:#800000;'>\_</span>} prefix is an <span style='color:#800000;'>\texttt</span>{std::vector} containing a set of Armadillo matrices. 
For instance, <span style='color:#800000;'>\texttt</span>{m<span style='color:#800000;'>\_</span>weights[0]} is a matrix of all weights connecting the input layer 
and the first hidden layer. This results in highly readable code that is very similar to the mathematical 
description <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forwardPropMatrix</span>} <span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{The weight-activation multiplication is in reverse order in the code because we use row vectors instead of column vectors.
Also, the indexing of the weight and bias vectors are different than in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forwardPropMatrixLammpsChapter</span>}, as 
the activation and preactivation vectors also include the input layer. This also applies to the implementation
of backpropagation.},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l &amp;= </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{l-1} + </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l &amp;= f_l(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l)</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forwardPropMatrixLammpsChapter}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The NN is differentiated by employing backpropagation according to the procedure outlined in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
arma::mat PairNNManyBody::backPropagation() {

  <span style='color:#898887;'>// derivative of output neuron is 1</span>
  arma::mat output(<span style='color:#b08000;'>1</span>,<span style='color:#b08000;'>1</span>); output.fill(<span style='color:#b08000;'>1</span>);
  m_derivatives[nLayers<span style='color:#b08000;'>+1</span>] = output;

  <span style='color:#898887;'>// we can thus compute the error vectors for the other layers</span>
  <b>for</b> (<span style='color:#0057ae;'>int</span> i=nLayers; i &gt; <span style='color:#b08000;'>0</span>; i--) {
      m_derivatives[i] = ( m_derivatives[i<span style='color:#b08000;'>+1</span>]*m_weightsTransposed[i] ) %
          sigmoidDerivative(m_preActivations[i]);
  }

  <span style='color:#898887;'>// no activation function for input layer</span>
  m_derivatives[<span style='color:#b08000;'>0</span>] = m_derivatives[<span style='color:#b08000;'>1</span>] * m_weightsTransposed[<span style='color:#b08000;'>0</span>];

  <b>return</b> m_derivatives[<span style='color:#b08000;'>0</span>];
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
where <span style='color:#800000;'>\texttt</span>{<span style='color:#800000;'>\%</span>} signifies element-wise multiplication <span style='color:#00a000;'>$</span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'>$</span>,
and <span style='color:#800000;'>\texttt</span>{m<span style='color:#800000;'>\_</span>derivatives[i]} is the error vector <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}$</span> of layer <span style='color:#00a000;'>$i$</span>. 
We compare to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>backPropMatrix</span>}<span style='color:#898887;'>%</span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_l = </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>_l </span><span style='color:#606000;'>\odot</span><span style='color:#00a000;'> (</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}^T_{l+1}</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\updelta</span><span style='color:#00a000;'>}_{l+1}) </span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'> </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{backPropMatrixLammpsChapter}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Consequentially, the output and gradient of the NN is obtained in a compact way,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
evdwl = network(inputVector);
arma::mat dEdG = backPropagation();
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We remember from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>} that the force is the product of the gradient of the NN and 
the symmetry functions <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomkChainRule</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> F_{i,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>} = -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^{N_i+1}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> R_{i,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}} = </span>
<span style='color:#00a000;'> -</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^{N_j+1}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{s=1}^{M_j}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_j}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{j,s}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_{j,s}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> R_{i,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forceAtomkChainRuleLammpsChapter}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Here the NN potential differs from ordinary many-body potentials, which 
compute forces from each neighbour on-the-fly: The potential energy of atom <span style='color:#00a000;'>$i$</span>
can only be calculated w.r.t all neighbours <span style='color:#800000;'>\textit</span>{simultaneously}. We therefore have to loop over the neighbours once more
to calculate the forces. By storing the configurations of all pairs and triplets during energy computation above, we
avoid recalculations. We show an excerpt of the code that implements <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomkChainRuleLammpsChapter</span>}. 
First, for <span style='color:#00a000;'>$G_i^2$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G2</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
<b>for</b> (<span style='color:#0057ae;'>int</span> s=<span style='color:#b08000;'>0</span>; s &lt; m_numberOfSymmFunc; s++) {
  <b>if</b> ( m_parameters[s].size() == <span style='color:#b08000;'>3</span> ) {
    arma::mat dG2(<span style='color:#b08000;'>1</span>,neighbours);

    <span style='color:#898887;'>// calculate derivative of G2 for all pairs</span>
    dG2dR(Rij, m_parameters[s][<span style='color:#b08000;'>0</span>], 
          m_parameters[s][<span style='color:#b08000;'>1</span>], m_parameters[s][<span style='color:#b08000;'>2</span>], dG2);

    <span style='color:#898887;'>// chain rule: all pair forces</span>
    arma::mat fpairs = -dEdG(<span style='color:#b08000;'>0</span>,s) * dG2 / Rij;

    <span style='color:#898887;'>// loop through all pairs</span>
    <b>for</b> (<span style='color:#0057ae;'>int</span> l=<span style='color:#b08000;'>0</span>; l &lt; neighbours; l++) {
      <span style='color:#0057ae;'>double</span> fpair = fpairs(<span style='color:#b08000;'>0</span>,l);

      <span style='color:#898887;'>// force on atom i</span>
      fx2 -= fpair*drij(<span style='color:#b08000;'>0</span>,l);
      fy2 -= fpair*drij(<span style='color:#b08000;'>1</span>,l);
      fz2 -= fpair*drij(<span style='color:#b08000;'>2</span>,l);

      <span style='color:#898887;'>// force on atom j</span>
      f[tagsj[l]][<span style='color:#b08000;'>0</span>] += fpair*drij(<span style='color:#b08000;'>0</span>,l);
      f[tagsj[l]][<span style='color:#b08000;'>1</span>] += fpair*drij(<span style='color:#b08000;'>1</span>,l);
      f[tagsj[l]][<span style='color:#b08000;'>2</span>] += fpair*drij(<span style='color:#b08000;'>2</span>,l);
    }
  }
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We observe that the outer loop is over symmetry function values, while the inner loop is over all atom pairs <span style='color:#00a000;'>$(i,j)$</span>.
The vectors <span style='color:#800000;'>\texttt</span>{Rij} and <span style='color:#800000;'>\texttt</span>{drij} contain the distances to and relative coordinates of all neighbours of atom <span style='color:#00a000;'>$i$</span>.
Note that we assign each pair force <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}_{ij}$</span> both to atom <span style='color:#00a000;'>$i$</span> and neighbour <span style='color:#00a000;'>$j$</span>, according to the symmetry 
considerations (<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>changeOfCoordsSymmetry2</span>}) in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>}. 

The derivatives of <span style='color:#00a000;'>$G_i^5$</span> are also computed inside the outer loop over symmetry function values,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{C++}
  <span style='color:#898887;'>// G5</span>
  <b>else</b> {
    <b>for</b> (<span style='color:#0057ae;'>int</span> l=<span style='color:#b08000;'>0</span>; l &lt; neighbours<span style='color:#b08000;'>-1</span>; l++) {
      <span style='color:#0057ae;'>int</span> numberOfTriplets = arma::size(Riks[l])(<span style='color:#b08000;'>1</span>);

      <span style='color:#898887;'>// calculate forces for all triplets (i,j,k) for this pair (i,j)</span>
      <span style='color:#0057ae;'>double</span> fj3[<span style='color:#b08000;'>3</span>], fk3[<span style='color:#b08000;'>3</span>], dGj[<span style='color:#b08000;'>3</span>], dGk[<span style='color:#b08000;'>3</span>];
      <b>for</b> (<span style='color:#0057ae;'>int</span> m=<span style='color:#b08000;'>0</span>; m &lt; numberOfTriplets; m++) {
      
	<span style='color:#898887;'>// calculate derivatives dGj and dGk of G5 w.r.t. </span>
	<span style='color:#898887;'>// coordinates of atom j and k respectively...</span>

	<span style='color:#898887;'>// chain rule: force atoms i-j</span>
	fj3[<span style='color:#b08000;'>0</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGj[<span style='color:#b08000;'>0</span>];
	fj3[<span style='color:#b08000;'>1</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGj[<span style='color:#b08000;'>1</span>];
	fj3[<span style='color:#b08000;'>2</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGj[<span style='color:#b08000;'>2</span>];

	<span style='color:#898887;'>// chain rule: force atoms i-k</span>
	fk3[<span style='color:#b08000;'>0</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGk[<span style='color:#b08000;'>0</span>];
	fk3[<span style='color:#b08000;'>1</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGk[<span style='color:#b08000;'>1</span>];
	fk3[<span style='color:#b08000;'>2</span>] = -dEdG(<span style='color:#b08000;'>0</span>,s) * dGk[<span style='color:#b08000;'>2</span>];

	<span style='color:#898887;'>// force on atom i </span>
	fix -= fj3[<span style='color:#b08000;'>0</span>] + fk3[<span style='color:#b08000;'>0</span>];
	fiy -= fj3[<span style='color:#b08000;'>1</span>] + fk3[<span style='color:#b08000;'>1</span>];
	fiz -= fj3[<span style='color:#b08000;'>2</span>] + fk3[<span style='color:#b08000;'>2</span>];

	<span style='color:#898887;'>// force on atom j</span>
	f[tagsj[l]][<span style='color:#b08000;'>0</span>] += fj3[<span style='color:#b08000;'>0</span>];
	f[tagsj[l]][<span style='color:#b08000;'>1</span>] += fj3[<span style='color:#b08000;'>1</span>];
	f[tagsj[l]][<span style='color:#b08000;'>2</span>] += fj3[<span style='color:#b08000;'>2</span>];

	<span style='color:#898887;'>// force on atom k</span>
	f[tagsk[l][m]][<span style='color:#b08000;'>0</span>] += fk3[<span style='color:#b08000;'>0</span>];
	f[tagsk[l][m]][<span style='color:#b08000;'>1</span>] += fk3[<span style='color:#b08000;'>1</span>];
	f[tagsk[l][m]][<span style='color:#b08000;'>2</span>] += fk3[<span style='color:#b08000;'>2</span>];
	
	<span style='color:#898887;'>// compute virial... </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We exploit the symmetries 
(<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>changeOfCoords3body1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>changeOfCoords3body2</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>changeOfCoords3body3</span>})
in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>} also here, 
by assigning the negative of the sum of the computed forces
<span style='color:#800000;'>\texttt</span>{fj3} and <span style='color:#800000;'>\texttt</span>{fk3} to atom <span style='color:#00a000;'>$i$</span>, while giving the negative of these forces to atom <span style='color:#00a000;'>$j$</span> and <span style='color:#00a000;'>$k$</span> seperately.

In fact, this is a requirement for our implementation 
to work: <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomkChainRuleLammpsChapter</span>} states that
the force on atom <span style='color:#00a000;'>$i$</span> is the derivative of the energies of all neighbours and <span style='color:#00a000;'>$i$</span> itself. This also applies to all neighbours <span style='color:#00a000;'>$j$</span>
of atom <span style='color:#00a000;'>$i$</span>: Each neighbour experiences a force contribution  <span style='color:#00a000;'>$</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i / </span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> R_{j,</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}$</span> from atom <span style='color:#00a000;'>$i$</span>. To avoid 
having to store all atomic energies <span style='color:#00a000;'>$E_i$</span>, we therefore take care of this contribution when <span style='color:#00a000;'>$E_i$</span> is available. 
The alternative is to compute and store the energies and the environments of all atoms before calculating the forces, but this
is tedious and requires too much storage for large systems.

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Summary</span></b>}
In this chapter we have illustrated some of the basic usage of LAMMPS by explaining the different commands of an example 
input script. Further, we have briefly discussed the class structure of LAMMPS, and how to extend this structure 
with a custom potential. 

The example code from the NNP class is an implementation of different theoretical aspects 
introduced in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MD</span>}, <span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>sec:machineLearning</span>}, and <span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>sec:NNPs</span>}. We showed how a
NN can be evaluated as a series of matrix-vector operations (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}), 
how to differentiate a NN with the backpropagation algorithm  (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:backprop</span>}), and how to 
obtain the forces when employing the Behler-Parrinello method (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>}). 

We will now continue with an introduction on how to train NNs with the machine learning library TensorFlow. 






<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>TensorFlow</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:tensorFlow</span>}
TensorFlow <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Abadi15,Abadi16</span>} is an open source software library 
developed by Google 
for numerical computation using data flow graphs (DFG). 
A DFG is a graphical representation of how data is
advanced through a system, including what kind of information is input
to and output from the system and where data is stored.
The data flow is represented as a directed graph
with a number of <span style='color:#800000;'>\textit</span>{nodes} that is connected by
<span style='color:#800000;'>\textit</span>{edges}. In TensorFlow, nodes represent
mathematical operations, while the edges represent
the multidimensional data arrays (tensors) 
communicated between them.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.4<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/DFG.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Data flow graph for the solutions (<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>quadraticEquationSolution1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>quadraticEquationSolution2</span>}) of a 
	   quadratic equation <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticEquation</span>}. The nodes (green circles) are mathematical 
	   operations connected by edges (arrows) that show the data flow direction. Inputs and outputs
	   are marked as rectangular boxes. Reproduced from:
  <span style='color:#800000;'>\href</span>{http://web.cecs.pdx.edu/mperkows/temp/JULY/data-flow-graph.pdf}{web.cecs.pdx.edu}}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:DFG</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

A simple example of a DFG is displayed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:DFG</span>}, which is a visualization of the flow of data
for the (real) solutions 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> x_1 &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{-b + </span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{b^2 - 4ac}}{2a}  </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{quadraticEquationSolution1} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> x_2 &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{-b - </span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{b^2 - 4ac}}{2a}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{quadraticEquationSolution2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
of a quadratic equation
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> ax^2 + bx + c = 0</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{quadraticEquation}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
We observe that each node (green circles)
contain the mathematical operations needed to compute
(<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>quadraticEquationSolution1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>quadraticEquationSolution2</span>}), while the arrows
(edges) show what data each node receive and where
the output is sent. The inputs and outputs
of the graph is marked with rectangular
boxes. 

TensorFlow was originally developed by 
the Google Brain Team<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://research.google.com/teams/brain/}{research.google.com}}
with the purpose of conducting machine learning and 
deep neural networks research, but is 
applicable in other domains as well, e.g.<span style='color:#800000;'>\ </span>for
solving partial differential equations. 
The package incorporates
a wide variety of machine learning algorithms, including
convolutional neural networks and recurrent neural networks.
A computation expressed using TensorFlow can 
be executed on a wide variety of devices and systems, 
from mobile phones to large-scale distributed systems
and GPUs. 

In this chapter we will go through a simple 
example on how to build a computational graph 
with TensorFlow, and how to train a
feed-forward neural network. We begin with a brief discussion on how to install the software. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Installing TensorFlow</span></b>}
The easiest way to install TensorFlow on Ubuntu<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://www.tensorflow.org/install/install<span style='color:#800000;'>\_</span>linux}{www.tensorflow.org/install/}}
is to use pip<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://pip.pypa.io/en/stable/}{pip.pypa.io/en/}},
a package management system for software written in 
Python, installed with the command
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> $ sudo apt-get install python-pip python-dev</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
TensorFlow can be installed with or without GPU 
support. The GPU version requires a system with
 CUDA-enabled NVIDIA GPUs
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://developer.nvidia.com/cuda-gpus}
{https://developer.nvidia.com/cuda-gpus}}. 
We have used the CPU version for the
development of our code, which is installed
with the command
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> $ pip install tensorflow</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
A simple
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[language=bash]
<span style='color:#a08000;'> $ python -c &quot;import tensorflow&quot;</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
will test if the installation was succesful. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>TensorFlow basic usage</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:TensorFlowBasic</span>}
TensorFlow relies on highly-optimized
C<span style='color:#800000;'>\texttt</span>{++} for its computations, but offers
APIs <span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://www.tensorflow.org/api_docs/}
{https://www.tensorflow.org/api<span style='color:#800000;'>\_</span>docs/}}
in both Python,
C<span style='color:#800000;'>\texttt</span>{++} and a few other languages. 
The <span style='color:#800000;'>\texttt</span>{Python} API is at present the most
complete, the easiest to use and the best documented.
We have therefore chosen to use Python
to train neural networks. For a discussion
on ways to import a trained NN to C<span style='color:#800000;'>\texttt</span>{++} see
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:extendingLammps</span>} and <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:transferNN</span>}. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Hello world</span></b>}
As already mentioned, a TensorFlow computation
is described by a graph which consists of nodes
that communicate with multi-dimensional arrays along
interconnected edges. Each node has zero or more
outputs, and represents the instantiation
of an <span style='color:#800000;'>\textit</span>{operation}. Each operation
occurs within a so-called <span style='color:#800000;'>\textit</span>{session}. 
A <span style='color:#800000;'>\texttt</span>{Hello world!} program can be written as
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
<span style='color:#924c9d;'>import</span> tensorflow <span style='color:#924c9d;'>as</span> tf

hello <b>=</b> tf.constant(<span style='color:#bf0303;'>'Hello world!'</span>)
sess <b>=</b> tf.Session()
<span style='color:#0057ae;'>print</span> sess.run(hello)
<span style='color:#898887;'># --&gt; Hello world!</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
This simple example outlines the general
workflow for building a TensorFlow graph,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Declare nodes (operations)
 <span style='color:#800000;'>\item</span> Initiate a session
 <span style='color:#800000;'>\item</span> Run operations within session
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
The only operation in the above example
is the built-in TF constant that holds the 
<span style='color:#800000;'>\texttt</span>{Hello world!} string, and the 
corresponding DFG only consists of one node. 
In more involved examples, visualizing 
the DFG can be useful for finding out how 
the script works. This is done with the 
included TF visualization tool TensorBoard.
We will now look at how to construct a neural network with TF and
visualize the corresponding DFG. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Creating a neural network</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:TensorFlowCreatingNN</span>}
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>} we demonstrated that a NN can be constructed 
as a set of matrix-vector multiplications and additions which are input to certain activation functions. 
The equations for evaluating a NN were <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forwardPropMatrix</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l &amp;= </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{W}_l</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_{l-1} + </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{b}_l </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{preActivation2} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{y}_l &amp;= f_l(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{u}_l) </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{activation2}</span>
<span style='color:#00a000;'> </span><span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
We now want to construct a NN with 3 inputs neurons, one hidden layer with 10 neurons and 1 output neuron. 
We use the sigmoid <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>sigmoidActivationFunction</span>} activation function for the hidden layer, while 
the output layer have the identity activation function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>outputActivation</span>}.
This can be achieved with the following Python script,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
<span style='color:#924c9d;'>import</span> tensorflow <span style='color:#924c9d;'>as</span> tf

<span style='color:#898887;'># NN parameters</span>
inputs  <b>=</b> <span style='color:#b08000;'>3</span>
nodes   <b>=</b> <span style='color:#b08000;'>10</span>
outputs <b>=</b> <span style='color:#b08000;'>1</span>

<span style='color:#898887;'># declare input tensor</span>
inputLayer <b>=</b> tf.placeholder( tf.float32, [<span style='color:#006e28;'>None</span>,inputs] )

<span style='color:#898887;'># weights input layer -&gt; hidden layer and biases hidden layer</span>
W1 <b>=</b> tf.Variable( tf.random_normal([inputs,nodes]) )
b1 <b>=</b> tf.Variable( tf.constant(<span style='color:#b08000;'>0.1</span>, shape<b>=</b>[nodes]) )

<span style='color:#898887;'># activation hidden layer</span>
preAct1 <b>=</b> tf.add( tf.matmul(inputLayer, W1), b1 ) 
act1 <b>=</b> tf.nn.sigmoid(preAct1)

<span style='color:#898887;'># weights hidden layer -&gt; output layer and bias output layer</span>
W2 <b>=</b> tf.Variable( tf.random_normal([nodes,outputs]) )
b2 <b>=</b> tf.Variable( tf.constant(<span style='color:#b08000;'>0.1</span>, shape<b>=</b>[outputs]) )

<span style='color:#898887;'># activation output layer</span>
preActOutput <b>=</b> tf.add( tf.matmul(act1, W2), b2 ) 
activationOutput <b>=</b> preActOutput
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
There are several aspects of the TensorFlow Python API to comment on here. 
We observe that the input layer is declared as a TF <span style='color:#800000;'>\texttt</span>{placeholder}. This is a special
type of TF tensor that is not initialized with any values, but is rather fed with data on its execution.
Specifying the first component of its shape as <span style='color:#800000;'>\texttt</span>{None} signifies that we can supply an arbitrary 
number of data points simultaneously.
These properties enables the same NN to be evaluated for different data vectors of varying size on-the-fly during training, 
explained in more detail below. 

The weights and biases are declared as TF <span style='color:#800000;'>\texttt</span>{Variables}. These are the trainable parameters 
of the computational graph, and are normally constructed as random numbers. In the present case,
the weights are normally distributed numbers while the biases are all set to the same value 0.1. 
We note that the shapes of the weight matrices and bias vectors are in accordance with the description in 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>}.

The activations (outputs) of each layer are calculated according to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>preActivation2</span>} and <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>activation2</span>}. 
Together with the input <span style='color:#800000;'>\texttt</span>{placeholder}, these are of type <span style='color:#800000;'>\texttt</span>{tf.Tensor}, i.e.<span style='color:#800000;'>\ </span>nodes in the computational 
graph. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Visualizing the graph</span></b>}
We can use TensorBoard to visualize the DFG of the above script. The DFG for the present example is quite
simple, but for more involved models it quickly becomes difficult to keep track of all the different components. 
A convenient tool to order and group the DFG is <span style='color:#800000;'>\textit</span>{name scoping}. Both <span style='color:#800000;'>\texttt</span>{Tensors} and <span style='color:#800000;'>\texttt</span>{Variables} can 
be name scoped and TensorBoard employs this information to define a hierarchy of the nodes in the graph. 
An example of name scoping is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
 <b>with</b> tf.name_scope(<span style='color:#bf0303;'>'layer1'</span>):
    W1 <b>=</b> tf.Variable( tf.random_normal([inputs,nodes]), name<b>=</b><span style='color:#bf0303;'>'weights'</span>)
    b1 <b>=</b> tf.Variable( tf.constant(<span style='color:#b08000;'>0.1</span>, shape<b>=</b>[nodes]), name<b>=</b><span style='color:#bf0303;'>'biases'</span>)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
which results in the following names,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>itemize</span>}
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\texttt</span>{layer1/weights}
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\texttt</span>{layer1/biases}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>itemize</span>}
Additionally, TensorBoard will group the two variables together under <span style='color:#800000;'>\texttt</span>{layer1}. 
To save the graph to file we have to start a TF session. The following script 
deploys name scoping, initiates a session and writes the DFG to file,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
<span style='color:#924c9d;'>import</span> tensorflow <span style='color:#924c9d;'>as</span> tf

<span style='color:#898887;'># NN parameters</span>
inputs <b>=</b> <span style='color:#b08000;'>3</span>
nodes <b>=</b> <span style='color:#b08000;'>10</span>
outputs <b>=</b> <span style='color:#b08000;'>1</span>

<b>with</b> tf.name_scope(<span style='color:#bf0303;'>'inputLayer'</span>):
    inputData <b>=</b> tf.placeholder(tf.float32, [<span style='color:#006e28;'>None</span>,inputs], name<b>=</b><span style='color:#bf0303;'>'inputData'</span>)

<b>with</b> tf.name_scope(<span style='color:#bf0303;'>'layer1'</span>):
    
    W1 <b>=</b> tf.Variable( tf.random_normal([inputs,nodes]), name<b>=</b><span style='color:#bf0303;'>'weights'</span> )
    b1 <b>=</b> tf.Variable( tf.constant(<span style='color:#b08000;'>0.1</span>, shape<b>=</b>[nodes]), name<b>=</b><span style='color:#bf0303;'>'biases'</span> )
 
    preAct1 <b>=</b> tf.add( tf.matmul(inputData, W1), b1, name<b>=</b><span style='color:#bf0303;'>'preActivation'</span> ) 
    act1 <b>=</b> tf.nn.sigmoid(preAct1, name<b>=</b><span style='color:#bf0303;'>'activation'</span>)

<b>with</b> tf.name_scope(<span style='color:#bf0303;'>'outputLayer'</span>):
    
    W2 <b>=</b> tf.Variable( tf.random_normal([nodes,outputs]), name<b>=</b><span style='color:#bf0303;'>'weights'</span> )
    b2 <b>=</b> tf.Variable( tf.constant(<span style='color:#b08000;'>0.1</span>, shape<b>=</b>[outputs]), name<b>=</b><span style='color:#bf0303;'>'biases'</span> )

    preActOutput <b>=</b> tf.add( tf.matmul(act1, W2), b2, name<b>=</b><span style='color:#bf0303;'>'preActivation'</span> ) 
    actOutput <b>=</b> tf.identity(preActOutput, name<b>=</b><span style='color:#bf0303;'>'activation'</span>)
    
<span style='color:#898887;'># initialization node</span>
initOperation <b>=</b> tf.global_variables_initializer()

<span style='color:#898887;'># start session</span>
<b>with</b> tf.Session() <span style='color:#924c9d;'>as</span> sess:
    
    <span style='color:#898887;'># initialize all variables</span>
    sess.run(initOperation)
    
    <span style='color:#898887;'># write summaries</span>
    tf.summary.FileWriter(<span style='color:#bf0303;'>'Summaries'</span>, sess.graph)
 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
There are two new TF function calls to comment on in this script. First, we have the line
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
 initOperation <b>=</b> tf.global_variables_initializer()
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
Constants are initialized when we call <span style='color:#800000;'>\texttt</span>{tf.constant}, and their value can never change. 
By contrast, variables need to be initialized using a seperate operation within a session. 
The function <span style='color:#800000;'>\texttt</span>{tf.global<span style='color:#800000;'>\_</span>variables<span style='color:#800000;'>\_</span>initializer} adds such an operation to the graph, which initializes
all global variables when run with <span style='color:#800000;'>\texttt</span>{sess.run(initOperation)}. 

Secondly, the graph is written to file for visualization with the function <span style='color:#800000;'>\texttt</span>{tf.summary.FileWriter}. 
The first argument is the location of the files, while the second is the graph that is launched in the
present session. 

The resulting graph is displayed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:graphExample</span>}. 
By default, only the top of the hierarchy defined by name scoping is shown. In our case, this
is the three NN layers. In this figure, the output layer is expanded to reveal all the nodes in this scope. 
The nodes are marked by ellipses, while the rectangles are namespaces defining another level in the hierarchy, 
each containing more nodes. The edges show how the nodes are connected<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{Each TF variable and constant are actually made up of several nodes, including <span style='color:#800000;'>\textit</span>{assign} and 
<span style='color:#800000;'>\textit</span>{read} operations. See TF API for more information.}.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}[H]
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/tensorBoardExample.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of a computational graph in TensorBoard of a NN consisting of an input layer, one 
	   hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces
	   containing several nodes.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:graphExample</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\noindent</span> Next, we will demonstrate how to train a NN with TensorFlow. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Training a NN with TensorFlow</span></b>} 
To train the NN described in the previous section, we need two additional components:
a cost function and an optimizer. The mean-square-error cost function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>} is available as a 
built-in function in TF, <span style='color:#800000;'>\texttt</span>{tf.nn.l2<span style='color:#800000;'>\_</span>loss}. To compute the error, we need to supply known 
target values that is fed into a new <span style='color:#800000;'>\texttt</span>{placeholder},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
 outputData <b>=</b> tf.placeholder(tf.float32, [<span style='color:#006e28;'>None</span>,outputs], name<b>=</b><span style='color:#bf0303;'>'outputData'</span>)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
The cost function is now defined as
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
 cost <b>=</b> tf.nn.l2_loss( tf.subtract(actOutput, outputData), name<b>=</b><span style='color:#bf0303;'>'cost'</span>)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
This adds an operation to the graph that calculates the mean-square-error between the NN prediction <span style='color:#800000;'>\texttt</span>{actOutput}
on the data set
and the known target values <span style='color:#800000;'>\texttt</span>{outputData}. In this work, the input data is configurations of atoms, 
while the target values are the potential energies of these configurations. 

A number of gradient descent optimizers are implemented in TensorFlow, including
vanilla gradient descent <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>gradientDescent</span>}, Momentum <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Qian99</span>}, Adagrad <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Duchi11</span>}, 
Adadelta <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Zeiler12</span>} and Adam <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kingma14</span>}.
To add the Adam optimizer operation to the graph, we simply write
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
optimizer <b>=</b> tf.train.AdamOptimizer()
trainStep <b>=</b> optimizer.minimize(trainCost)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
To sum up, we have added the following to the script in the previous section (now with name scoping),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
outputData <b>=</b> tf.placeholder(tf.float32, [<span style='color:#006e28;'>None</span>,outputs], name<b>=</b><span style='color:#bf0303;'>'outputData'</span>)

<span style='color:#898887;'># cost function</span>
<b>with</b> tf.name_scope(<span style='color:#bf0303;'>'cost'</span>):
    error <b>=</b> tf.subtract(actOutput, outputData, name<b>=</b><span style='color:#bf0303;'>'deviation'</span>)
    cost <b>=</b> tf.nn.l2_loss(error, name<b>=</b><span style='color:#bf0303;'>'L2norm'</span>)

<span style='color:#898887;'># optimizer</span>
<b>with</b> tf.name_scope(<span style='color:#bf0303;'>'optimizer'</span>):
    optimizer <b>=</b> tf.train.AdamOptimizer(learning_rate<b>=</b><span style='color:#b08000;'>0.001</span>, name<b>=</b><span style='color:#bf0303;'>'Adam'</span>)
    trainStep <b>=</b> optimizer.minimize(trainCost, name<b>=</b><span style='color:#bf0303;'>'trainStep'</span>)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}[b]
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/tensorBoardTraining.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of a computational graph in TensorBoard for training a NN consisting of an input layer, one 
	   hidden layer and an output layer. Nodes are marked by ellipses, the rectangles are namespaces
	   containing several nodes. A cost function describing the error of the NN fit and an optimizer that minimizes
	   this error is included in the graph. The optimizer namespace is expanded to display its contents:
	   <span style='color:#800000;'>\texttt</span>{gradients} computes the gradient of all nodes in the graph, <span style='color:#800000;'>\texttt</span>{trainStep} implements
	   the algorithm for updating the NN parameters, while <span style='color:#800000;'>\texttt</span>{beta1<span style='color:#800000;'>\_</span>power} and <span style='color:#800000;'>\texttt</span>{beta2<span style='color:#800000;'>\_</span>power}
	   are the parameters of the Adam optimizer algorithm.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:tensorBoardTraining</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
The resultant graph can be seen in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:tensorBoardTraining</span>}. 
The optimizer namespace is expanded to display its contents. 
We observe that the optimizer is connected directly to all other parts of the graph, enabling it
to compute the derivative of the cost function with respect to all the weights and biases. This task is
performed by the <span style='color:#800000;'>\texttt</span>{gradients} operation, that in turn sends the total gradient to <span style='color:#800000;'>\texttt</span>{trainStep}, 
which implements the Adam algorithm for updating the NN parameters. 

We are now ready to launch this graph in a training session. We assume that input data
and target values are available as <span style='color:#800000;'>\texttt</span>{NumPy} arrays <span style='color:#800000;'>\texttt</span>{xTrain} and <span style='color:#800000;'>\texttt</span>{yTrain} respectively. 
The code example implements the different variants of gradient descent described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:gradientDescentVariants</span>}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}.
<b>for</b> epoch <b>in</b> <span style='color:#0057ae;'>xrange</span>(numberOfEpochs<b>+</b><span style='color:#b08000;'>1</span>): 
    
    <span style='color:#898887;'># for shuffling the data set</span>
    indicies <b>=</b> np.random.choice(trainSize, trainSize, replace<b>=</b><span style='color:#006e28;'>False</span>)
    
    <span style='color:#898887;'># offline learning / batch gradient descent</span>
    <b>if</b> batchSize <b>==</b> trainSize:    
	
	<span style='color:#898887;'># pick whole set in random order               </span>
	xBatch <b>=</b> xTrain[indicies]
	yBatch <b>=</b> yTrain[indicies]
	
	<span style='color:#898887;'># train</span>
	sess.run(trainStep, 
		 feed_dict<b>=</b>{inputLayer: xBatch, outputData: yBatch})
	
    <span style='color:#898887;'># online learning</span>
    <b>else</b>:                      
	<span style='color:#898887;'># loop through whole set, train each iteration</span>
	<b>for</b> b <b>in</b> <span style='color:#0057ae;'>xrange</span>(numberOfBatches):
	    batch <b>=</b> indicies[b<b>*</b>batchSize:(b<b>+</b><span style='color:#b08000;'>1</span>)<b>*</b>batchSize]
	    xBatch <b>=</b> xTrain[batch]
	    yBatch <b>=</b> yTrain[batch]
	    
	    <span style='color:#898887;'># train</span>
	    sess.run(trainStep, 
	             feed_dict<b>=</b>{inputLayer: xBatch, outputData: yBatch})
	    
    trainError <b>=</b> sess.run(cost, 
                        feed_dict<b>=</b>{inputLayer: xBatch, outputData: yBatch})
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We observe that the training step is performed with the single line,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
sess.run(trainStep, feed_dict<b>=</b>{inputLayer: xBatch, outputData: yBatch})
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
which runs the <span style='color:#800000;'>\texttt</span>{trainStep} operation defined above. This operation initiates a series of
calculations that is performed automatically by TensorFlow. 
First, the NN is evaluated on the training batch <span style='color:#800000;'>\texttt</span>{xBatch} and the output is compared to the corresponding
batch of target values <span style='color:#800000;'>\texttt</span>{yBatch}. The training batch is fed to the NN through the placeholder
<span style='color:#800000;'>\texttt</span>{inputLayer}, while <span style='color:#800000;'>\texttt</span>{outputData} receives the target batch. Next, the cost function 
is calculated, and the error is backpropagated through the DFG to obtain its gradient. Finally, 
the Adam update rule is applied and the weights and biases adjusted. 



<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Constructing a neural network potential</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:constructingNNP</span>}
The general workflow for constructing and applying a high-dimensional neural network potential (NNP) is as follows: 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Generate training data relevant for the intended application of the NNP.
 <span style='color:#800000;'>\item</span> Train a NN to fit the data, employing a suitable set of hyperparameters.
 <span style='color:#800000;'>\item</span> Use the analytical expression for the eventual NNP as a potential in MD simulations.
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
In this chapter we will go into details regarding these three steps. 
We describe different ways of assembling 
an adequate reference set for the NN, which is the most crucial factor for the performance of 
the final potential. Further, we discuss how suitable hyperparameters can be determined, including the 
symmetry function parameters. Lastly, we compare the performance of two different methods for evaluating a
trained NN in simulations. 




<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Selecting the reference set</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:selectingTrainingData</span>}
The number of possible configurations of atomic or molecular systems quickly grows very large. 
Consider for instance water <span style='color:#00a000;'>$</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{H}_2</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{O}$</span>, which is a three-body molecule. The configuration of the
three atoms can be described by three internal coordinates, each with a certain range of values. 
If each range is divided into <span style='color:#00a000;'>$N$</span> equal segments, the total number of distinct configurations is <span style='color:#00a000;'>$N^3$</span>. 
If we were to construct an <span style='color:#800000;'>\textit</span>{ab inito} potential that characterizes this system, we would have to compute the energies 
and forces of all these configurations. 
Such a brute-force approach may be feasible for a three-atom molecule, but for more complicated systems
it quickly becomes practically impossible due to the computational cost. 

In this work we do not use <span style='color:#800000;'>\textit</span>{ab inito} methods to compute energies and forces, but this problem is still highly relevant
due to the high-dimensional nature of our approach. The average number of neighbours for the Stillinger-Weber Si simulations
in this thesis is about <span style='color:#00a000;'>$n=6$</span> (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}), which is a small number compared to most other systems.  
This number is of great importance to the efficiency of the fitting process because it defines the dimensionality 
of the problem,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> When increasing <span style='color:#00a000;'>$n$</span>, each symmetry function is composed of 
 more terms. The number of terms <span style='color:#00a000;'>$N_{terms}$</span> scales like <span style='color:#00a000;'>$N_{terms} </span><span style='color:#606000;'>\propto</span><span style='color:#00a000;'> n$</span> for the radial symmetry functions, 
 but for the angular symmetry functions we have that <span style='color:#00a000;'>$N_{terms} </span><span style='color:#606000;'>\propto</span><span style='color:#00a000;'> n^2$</span>. 
 <span style='color:#800000;'>\item</span> A larger <span style='color:#00a000;'>$n$</span> leads in general to a larger configuration space to fit, thus additional symmetry functions and 
 a larger data set is required. 
 <span style='color:#800000;'>\item</span> The number of NN parameters must also be increased to obtain the same error as for a lower-dimensional problem
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
These observations demonstrate that performance is a critical factor whether we fit <span style='color:#800000;'>\textit</span>{ab inito} data or 
not. However, we know that only a small subset of the possible configurations of a system is physically realizable. 
Most of the configurations will not be visited by a system obeying Newton's laws, and are therefore not important to
the dynamics. We thus need some kind of importance sampling method that adequately samples this subset. 
We realize that the obvious way to achieve this is to sample configurations from molecular dynamics simulations, which
yields precisely the subset we are looking for. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Iterative molecular dynamics sampling</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:iterativeMDsampling</span>}
In this thesis we employ an iterative scheme to construct a NNP with data sampled from MD simulations. 
The method have been used by e.g. Raff et al <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff12</span>} and Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>}, and is as follows
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Sample an initial data set from MD simulations. <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sample</span>} 
 <span style='color:#800000;'>\item</span> Calculate the energies and optionally the forces of this initial data set with a chosen method. <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>refEnergies</span>}
 <span style='color:#800000;'>\item</span> Train a NN on the data from <span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>sample</span>}. with reference values from <span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>refEnergies</span>}. to
 construct a preliminary NN potential.
 <span style='color:#800000;'>\item</span> Carry out simulations using the NNP to find new relevant structures. 
 <span style='color:#800000;'>\item</span> Compute the energies (and forces) of the new structures with the method from 2. and add them to the data set
 <span style='color:#800000;'>\item</span> Train a NN on the extended data set to improve the NNP
 <span style='color:#800000;'>\item</span> Repeat 4-6. until no new relevant configurations are found. 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>NNPalgorithm</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
The idea is to improve the NNP iteratively and in a self-consistent 
way by applying it in MD simulations to find 
''holes'' in the data set, i.e.<span style='color:#800000;'>\ </span>configurations on which the NNP predicts incorrect energies and forces. The process
is different depending on whether we are constructing a NNP from <span style='color:#800000;'>\textit</span>{ab inito} electronic structure data
or reproducing known, empirical MD potentials. 

For the latter, we construct the initial data set by sampling configurations from MD simulations that is relevant
for the intended applications of the NNP. Furthermore, we are extracting both coordinates and energies from the simulations, i.e.<span style='color:#800000;'>\</span>
we get both the input data in step 1 and the target values in step 2 simultaneously. To compute the energies in
step 4, we simply run a pseudo-simulation on the new structures. This is easily done in LAMMPS by using 
the <span style='color:#800000;'>\texttt</span>{rerun} command, which tells LAMMPS to compute the energies and forces of atomic configurations 
supplied from file without doing any integration. 

When constructing an <span style='color:#800000;'>\textit</span>{ab inito} NNP however, we are immediately faced with a problem when trying to execute this process: 
to run MD simulations in step 1 we need the PES we are trying to develop. 
This is often termed as the ''self-starting problem'', 
and several procedures have been suggested for handling it. 

One option is obviously to generate random configurations. This does however take us back to the 
original problem discussed in the beginning of this chapter: The large configuration space volume of
high-dimensional systems enforces the use of importance sampling methods. 
It is highly unlikely that random sampling will yield the important regions of configuration space, thus 
vast amounts of data is needed. Nonetheless, random sampling can be applicable to small systems of 1-3 atoms where
the complete configuration space can be represented without too much effort, see <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:LJValidation</span>}. 

Another option is to use chemical intuition to select certain configurations of 
the system under investigation. 
For instance, if a specific chemical reaction is studied, one can use previous knowledge about this reaction to 
select configurations along the reaction path <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Ischtwan94</span>}. The disadvantage of this approach is that
such information is not always available. 

A more effective method has been presented by Raff et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff05</span>}, where a semi-empirical
potential has been deployed to initiate the sampling. If a reasonable potential is chosen and an adequate
sampling method is devised (discussed below), several thousand physically relevant structures can be obtained
that will constitute the initial data set. It has also been shown <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff05</span>} that the final PES obtained 
by the iterative sampling of MD trajectories is not sensitive to the choice of the empirical surface employed 
in the initial step. 

In this thesis we do not construct <span style='color:#800000;'>\textit</span>{ab inito} NNPs, thus we will not go into further details
of the above methods. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Sampling algorithms</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:samplingAlgorithms</span>}
We have seen that MD simulations supply the data that is needed to construct a NN potential, but we 
also have to consider how this sampling should be performed to identify new relevant structures.
NNs have a very flexible functional form which makes them able to accurately fit data sets, 
but they can also yield potentially large energy and force errors 
when evaluated for configurations that are very different from the ones included in the data set.
It is therefore of vital importance to include all structures that are relevant for the intended
application of the NNP.

Sampling methods should in general be optimized to produce a reasonably uniform density of 
data points in those regions of configuration space that are relevant to the application of the fit <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff12</span>}. 
If there are regions with very high point density compared to others, the NN will be trained to accurately fit these, 
at the cost of regions that are poorly represented. An exception to this rule is regions where the potential gradient, or forces, 
are large. These regions, typically configurations where two atoms are close to each other and experience strong
repulsive forces, are the most difficult for the NN to fit, 
and should therefore have a higher density. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Initial sampling</span></b>}
The initial data set is supposed to function as a starting point for the construction of the NNP. 
In this work we use empirical MD potentials to do the initial sampling. Before the process is initiated, 
we must decide on the specific application for the NNP that we want to develop. This entails 
defining a range of temperatures, pressures etc.<span style='color:#800000;'>\ </span>that the final NNP will be valid for, and sample
configurations based on these. 

The time interval between selecting configurations during the MD simulations must also be decided. 
One alternative is to simply sample points at constant time intervals. This approach will
however result in a non-uniform density of data points considering that atoms spend most of their time
in regions where the forces are small, at least in equilibrium MD. 

To sample configuration space with something close to a uniform density, the time interval <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span> between 
sampling needs to be a function of the atomic accelerations (or forces). One such variable 
interval sampling algorithm, suggested by Pukrittayakamee et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Pukrittayakamee09</span>}, is given as
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'> = </span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/a_{</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t, </span>
<span style='color:#00a000;'>&amp;</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{if floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/a_{</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] &gt; 0 </span><span style='color:#606000;'>\\</span>
<span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/a_{</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t + </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t, </span>
<span style='color:#00a000;'>&amp;</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{if floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/a_{</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] = 0</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#606000;'>\label</span><span style='color:#00a000;'>{variableIntervalSampling}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$a_i$</span> is the absolute value of the acceleration of atom <span style='color:#00a000;'>$i$</span>, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t$</span> is the integration step size, 
<span style='color:#00a000;'>$a_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}$</span> is the maximum acceleration of a chosen molecule, and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}(x)$</span> yields
the integer part of <span style='color:#00a000;'>$x$</span>. The constant <span style='color:#00a000;'>$</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>$</span> is system dependent and must be determined empirically. It is the value 
of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>$</span> that determines the sampling frequency. We see from the second condition in <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>variableIntervalSampling</span>} that
configurations with an acceleration <span style='color:#00a000;'>$a_i &gt; </span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>$</span> yields <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t$</span>, i.e.<span style='color:#800000;'>\ </span>the shortest possible sampling
interval. On the other hand, if <span style='color:#00a000;'>$a_i &lt; </span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>$</span>, the interval increases by a factor <span style='color:#00a000;'>$</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}(</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/a_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max})$</span>. 
This approach ensures that we sample all configurations where the gradient of the PES is large, while skipping many
of the low-gradient regions of configuration space. 

We have chosen to use the forces <span style='color:#00a000;'>$|</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{F}_i| </span><span style='color:#606000;'>\propto</span><span style='color:#00a000;'> a_i$</span> instead of the accelerations, simply because they are
readily available in LAMMPS. When sampling systems with periodic boundary conditions, we have to consider 
the way forces are updated during a simulation. In LAMMPS, any forces attributed to ghost atoms are added back 
to the originals after <span style='color:#800000;'>\textit</span>{all} forces are computed. This is done in
<span style='color:#800000;'>\texttt</span>{verlet.cpp},
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{C++}
  <b>if</b> (pair_compute_flag) force-&gt;pair-&gt;compute(eflag,vflag);
  <span style='color:#898887;'>// ...</span>
  <b>if</b> (force-&gt;newton) comm-&gt;reverse_comm();
 <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
The first line computes the forces on all atoms with the chosen <span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>style} potential, while
the second line communicates the forces on ghost atoms back to the original atoms according to Newton's third law. 
This means that to employ the algorithm
<span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>variableIntervalSampling</span>}, we need to sample configurations <span style='color:#800000;'>\textit</span>{after} this reverse communication is
executed, if not the forces will be incorrect. This can be achieved by extending LAMMPS with a new <span style='color:#800000;'>\texttt</span>{compute}<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{See <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:lammpsInputScript</span>} for an explanation of various LAMMPS input script commands, 
and <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:extendingLammps</span>} for a general description on how to add functionality to LAMMPS.},
as all <span style='color:#800000;'>\texttt</span>{computes} and <span style='color:#800000;'>\texttt</span>{fixes} are invoked after the
force calculations. 

We have made an additional modification to the algorithm <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>variableIntervalSampling</span>}.  
Pukrittayakamee et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Pukrittayakamee09</span>} sample clusters of atoms for subsequent DFT calculations
of the energies of these clusters, and computes <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span> based on the maximum acceleration of an atom in a cluster. 
Our strategy is to select a few atoms, usually 1-10, and compute <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span> seperately for each of these atoms. When 
the sampling criterion is met, the coordinates of all neighbours relative to the central atom is written to file, 
as well as the potential energy the central atom experiences in the presence of its neighbours. These coordinates
and energies make up the input data and target values, respectively, for a NN to interpolate. 
Our modified sampling algorithm is thus as follows (written in a less convoluted way than above),
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'> = </span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/F_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>], &amp;F_i </span><span style='color:#606000;'>\leq</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  1, &amp;F_i &gt; </span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}, &amp;</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{floor}</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>/F_i</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>] &gt; </span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>cases</span>}
<span style='color:#606000;'>\label</span><span style='color:#00a000;'>{samplingAlgorithmModified}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span> is measured in units of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max}$</span> is the maximum value of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span>.
<span style='color:#00a000;'>$F_i$</span> is the total force on atom <span style='color:#00a000;'>$i$</span> including ghost contributions. 
We have implemented this sampling algorithm as follows,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{c++}
<span style='color:#898887;'>// ...</span>
<span style='color:#898887;'>// loop through atoms that is to be sampled</span>
<b>for</b> (<b>auto</b> ii : chosenAtoms) {
    i = ilist[ii];
    itype = type[i]<span style='color:#b08000;'>-1</span>;

    <span style='color:#0057ae;'>double</span> F = sqrt(f[i][<span style='color:#b08000;'>0</span>]*f[i][<span style='color:#b08000;'>0</span>] + f[i][<span style='color:#b08000;'>1</span>]*f[i][<span style='color:#b08000;'>1</span>] + f[i][<span style='color:#b08000;'>2</span>]*f[i][<span style='color:#b08000;'>2</span>]);

    <span style='color:#898887;'>// update tau every maxDelay step</span>
    <b>if</b> ( !(myStep % maxDelay) ) {

      <span style='color:#898887;'>// no delay if force is larger than alpha</span>
      <b>if</b> (F &gt; alpha[itype]) tau[atomCount] = <span style='color:#b08000;'>1</span>;

      <span style='color:#898887;'>// calculate delay if force less than alpha</span>
      <b>else</b> {
        <span style='color:#0057ae;'>int</span> factor = floor( alpha[itype] / F );
        <b>if</b> (factor &gt; maxDelay) tau[atomCount] = maxDelay;
        <b>else</b>                   tau[atomCount] = factor;
      }
    }

    <span style='color:#898887;'>// decide whether to sample or not based on current value of tau for atom i</span>
    <b>if</b> ( (myStep % tau[atomCount] &gt; <span style='color:#b08000;'>0</span>) &amp;&amp; useAlgo) <b>continue</b>;

    <span style='color:#0057ae;'>double</span> xi = x[i][<span style='color:#b08000;'>0</span>];
    <span style='color:#0057ae;'>double</span> yi = x[i][<span style='color:#b08000;'>1</span>];
    <span style='color:#0057ae;'>double</span> zi = x[i][<span style='color:#b08000;'>2</span>];
    
    <span style='color:#898887;'>// loop through neighbours of atom i </span>
    <span style='color:#898887;'>// write relative coordinates and atomic energy to file</span>
    jlist = firstneigh[i];
    jnum = numneigh[i];
    <b>for</b> (jj = <span style='color:#b08000;'>0</span>; jj &lt; jnum; jj++) {
      j = jlist[jj];
      <span style='color:#898887;'>// ...</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
The vector <span style='color:#800000;'>\texttt</span>{chosenAtoms} contains the indicies of the atoms that are to be sampled. These indicies are decided
when the parameters of the employed potential is initialized with the input script command <span style='color:#800000;'>\texttt</span>{pair<span style='color:#800000;'>\_</span>coeff}. 
The variable <span style='color:#800000;'>\texttt</span>{maxDelay} decides how often <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span> is updated, and serves as a maximum value of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>$</span>. 
Its value does not change in the course of the simulation. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minipage</span>}{0.48<span style='color:#800000;'>\linewidth</span>}
  <span style='color:#800000;'>\subcaption</span>{}
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\textwidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/forceDistSamplingAlgo.pdf</span>}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:forceDistSamplingAlgo:a</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minipage</span>}
<span style='color:#800000;'>\quad</span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minipage</span>}{0.48<span style='color:#800000;'>\linewidth</span>}
<span style='color:#800000;'>\subcaption</span>{}
<b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\textwidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/forceDistSamplingAlgoNormed.pdf</span>}
 <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:forceDistSapmlingAlgo:b</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minipage</span>}
<span style='color:#800000;'>\vspace</span>{-1ex}
  <span style='color:#800000;'>\caption</span>{Distribution of forces for one Si atom in a quartz <span style='color:#00a000;'>$</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{SiO}_2$</span> crystal with 
	   and withouth the use of the sampling algorithm <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>}. 
	   The crystal consists of 576 atoms, with an initial temperature <span style='color:#00a000;'>$T= </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{1000}{</span><span style='color:#606000;'>\kelvin</span><span style='color:#00a000;'>}$</span>, 
	   run for <span style='color:#00a000;'>$N = 10000$</span> time steps. The atoms are sampled from the microcanonical ensemble with
	   periodic boundary conditions.
	   Sampling algorithm parameters: <span style='color:#00a000;'>$</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>=3.0$</span> and 
	   <span style='color:#00a000;'>$</span><span style='color:#606000;'>\tau</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{max} = 10$</span>.}
<span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:forceDistSamplingAlgo</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:forceDistSamplingAlgo</span>} displays the distribution of forces for one Si atom in a MD simulation
of quartz <span style='color:#00a000;'>$</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{SiO}_2$</span>, with and withouth the use of the sampling algorithm <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>}. 
We see from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:forceDistSamplingAlgo:a</span>} that the 
sampling procedure cuts the number of low-gradient configurations significantly, 
while keeping all the environments corresponding to large forces. The resulting distribution is however not uniform, simply
because the ratio of force counts in the range <span style='color:#00a000;'>$F_i = </span><span style='color:#606000;'>\SIrange</span><span style='color:#00a000;'>{0}{2}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span> to the range <span style='color:#00a000;'>$F_i = </span><span style='color:#606000;'>\SIrange</span><span style='color:#00a000;'>{2}{4}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>
is too high originally.

One way to smooth the distribution further is to manually delete configurations after the simulation 
is finished, but then we risk removing relevant structures. The gradient of the PES is not the only way 
to measure the quality of the sampled set, the time aspect is also important. 
By selecting samples on-the-fly, we are guaranteed that the they are distributed somewhat uniformly in the time domain. 
Deleting structures blindly subsequent to the simulation is therefore not a good idea.

Another option is to sample different types of systems where the forces are more evenly distributed. 
Examples are non-periodic systems or systems containing only a few atoms. Also, it can be a good idea 
to do short simulations where the atoms start out in non-equilibrium positions to obtain configurations
that are not visited by systems with a regular lattice initial state. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Iterative sampling</span></b>}
The initial data set is usually not sufficient to construct a stable neural network PES. 
It is difficult to identify all the relevant structures that the NN have to learn to perform well, and 
since there is always a small fitting error, the 
NNP will generate different trajectories than the PES that has been used to train it. The data set can be extended
by sampling new relevant structures from simulations using the preliminary NNP trained on the initial data set. 
In this way, new configurations
will ''naturally present themselves'', and the NNP will iteratively improve in a self-consistent way.

The obvious way to identify new structures not already in the initial data set is to evalate both the NNP and 
the PES we are trying to fit for each structure in an NNP MD simulation and compare the energies (and eventually 
the forces). If the energy deviation is above a certain threshold, the structure is added to the data set.
The simulations should be carried out under the conditions of the intended application of the NNP to ensure
that the relevant parts of configuration space are sampled.
After a certain amount of new data is added, 
the NN is trained on this extended data set and thus improved. Then, new simulations are carried out with the improved
NNP and so on, according to the iterative process described in the beginning of this chapter. 

This approach is easily feasible when the target PES is an empirical potential, but too computationally expensive 
in the case where electronic structure methods are employed. Raff et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Raff05</span>} have developed a sampling
algorithm called <span style='color:#800000;'>\textit</span>{modified novelty sampling} that does not require any quantum mechanical calculations of
the energies of new structures. They describe the database as a set of vectors <span style='color:#00a000;'>$</span><span style='color:#606000;'>\{\vec</span><span style='color:#00a000;'>{q}</span><span style='color:#606000;'>\}</span><span style='color:#00a000;'>$</span>, where vector
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_i$</span> contains the internal coordinates of the <span style='color:#00a000;'>$i_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{th}$</span> cluster of atoms. New configurations
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n$</span> are generated using the NNP trained on this data base and the 2-norm difference <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{d}$</span> between vectors
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_i$</span> are computed,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{d} = | </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_i - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n | = </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_i - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n)</span><span style='color:#606000;'>\cdot</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_i - </span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n)</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]^{1/2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
for all clusters <span style='color:#00a000;'>$i$</span>. The difference vector is characterized by its minimum value <span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{min} = </span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{min}(</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{d})$</span> and 
the mean seperation distance <span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{mean} = </span><span style='color:#606000;'>\langle\vec</span><span style='color:#00a000;'>{d}</span><span style='color:#606000;'>\rangle</span><span style='color:#00a000;'>$</span>. 
The basic idea is to add configuration point <span style='color:#00a000;'>$</span><span style='color:#606000;'>\vec</span><span style='color:#00a000;'>{q}_n$</span> to the database with high
probability if <span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{min}$</span> or <span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{mean}$</span> is large, but low probability if both 
<span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{min}$</span> and <span style='color:#00a000;'>$d_</span><span style='color:#606000;'>\textrm</span><span style='color:#00a000;'>{mean}$</span> are small. The energy output of the NNP are also included in the scheme, along with
a more involved selection algorithm. As this thesis only deals with empirical target potentials, the details will
not be further discussed here. 

In this work we use the Behler symmetry functions to transform atomic cartesian coordinates to vectors of symmetry values.
These functions act as descriptors of chemical environments, and should therefore be excellent candidates
to identify relevant structures during MD simulations without computing additional target energies. 
Behler has suggested two methods <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>} where these symmetry functions are employed to improve the NNP. 
The first and simplest method identifies structures that fall outside of the range of input values spanned by the data set. 
This is achieved by comparing the symmetry function vector of each atom during the MD simulation with 
the minimum and maximum values of each symmetry function in the database for the respective element. 
When such configurations are encountered, a warning can be issued and the structure sampled.  

The other scheme developed by Behler is called the <span style='color:#800000;'>\textit</span>{multiple-NN method} and is used to identify 
structures that are within the range of existing symmetry values, but located
in parts of configuration space that are poorly represented in the reference set.
The idea is to use the high flexibility and reasonably fast training and evaluation of NNs to validate 
the potential. The procedure is as follows,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> Fit several NNs with different architectures to the same data set. All the NNs should be trained
 to the same accuracy. 
 <span style='color:#800000;'>\item</span> Generate a large number of trajectories and corresponding energies using one of these NNPs in MD simulations.
 <span style='color:#800000;'>\item</span> Recalculate the energies of the obtained trajectories using the other NNPs.
 <span style='color:#800000;'>\item</span> Structures for which the energy disagreement between the different NNPs is significant should be added to 
 the data set.
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
This method takes advantage of the poor extrapolation abilities of NNs: The energy predictions of the different NNPs will
be more or less random on structures that are far away from the training points. Thus, missing structures can 
be identified by comparing the energy predictions of the different NNPs on the generated trajectories. The procedure 
also provides a criterion for determining the convergence of the PES without requiring the repeated computation 
of dynamic properties of the system under study. When, after several iterations of the multiple-NN method,
no problematic structures are found, the NNP should be reliable and ready for use. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Summary</span></b>}
In this section we have presented several procedures to 
assemble the data set for the NN, which depends on how the target energies are computed. 
In this work, we know the PESs we are trying to fit, which are common empirical MD potentials. 
Therefore, the NNP can be validated simply by comparing its predictions to the target PES.  
On the other hand, if an <span style='color:#800000;'>\textit</span>{ab inito} NNP is to be constructed, the underlying target PES is not known, 
and computing energies and forces of eventual new structures are expensive. Two methods circumventing
this problem has been described.

Still, there are several factors to consider when constructing the reference set, irrelevant of how the target PES looks like.  
We have implemented and employed a sampling algorithm <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>} in this thesis to obtain a more uniform
distribution of forces in the data set. The thermodynamical properties of the MD simulations that we sample configurations 
from have also been carefully considered, described in more detail in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}.  



<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Constructing the symmetry function sets</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:constructingSymmetry</span>}
The choice of symmetry functions is important to achieve a good fit to the reference data. 
They need to be able to sufficiently distinct inequivalent structures to avoid fitting 
contradictory data. If this distinction is not possible, different reference structures with 
different energies and forces give rise to the same symmetry function values, which results in poor fits. 

Although a NN potential is general and unbiased because it has no predefined functional form, the 
construction of the symmetry function sets is to some extent empirical. The functional form of the NN
is adjusted ''automatically'' when trained, while the symmetry function set has to be constructed manually.
This observation does however not change the fact that the high-dimensional neural network approach to
constructing potentials used in this work is general, and can in principle be applied to any system. The data set
and symmetry function sets have to be customized to each system, but the overarching procedure is the same. 

In this section we discuss various teqhniques for building a suitable symmetry function set. 
A good strategy is to start out with an initial set that satisfies certain criteria, and then customize this set to 
fit the reference data at hand. We follow and extend upon the discussions in <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11symmetry</span>} and <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler15</span>}. 
Note that each symmetry function 
can be treated as a variable that is assigned a unique value for each training example in the data set. 
Thus we occasionally refer to a single symmetry function as an <span style='color:#800000;'>\textit</span>{input variable}. The symmetry function definitions 
are found in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>}. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Initial set</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:initialSymmSet</span>}
There are some general rules that guide the construction of the initial symmetry function set.
First, <span style='color:#800000;'>\textit</span>{the symmetry function set should cover the configuration space in an unbiased way}.
This can be realized by simply choosing an equidistant set of initial radial symmetry functions (the angular
functions are discussed below). Further, the distribution of inter-atomic distances in the data set may be investigated to acquire a 
lower and upper bound on the range of radial parameter values. For <span style='color:#00a000;'>$G_i^2$</span>, a rule of thumb is that, for each element, 
the width (standard deviation) of the Gaussian
with the largest <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> value should correspond to the shortest inter-atomic distance present in the data set 
and vice versa. 

Also, a set of symmetry functions with equidistant <span style='color:#00a000;'>$r_s$</span> parameters should be present in the initial set to improve 
the sensitivity at various radii. 
These should all have the same, large <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> value to prevent too much overlap. 
An example of an initial radial symmetry function set is displayed
in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:radialParams</span>}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/radialParams.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of an initial radial symmetry function set. The parameters <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$r_s$</span> are varied to 
  probe the radial arrangement of neighbouring atoms.
  All symmetry functions have the same cutoff <span style='color:#00a000;'>$r_c = 6.0$</span>. 
  <span style='color:#00a000;'>$r_{ij}$</span>, <span style='color:#00a000;'>$r_s$</span> and <span style='color:#00a000;'>$r_c$</span> are in units of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>, while <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> have units <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-2}$</span>.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialParams</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Angular symmetry functions</span></b>}
A set of initial angular symmetry functions can be made in a similar way. 
The angular distribution of neighbours is probed by varying <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span> at differing radii, i.e.<span style='color:#800000;'>\ </span>for 
several values of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> and/or <span style='color:#00a000;'>$r_c$</span>. 
We have found from empirical investigations that fixing <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span>, while varying <span style='color:#00a000;'>$r_c$</span> yield the best results. 
In contrast to the radial parameters, the set of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> parameters should not be equidistant. Instead, the values of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span>
should increase exponentially to ensure that the symmetry functions are sufficiently dissimilar, i.e.<span style='color:#800000;'>\ </span>to <span style='color:#800000;'>\textit</span>{avoid
strong correlations} (discussed further below).  An example is shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:angularParams</span>}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/angularParams1.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example of an initial angular symmetry function set
  with <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{0.01}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-2}$</span> and 
  <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> = 1$</span>. A corresponding set of functions with <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> = -1$</span> should also be added to make the set more complete. 
  The <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> parameter is increased in a non-linear way to avoid excessively overlapping functions. }
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:angularParams</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Number of symmetry functions</span></b>}
How many symmetry functions should the initial set consist of?
There is a trade-off between the ability of the symmetry function set to cover the configuration space and 
the efficiency of the calculation of the NN energy output. The key is to select enough functions to capture 
all regions of configuration space without having any redundancies. 

Further, the number of functions must be large enough for the NNP to be able to distinguish distinct atomic structures that
should yield different total energies. This distinction is more difficult for configurations of a large number of atoms, because
e.g.<span style='color:#800000;'>\ </span>removing one atom from an atomic environment containing 100 atoms only results in a small relative 
perturbation of the symmetry function values. Thus, we should obtain information on how many input atoms 
the eventual NNP is expected to experience in simulations. 

In our case this is an easy task: We simply run MD simulations 
of the system under investigation with the empirical potential to be fitted, and measure coordination numbers. When the 
target potential is unknown, the information can be extracted from empirical knowledge of the system. The number of 
neighbouring atoms will obviously depend on the force cutoffs being used. 

One could immidiately conclude that we
should have at least as many radial and angular functions as there are atomic pairs and triplets respectively, but this is overkill
considering that each one is a function of <span style='color:#800000;'>\textit</span>{all} the atoms inside the cutoff sphere.
From experimenting, we have found that a rule of thumb is to use at least as many radial symmetry functions
as the mean number of expected neighbours. The number of angular symmetry functions should in general be larger because
there are more atomic triplets than pairs for a given ensemble of atoms.  


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Adjusting the set</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:adjustSymmSet</span>}
When the initial set is assembled, we carry out various analyses to investigate whether our chosen symmetry functions 
adequately probe the configuration space defined by the data set. There are two requirements
that serve as the guiding principles for the analyses.

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>For each symmetry function, the range of function values should be as large as possible</span></b>}
Each symmetry function
assigns <span style='color:#800000;'>\textit</span>{one} numerical value to each training example. If the range of a symmetry function is too small, 
the difference between the function values for distinct structures will also be small. Consequentially, 
the NN will try to assign substantial energy changes to tiny changes in the symmetry function, which 
can lead to numerical instability. 

This can be avoided by analyzing the distribution of values of each symmetry function on the data set. If the difference 
between the maximum and the minimum function value is below a certain threshold
the symmetry function should be removed or its parameters adjusted. Specifically, 
symmetry functions which have a value of zero for all configurations should be avoided, as these will not contribute
to the description of the chemical environments. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>The set of values of two different symmetry functions on a given data set should not be strongly correlated</span></b>}
If there is a large correlation, the two symmetry functions are almost linearly dependent, 
and we have a redundancy of information. Consequentially, one of them should be removed, or the parameters adjusted
to decrease the correlation. Removing functions in this way contributes to keeping the symmetry function set as 
small as possible.

To demonstrate how the correlations can be calculated, we remember that in 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:training</span>} the data set was represented as a matrix X, where
each row is a training example, in this case the values of all the symmetry functions for a single atomic configuration.
We can calculate the correlations between each pair of symmetry functions with the <span style='color:#800000;'>\texttt</span>{NumPy} method <span style='color:#800000;'>\texttt</span>{corrcoef}, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{python}
  R <b>=</b> np.corrcoef(np.transpose(X))
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
The data set matrix <span style='color:#00a000;'>$X$</span> is transposed because <span style='color:#800000;'>\texttt</span>{corrcoef} expects each <span style='color:#800000;'>\textit</span>{row} to represent all the samples of one
variable (function), while each <span style='color:#800000;'>\textit</span>{column} is a single observation of all the variables.
A matrix <span style='color:#00a000;'>$R$</span> is returned where the <span style='color:#00a000;'>$(i,j)$</span>-th element is the Pearson correlation coefficient between 
symmetry function <span style='color:#00a000;'>$i$</span> and <span style='color:#00a000;'>$j$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> R_{ij} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{C_{ij}}{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_i</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_j}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$C$</span> is the covariance matrix and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_i$</span> is the standard deviation of the values of symmetry function <span style='color:#00a000;'>$i$</span> for 
the whole data set. Pair of functions which are strongly correlated can thus be identified and altered.




<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Setting hyperparameters</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:settingHyperParams</span>}
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:training</span>} and <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:optimization</span>} we presented the theory on how a MLP is trained 
by optimizing the weights and biases using an optimization algorithm of choice. 
At the end of <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>}, we briefly mentioned that a number of so-called hyperparameters must be set
prior to the actual weight optimization, such as the size of the network. 
Choosing hyperparameter values can be treated as a
kind of <span style='color:#800000;'>\textit</span>{model selection}. One set of parameters defines one model, and the goal is to pick the most appropriate 
among a family of such models. The choice can have significant impact on the quality of the fit. 

We distinguish between two kinds of hyperparameters: those that specify the structure of the 
network itself, and those that determine how the network is trained. 
The first type contains parameters that are an integral part of the NN and must be included in the evaluation of the eventual NNP, 
while the second are parameters that are only used during training. The former includes the symmetry function parameters
and any other parameters related to transformations of the input data, the number of layers and nodes, and the choice of 
activation function(s). The second includes 
the learning rate, the mini-batch size, the number of training iterations, the choice of loss function and any additional 
parameters associated with the optimization algorithm and weight initialization.

<span style='color:#800000;'>\noindent</span> In this section we will discuss common techniques for setting hyperparameters. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Preconditioning the input data</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:transformInputData</span>}
As stated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:adjustSymmSet</span>}, the individual symmetry functions may have very different ranges of values. 
For a given configuration, each node in the first hidden layer receives as input a weighted sum of all the symmetry function values.
Symmetry functions with large absolute values will have a signicantly stronger impact on the output of these nodes than 
functions with small absolute values. Consequentially, the areas of configuration space covered by the latter functions
may take a longer time for the NN to learn. The importance of the different symmetry functions can be balanced
by normalizing the values of each symmetry function <span style='color:#00a000;'>$G_s$</span>  <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler15</span>}, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_{i,s}^</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{norm} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{2(G_{i,s} - G^{</span><span style='color:#606000;'>\min</span><span style='color:#00a000;'>}_s)} {G^{</span><span style='color:#606000;'>\max</span><span style='color:#00a000;'>}_s - G^{</span><span style='color:#606000;'>\min</span><span style='color:#00a000;'>}_s} - 1</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{scalingInputData}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$G_{i,s}$</span> is the value of symmetry function number <span style='color:#00a000;'>$s$</span> in the <span style='color:#00a000;'>$i$</span>-th training example<span style='color:#898887;'>% </span>
<span style='color:#800000;'>\footnote</span>{Corresponding to element <span style='color:#00a000;'>$X_{ij}$</span> of the input data matrix.}, and
<span style='color:#00a000;'>$G_s^{</span><span style='color:#606000;'>\min</span><span style='color:#00a000;'>}$</span> and <span style='color:#00a000;'>$G_s^{</span><span style='color:#606000;'>\max</span><span style='color:#00a000;'>}$</span> are the smallest and largest values of function <span style='color:#00a000;'>$G_s$</span> in the data set respectively.

Further, convergence is usually faster if the average of each input variable to a NN over the 
data set is close to zero <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun12</span>}.
This is because the update of a weight to a node in the first hidden layer is proportional 
to <span style='color:#00a000;'>$</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'> y_0 = </span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'> G_s$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>weightUpdate</span>} where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>$</span> is the error of the node,
and <span style='color:#00a000;'>$y_0 = G_s$</span> is the value of an input node. 
Since all the input values <span style='color:#00a000;'>$G_s$</span> are always positive, the weight updates will all have the same sign, namely <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{sign}(</span><span style='color:#606000;'>\delta</span><span style='color:#00a000;'>)$</span>. 
As a result, these weights can only increase or decrease <span style='color:#800000;'>\textit</span>{together} for a given input vector, leading to slow convergence.
The mean of each symmetry function can be shifted to zero by subtracting the mean <span style='color:#00a000;'>$</span><span style='color:#606000;'>\overline</span><span style='color:#00a000;'>{G}_s$</span> over the data set from 
all function values,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_{i,s}^</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{shift} = G_{i,s} - </span><span style='color:#606000;'>\overline</span><span style='color:#00a000;'>{G}_s =  G_{i,s} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{N}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^N G_{j,s}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{shiftInputData}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$N$</span> is the number of training examples. Note that scaling <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>scalingInputData</span>} and shifting <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftInputData</span>}
can also be combined. 

For reasons outlined in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:hyperParamsActFunctions</span>},
another operation that can speed up training is variance scaling <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun12</span>}.
This is combined with shifting of means, such that each symmetry function e.g.<span style='color:#800000;'>\ </span>has zero-mean and unit-variance,
called <span style='color:#800000;'>\textit</span>{standardization},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_{i,s}^</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{stand} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{G_{i,s} - </span><span style='color:#606000;'>\overline</span><span style='color:#00a000;'>{G}_s}{</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_s} </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{scaleVariance}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_s$</span> is the standard deviation of symmetry function <span style='color:#00a000;'>$G_s$</span>. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Convergence speed vs evaluation speed</span></b>}
We have seen that a number of operations can be performed on the input data to speed up the fitting process.  
Note that the these can be applied to the target values as well <span style='color:#898887;'>%  </span>
<span style='color:#800000;'>\footnote</span>{The observations on scaling and shifting of input variables are valid for most 
types of input and output data. Scaling of target values are more important 
for classification, where the output is binary (e.g.<span style='color:#800000;'>\ \{</span>-1,1<span style='color:#800000;'>\}</span>), than for regression.}. However, for the construction of NNPs, 
the convergence speed is not a main issue. In contrast to e.g.<span style='color:#800000;'>\ </span>object recognition with deep convolutional networks, where
often thousands of NN parameters are needed <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun99</span>}, NNPs usually require only up to a few hundred weights. Even though 
the data set to be fitted can be quite extensive, the duration of the training is rarely more than 30 minutes to 1 hour before 
an acceptable error is achieved.

Further, there is a trade-off between the convergence speed and the evaluation speed of the resulting NNP. 
As for the symmetry transformations, the above operations become an integral part of the NNP, and must be performed 
when the potential is to be applied in simulations. Considering the importance of computational performance in MD simulations,
an increase in training speed at the cost of evaluation speed is not to be preferred, as long as the <span style='color:#800000;'>\textit</span>{quality} of the fit 
is not influenced by these operations. This will be investigated in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}. 



<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Activation functions and weight initialization</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:hyperParamsActFunctions</span>}
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:activationFunctions</span>} we stated that the sigmoid <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>sigmoidActivationFunction</span>} and 
the hyperbolic tangent <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>tanhActivationFunction</span>} are the most common activation functions for regression
with neural networks. They are both <span style='color:#800000;'>\textit</span>{saturating} functions because they map real numbers into the restricted intervals 
[0,1] and [-1,1] respectively. This property keeps the outputs of neurons from diverging, but it also contributes to 
the problem of <span style='color:#800000;'>\textit</span>{vanishing gradients}. We know from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:backprop</span>} that
weight updates are proportional to the derivatives of neuron outputs <span style='color:#00a000;'>$y$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>backprop</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> y^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'> = f^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>(u)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$f$</span> is the activation function and <span style='color:#00a000;'>$u$</span> is the input. 
We see from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:SigmoidActivationFunctions</span>} that the sigmoid and hyperbolic
tangent have very small gradients for all inputs except those in the small non-linear region around zero, corresponding 
to about [-4,4] and [-2,2] respectively. 
Large positive or negative inputs therefore result in vanishing gradients and tiny updates, slowing down learning. 

Note that saturation occurs naturally if a network is absolutely correct on all training examples and 
hence does not need to change its weights. However, premature saturation is a common problem in neural network training, and may slow
down or even prevent learning. It can be avoided by restricting neuron inputs <span style='color:#00a000;'>$u$</span> to the non-linear region of their 
activation functions, with the help of two different procedures. 

First, scaling/standardization (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:transformInputData</span>}) of the input data helps to achieve this for
the neurons in the first hidden layer. The distribution of inputs to the next layer depend on the choice of activation function, and 
here the sigmoid and the tangent differ: The former have mean 1/2, while the latter is symmetric around zero. 
Obviously, zero-mean is preferred, as any positive or negative bias inevitably have a larger risk of pushing the activations 
towards saturation. Thus, the hyperbolic tangent is generally preferred over the sigmoid, which have also been demonstrated 
empirically <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Karlik11,Glorot10</span>}. 

Secondly, since the weights and biases also contribute to the sum <span style='color:#00a000;'>$u$</span>, they should be initialized as small numbers centered around zero. 
A common scheme is to 
set all biases to zero, and assign weight values according to some probability distribution with zero-mean and 
a suitable (small) variance. Assuming standardized input data, 
a frequently used heuristic is to initialize the weights <span style='color:#00a000;'>$W_{ij}$</span> at each layer by 
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bengio12,LeCun12</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> W_l </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> U</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{n_{l-1}}}, </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{n_{l-1}}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$U[-a, a]$</span> is the uniform distribution in the interval <span style='color:#00a000;'>$[-a,a]$</span>, <span style='color:#00a000;'>$n_l$</span> is the size of layer <span style='color:#00a000;'>$l$</span>, and 
<span style='color:#00a000;'>$W_l$</span> is the weight matrix connecting layer <span style='color:#00a000;'>$l-1$</span> and <span style='color:#00a000;'>$l$</span>. 
A disadvantage of this method is that a layer-dependent weight variance might result in saturated upper layers 
if the layer size changes through the network.  
Glorot and Bengio <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Glorot10</span>} have suggested a modification of this scheme that gives rise to a variance that 
is equal through the whole network. The idea is to 
conserve the input signal when propagated through the network by preventing it from shrinking or growing too much,  
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> W_l </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> U</span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{6}}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{n_{l-1}+n_{l}}}, </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{6}}{</span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{n_{l-1}+n_{l}}}  </span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This is only valid for an activation function that is symmetric and has a unit derivative at 0 (<span style='color:#00a000;'>$f^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>(0) = 1$</span>), i.e.<span style='color:#800000;'>\ </span>only 
for the hyperbolic tangent. Further, all input variables are assumed to have the same variance.




<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Network architecture and overfitting</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:overfitting</span>}
There is no standard and accepted method for finding the optimal size of a NN for a specific data set. 
In this work, the input layer is a set of symmetry functions that acts as descriptors of atomic configurations.
The size of this set thus decides the number of neurons in the input layer, which is discussed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:initialSymmSet</span>}. 

The output layer only consists of <span style='color:#800000;'>\textit</span>{one} neuron that represents the total energy of an atomic 
configuration. Additionaly, we can have an arbitrary number of hidden layers and nodes. 
Many nodes means many parameters that can be tuned to the data set, resulting in a better fit.
However, if the number of neurons is too large, the network will <span style='color:#800000;'>\textit</span>{overfit} the reference data. 
In this case, the network fits the data set very well, but interpolation between the training points is poor. Thus, 
the NNP becomes less predictive, and will not perform well in simulations.

<span style='color:#800000;'>\textit</span>{Regularization} is a number of processes from statistical learning theory that deals with ill-posed optimization
problems and overfitting. A well-known method to prevent the latter is <span style='color:#800000;'>\textit</span>{early stopping}. 
Here, the reference data is randomly split into 
a <span style='color:#800000;'>\textit</span>{training set}, which is employed for the optimization of the weights, and a <span style='color:#800000;'>\textit</span>{test set}, whose error is monitored
during the fit but which is not used to adjust the weights. A typical test set size is 10-20 <span style='color:#800000;'>\%</span> of the reference data.
If the errors of these two sets are similar, we know that the NN
generalizes to data that has not been used in its training. On the other hand, if the test set error is significantly larger
than the training set error, overfitting is occurring, and training should be stopped. The most common error measure
is the root-mean-square-error (RMSE), 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{RMSE}(</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}) = </span><span style='color:#606000;'>\sqrt</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{N}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^N (Y_i - y_i)^2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$y_i$</span> is the NN prediction and <span style='color:#00a000;'>$Y_i$</span> is the reference value, as before. 
This expression is almost equivalent to the cost function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>} used in the optimization of the NN.

<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:overfitting</span>} displays a typical case of overfitting. Here we have sampled a training set 
of 5578 configurations and energies from a Si Stillinger-Weber simulation. Then, we have trained two NNs 
with the same set of hyperparameters, first on the full training set a), then on only the first 300 training examples b).
The test set is identical in both cases. We observe that the training set error and the test set 
error are very similar in a), while in b) there are a significant deviation between the two. The latter is clearly a case 
of overfitting caused by an incomplete training set.  
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/overfitting.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Demonstration of overfitting. Both figures show the RMSE of a NN evaluated on both the training set and 
	   the test set. The training set consists of 5578 training examples from a Si Stillinger-Weber simulation.
	   In a), the NN has been trained on 
	   the whole training set, while in b) only the first 300 training examples were used. The test set is 
	   identical in both cases. We clearly see that overfitting occurs in b).}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:overfitting</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

The early stopping method automatically determines another hyperparameter, namely the number of epochs to train. 
However, it does not give us the optimal NN architecture. Overfitting can also be caused by an incomplete data set, 
and reducing the number of neurons is not necessarily the best solution. Often, more data should be added instead, 
and the current NN size may fit the extended data set better. 

Early stopping is in other words not necessarily a reliable method to decide if a NN is of a suitable size or not, 
but primary a tool to prevent overfitting. 
The only way to find the optimal architecture is by training several networks of different sizes and choose the one that yields
the lowest error while simultaneously not overfitting. The problem with this strategy is that the possibilities are very many. 
We can however make a few observations to narrow down the search. 
According to Larochelle et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Larochelle09</span>}, having the same number of nodes in each hidden layer gives the best results. 
Further, in the literature we have studied concerning NNPs, e.g.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler07,Raff05,Witkoskie05</span>}, 
only one or two hidden layers are used, and we know from the universal approximation theorem (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:whyMLP</span>})
that one hidden layer is sufficient to represent any function. For typical AI tasks, deeper architectures are required 
<span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bengio07</span>}, but in this thesis we will limit ourselves to maximum three hidden layers. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Other regularization teqhniques</span></b>}
There exist other regularization techniques in addition to early stopping that reduce overfitting. 
<span style='color:#800000;'>\textit</span>{Weight decay} or <span style='color:#800000;'>\textit</span>{L2 regularization} adds an extra term to the cost function, called 
the <span style='color:#800000;'>\textit</span>{regularization term}, which is the sum of squares of all weights in the network <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Krogh1992</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>_0 + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>}{2N}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_w w^2</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'>_0$</span> is the original, unregularized cost function, <span style='color:#00a000;'>$N$</span> is the size of the training set and 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span> is the <span style='color:#800000;'>\textit</span>{regularization parameter}. The idea is to penalize large weights, and the modified 
cost function can be viewed as a compromise between finding small weights and minimizing the original cost function.
The parameter <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span> determines the relative importance of these two elements. 
Keeping the weights small makes the output of the NN more resistant to outliers and noise in the input data, thus
putting more emphasis on the frequent input patterns. 
The hope is that this will force the NN to see ''the bigger picture'', 
and hence generalize better. 

Another more recent technique to reduce overfitting is <span style='color:#800000;'>\textit</span>{dropout} <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Srivastava14</span>}. Here, the key idea 
is to randomly remove neurons from the NN during training. This prevents nodes from co-adapting too much and 
also decreases the size of the network. We will not go into further detail on this subject here. 

We have decided to employ early stopping only to prevent overfitting, as more training data is easily obtained from MD 
simulations with empirical potentials, making overfitting not a pressing issue. 


<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Optimizer parameters</span></b>}
There are several optimization hyperparameters that need to be decided, namely the learning rate, the mini-batch size
(if we employ mini-batch gradient descent) and parameters inherent to the update rule of choice. 
We restrict ourselves to optimization algorithms that use adaptive learning rates, thus only an <span style='color:#800000;'>\textit</span>{initial}
learning rate needs to be set. According to Bengio <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bengio12</span>}, a default value of 0.01 is appropriate, but 
a set of different values should be tested (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:optimizingHyperparameters</span>}), 
as the optimal value may depend strongly on the data set and the NN architecture. 

In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:gradientDescentVariants</span>} we discussed different types of gradient descent learning, and 
declared that mini-batch gradient descent usually is the preferred scheme. The main reason for this is that data sampled from molecular 
dynamics simulations, even with a good sampling algorithm, often contain redundant information: many configurations 
are similar because the system tends to stay near equilibrium<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{This is of course not the case if the intended
application of the NNP is for non-equilibrium molecular dynamics.}. Thus, batch learning, where the weights are updated 
only after the complete training set have been fed to the network, is wasteful because it recomputes nearly the same outputs
several times before updating the weights <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>LeCun99</span>}. 

However, to employ mini-batch gradient descent, we have to determine another 
hyperparameter, namely the mini-batch size. According to Bengio <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Bengio12</span>}, 32 is a good default value. Further, it is argued 
that the mini-batch size should mainly impact the training time and not so much the eventual error of the fit.
Therefore, we will not discuss this parameter any further, and refer to <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:optimizingHyperparameters</span>} for a discussion
on how to optimize hyperparameters.

Some optimizers, like Adam <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>adamUpdateRule</span>}, contain other hyperparameters in addition to a learning rate.  
Each of these algorithms has a set of default values that are well justified, and we have decided not to alter these. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Cost function</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:hyperParamsCostFunction</span>}
We only use the mean-square error <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>quadraticCost</span>} as a cost function in this thesis. 
An obvious weakness of using this is that we only fit the reference function Y (the potential) itself, 
not its gradient <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{Y}^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>$</span> (the forces). If a small error is achieved for the potential fit, we would expect that 
the error is low also for the gradient, but this is not assured. The force error is of vital importance, considering 
that the forces determine the dynamics of a simulation. 

Pukrittayakamee et al.<span style='color:#800000;'>\ </span><span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Pukrittayakamee09</span>} have suggested an extension to the standard mean-square error, that 
enables simultaneous fitting of both a function and its gradient (the CFDA algorithm). 
This is achieved by penalizing both the error 
in the potential and in the forces, and has also been employed by Behler in a modified form <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler15</span>},
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Gamma</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{N}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{i=1}^N </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>[(E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ref}} - E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}})^2 + </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\beta</span><span style='color:#00a000;'>}{3N_i}</span><span style='color:#606000;'>\sum</span><span style='color:#00a000;'>_{j=1}^{3N_i} (F_{i,j}^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ref}} - F_{i,j}^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}})^2 </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>]</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{CFDA}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
As before, the first sum is over <span style='color:#00a000;'>$N$</span> training examples, where <span style='color:#00a000;'>$E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}}$</span>
is the energy prediction of the NN for training example <span style='color:#00a000;'>$i$</span> and <span style='color:#00a000;'>$E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ref}}$</span> is the <span style='color:#00a000;'>$i$</span>-th reference energy. 
For training example <span style='color:#00a000;'>$i$</span>, there are <span style='color:#00a000;'>$3N_i$</span> force components corresponding to a configuration of <span style='color:#00a000;'>$N_i$</span> atoms. We have that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> F_{i,j}^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ref}} &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ref}}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{i,j}} </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forcesCFDA1} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> F_{i,j}^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}} &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> E_i^{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{i,j}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{forcesCFDA2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
where <span style='color:#00a000;'>$r_{i,j}$</span> is the <span style='color:#00a000;'>$j$</span>-th position component of training example <span style='color:#00a000;'>$i$</span>. The NN still has only one output neuron
that represents the total energy of a training example. The forces (<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>forcesCFDA1</span>},~<span style='color:#f00000;'>\ref</span>{<span style='color:#0000d0;'>forcesCFDA2</span>}) are computed as
described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>}.
To minimize <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>CFDA</span>} we need its gradient, which means that second derivatives of the NN 
are required. This seems like a daunting task, but can be performed quite easily in TensorFlow. 
The command
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
 tf.gradients(y, x)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
computes the symbolic partial derivative of TF tensor <span style='color:#800000;'>\texttt</span>{y} with respect to TF tensor <span style='color:#800000;'>\texttt</span>{x}. In general, each tensor 
in a computational graph can be differentiated with respect to any other tensor in the same graph. 

We will not go into further detail on how the CFDA algorithm can be embedded in the training process. 
Computing second derivatives of the NN during training are computationally demanding, and since we have obtained 
good results without directly fitting the forces as well, we will stick to the ordinary mean-square cost function. 



<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Hyperparameter space exploration</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:optimizingHyperparameters</span>}
In this section we have discussed various tricks of the trade regarding hyperparameters for training neural networks. 
We realize that there are a substantial number of choices to be made, corresponding to a large parameter space, and 
that many (if not all) of these choices are important for a successful outcome. Even though the above discussions narrows down the search 
for suitable hyperparameters, it is still a difficult task. 

An alternative is to treat hyperparameter selection as an optimization problem, i.e.<span style='color:#800000;'>\ </span>to train the NN for many different 
parameter sets and choose the set that yields the best fit. To perform this task, it is common to split the data set 
into three parts: A training set, a test set, and a <span style='color:#800000;'>\textit</span>{validation set}. The first two sets have the same 
purpose as before (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:overfitting</span>}), while the validation set is used to test the model during hyperparameter 
optimization. This is done so that we do not overfit the hyperparameters to the test set, which is only used to assess 
the performance of the final NN model of choice.

In the following we will describe various hyperparameter optimization schemes. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Coordinate descent</span></b>}
In coordinate descent, we keep all hyperparameters fixed except for one. Then we adjust this single parameter and train the NN
for each value to minimize the validation error  .

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Grid search</span></b>}
Grid search is the most simple and brute-force way to optimize hyperparameters: We simply try every hyperparameter setting 
over a specified range of values and choose the set with the lowest validation error. This is obviously very computationally 
expensive, and is often infeasible in practice. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Random search</span></b>}
Instead of performing a grid search, we can sample the hyperparameter space randomly. This is usually a better option because 
a grid search may take a long time to reach sensible combinations of parameters, and because only a few parameters
tend to be vital for the result. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Summary</span></b>}
We have in this section discussed all hyperparameters that needs to be chosen to construct a NNP. 
It must be stressed that many of the above recommendations for hyperparameter choices may not be suitable 
for all problems, and that the only way to find unbiased, optimal values is by an extensive search 
in hyperparameter space. However, the quality of the fit may be insensitive to some parameters, which makes their 
determination less important. 
We refer to <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>} for a demonstration on how 
a combination of qualitative arguments and optimization can be performed to find suitable parameters 
for a specific case. 



<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Applying the neural network potential</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:applyingNNP</span>}
The quality of a NNP is measured by its test set error, but that does not necessarily mean it performs 
well in MD simulations. To properly validate the NNP, we have to apply it in simulations, measure thermodynamic quantities, 
and see if we reproduce known, experimental results. 

In this thesis we use the TensorFlow (TF) Python API to train neural networks, while LAMMPS, written in C<span style='color:#800000;'>\texttt</span>{++}, is
employed to run MD simulations for speed purposes. 
We therefore need a way to transfer the NNP from Python to C<span style='color:#800000;'>\texttt</span>{++}. 

There are built-in functions in TF that automatically saves computational graphs (architecture of network plus
nodes for training) and variables (weights and activation functions) as binary files.
These files can then be read and the NN evaluated using the TF C<span style='color:#800000;'>\texttt</span>{++} API. This API is however quite under-developed
and the documentation is very sparse. 

Another alternative that we have developed is to manually save the weights and biases
as a text file, along with the necessary hyperparameter values, such as choice of activation function and 
symmetry function parameters. This file 
can then be used to reconstruct the NN in the same way that it is made in Python: by representing
the connection weights between layers as matrices and the biases as vectors 
(<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:MLPmodel</span>} and <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:TensorFlowCreatingNN</span>}). 
The activation functions must be implemented manually. A linear algebra library can
be utilized for speed and simplicity; we have chosen to use Armadillo<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{http://arma.sourceforge.net/}{Armadillo}}. 

To find out which alternative that is the most computationally efficient, we compare the 
time usage between evaluating a NN with TensorFlow and Armadillo in C<span style='color:#800000;'>\texttt</span>{++}. We also compare with TF in Python to 
investigate if any differences between TF and Armadillo in C<span style='color:#800000;'>\texttt</span>{++} is caused by an under-developed C<span style='color:#800000;'>\texttt</span>{++} TF API, or 
if the cause is inherent to the TF implementation of neural networks.
The evaluation time 
is measured for a range of different NN sizes, and for each size an average of 50 evaluations have been computed.
The results are shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:timeComparisonEvaluateNetworkTotalScatter</span>}.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}[t]
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.9<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Tests/timeComparisonNetworkNew.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Scatter plot of time usage when evaluating untrained NNs with random weights and sigmoid activation functions
	   using the TF Python API (<span style='color:#00a000;'>$T_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{TFP}}$</span>), the TF C<span style='color:#800000;'>\texttt</span>{++} API (<span style='color:#00a000;'>$T_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{TFC}}$</span>) and Armadillo (<span style='color:#00a000;'>$T_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ARMA}}$</span>). 
	   <span style='color:#00a000;'>$L$</span> is the number of hidden layers, <span style='color:#00a000;'>$n$</span> is the number of nodes in each hidden layer. All the NNs
	   have one input and one output. The time has been calculated by averaging over 50 evaluations
	   for each NN architecture.}
<span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:timeComparisonEvaluateNetworkTotalScatter</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

Figure a) and b) display the ratio <span style='color:#00a000;'>$T_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{TFC}}/T_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ARMA}}$</span> for NN sizes up to 200, the former with all data points 
visible, while the latter is zoomed in to give a more detailed picture. We observe that the time difference is huge 
for small networks, and that even for NNs of size 200 the ratio is still about 5. From c), we deduce that the ratio seems to converge 
to 1 for large networks, but the convergence rate is surprisingly low. 
As we see from d), the performance of the built-in TF functions is similar 
in C<span style='color:#800000;'>\texttt</span>{++} and Python, thus the time deviation can not be caused by an under-developed C<span style='color:#800000;'>\texttt</span>{++} API. 

One reason for this phenomenon may be that the <span style='color:#800000;'>\texttt</span>{Session} environment in TF has a lot of overhead. 
Further, a NN size of 3000 is small compared to e.g.<span style='color:#800000;'>\ </span>state-of the art convolutional networks, which are usually trained with GPUs. 
Considering that we will not need NNs containing more than 100-200 neurons, a manual implementation of NNs with Armadillo 
is definitively the best choice. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Summary</span></b>}
In this chapter we have discussed the details of the NNP construction steps, specifically how to sample an adequate training set,
how to determine hyperparameters, and how to apply a NNP in simulations. 
The construction process will be exemplified 
in two specific cases. In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:LJValidation</span>}, we will train a NN reproduce the truncated and shifted 
Lennard-Jones potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}. The quality of the fit will be analyzed, but the NNP will not be applied 
in simulations. In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}, we will construct a NNP for solid-state Si and apply it in simulations 
to measure the radial distribution function and various mechanical properties. 




<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Lennard-Jones validation</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:LJValidation</span>}
As a first test of our implementation of the NNP construction process, we train a neural network to reproduce 
the truncated and shifted Lennard-Jones potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}. 
The fitting is performed with a NN that has one input and one output, resulting in 
a NNP that computes the potential energy of a central atom <span style='color:#00a000;'>$i$</span> in the presence of a single neighbour <span style='color:#00a000;'>$j$</span> of the same element at 
a distance <span style='color:#00a000;'>$r_{ij}$</span>. This one-dimensional NNP does not face the same symmetry issues as its many-dimensional counterpart: 
having only one input removes the problem that the energy should be invariant to the order of atoms, and 
the relative coordinate <span style='color:#00a000;'>$r_{ij}$</span> is invariant to a translation or rotation of a diatomic system. Thus, no symmetry functions 
are needed. 

The configuration space of this NNP is simply <span style='color:#00a000;'>$r_{ij} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [a,b]$</span>, where <span style='color:#00a000;'>$a$</span> and <span style='color:#00a000;'>$b$</span> are the smallest and largest 
interatomic distance respectively that the NNP is expected to experience in a simulation. 
Suitable values of <span style='color:#00a000;'>$a$</span> and <span style='color:#00a000;'>$b$</span> are found by running a MD simulation with the potential that is to be fitted
at the temperature intended for the NNP application of the NNP, and measure the minimum and maximum pair-wise distance. 
To sample this configuration space, we simply draw random numbers from a uniform distribution with <span style='color:#00a000;'>$a$</span> and <span style='color:#00a000;'>$b$</span> as endpoints,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
 inputTrain <b>=</b> np.random.uniform(a, b, trainSize)
 inputTest  <b>=</b> np.random.uniform(a, b, testSize)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
while the reference energies are calculated by employing the analytical expression for the target function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}. 
The hyperparameters used are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsLJ</span>}. These have not been optimized, but suitable values
have been determined on the basis of the discussion in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:settingHyperParams</span>}. The input data has 
not been transformed in any way. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>}[t]
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{12cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> ll}
      <span style='color:#800000;'>\toprule</span>
      Hyperparameters <b><span style='color:#002793;'>&amp;</span></b>  <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      Network structure <b><span style='color:#002793;'>&amp;</span></b> Inputs <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Outputs <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden layers <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden nodes <b><span style='color:#002793;'>&amp;</span></b> 10 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Activation <b><span style='color:#002793;'>&amp;</span></b> Sigmoid <span style='color:#800000;'>\\</span> 
      Training <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$a$</span>  <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#800000;'>\SI</span>{3.2}{<span style='color:#800000;'>\angstrom</span>} <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$b$</span>  <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#800000;'>\SI</span>{8.5125}{<span style='color:#800000;'>\angstrom</span>} <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Training set size <b><span style='color:#002793;'>&amp;</span></b> 10000 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Test set size <b><span style='color:#002793;'>&amp;</span></b> 1000 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Mini-batch size <b><span style='color:#002793;'>&amp;</span></b> 200 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Number of epochs <b><span style='color:#002793;'>&amp;</span></b> 5000 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Optimizer <b><span style='color:#002793;'>&amp;</span></b> Adam <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Initial learning rate <b><span style='color:#002793;'>&amp;</span></b> 0.001 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Weight initialization <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$U </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [-0.1, 0.1]$</span> <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Bias initialization <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$U </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [-0.1, 0.1]$</span> <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span> {Hyperparameters used in the fitting of the truncated and shifted Lennard-Jones potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}.
		The meaning of each parameter is found in the text. 
                ''Hidden nodes'' signifies the number of nodes in each hidden layer. } 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:hyperParamsLJ</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}

After 5000 epochs, we obtain a test set RMSE of 0.896 meV. This is a good result, considering that no hyperparameters 
were optimized. However, fitting a one-dimensional target function is not a great challenge, despite its non-linear behaviour. 
The test set RMSE and the training set RMSE follow each other very closely, and no overfitting occurs. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Error in configuration space</span></b>}
It can be useful to visualize the error distribution in configuration space 
by evaluating the trained NN on all data points in in the test set, and compare with the target function, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
...
<span style='color:#898887;'># evaluate trained network on this interval and find absolute error</span>
inputTest <b>=</b> np.sort(inputTest)
energiesNN <b>=</b> sess.run(NNprediction, feed_dict<b>=</b>{x: inputTest})
energiesLJ <b>=</b> LennardJones(inputTest)
energyError <b>=</b> energiesLJ <b>-</b> energiesNN
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
where <span style='color:#800000;'>\texttt</span>{x} is a <span style='color:#800000;'>\texttt</span>{tf.placeholder} as described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:TensorFlowCreatingNN</span>}. 
The absolute error <span style='color:#00a000;'>$E_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{LJ} - E_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{NN}$</span> is plotted as a function of distance <span style='color:#00a000;'>$r_{ij}$</span> in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:LJError</span>}.
We notice that the error oscillates around zero, and that the amplitude is larger for smaller distances.
This is in accordance with the observations made in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:samplingAlgorithms</span>}: The regions of configuration space 
where the gradient of the target function is large, is more difficult to fit, and should therefore have a higher density. 
The training set is in this case uniformly distributed, thus the error is larger for small inter-atomic distances where 
strong repulsive forces are at work. Nevertheless, the NN energy prediction follows the shape of the LJ potential very closely
for most values of <span style='color:#00a000;'>$r_{ij}$</span>. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/LJError.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Absolute error of a NN trained to reproduce the truncated and shifted Lennard-Jones potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}. 
           The plot is as a visualization of the error distribution in the configuration space <span style='color:#00a000;'>$r_{ij} </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [a,b]$</span> 
           defined by the test set. The employed hyperparameters are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsLJ</span>}.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:LJError</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

To obtain the forces in a simulation, we must calculate the gradient of the NN. 
In <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:hyperParamsCostFunction</span>} we mentioned that a small energy RMSE does not 
necessarily imply a small force RMSE. In this one-dimensional example, it is easy to measure the gradient error. 
This is done in a same manner as for the energy above, except now we are comparing the gradients of the NN and the target function. 
We also calculate the RMSE of the gradient on the test set,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
...
derivativeLJ <b>=</b> LennardJonesDerivative(inputTest)

<span style='color:#898887;'># define a NN gradient operation and differentiate</span>
gradientOP <b>=</b> tf.gradients(NNprediction, x)
derivativeNN <b>=</b> sess.run(gradientOP, feed_dict<b>=</b>{x: inputTest} )[<span style='color:#b08000;'>0</span>]

gradientError <b>=</b> derivativeLJ <b>-</b> derivativeNN
gradientRMSE <b>=</b> np.sqrt(np.<span style='color:#0057ae;'>sum</span>(gradientError<b>**</b><span style='color:#b08000;'>2</span>)<b>/</b>testSize)
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
We obtain a gradient RMSE of 24.3 meV, which is about one order of magnitude larger than for the function itself. 
This function-gradient RMSE ratio is also reported by others <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Natarajan16,Artrith12</span>}. 

The absolute error plot is shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:LJErrorDerivative</span>}. 
Again we have an oscillating error over the training interval, but with 
larger amplitudes. The gradient error at <span style='color:#00a000;'>$r_{ij} = a$</span> is about 50 times larger than the energy error at this point, which  
demonstrates the importance of having a higher density of training points in certain regions. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Implementation/LJErrorDerivative.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Absolute error of the gradient of a NN trained to reproduce the truncated and shifted Lennard-Jones 
           potential <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>shiftedLJ</span>}. Comparing with <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:LJError</span>}, we see that the graph is of the same shape, 
           but with larger oscillations. The employed hyperparameters are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsLJ</span>}. }
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:LJErrorDerivative</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}




<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Neural network potential for Si</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}
In this chapter we perform a demonstration of the complete NNP construction process, including 
the Behler-Parrinello method. The goal is construct a potential for solid-state Si by sampling atomic configurations 
and energies from Stillinger-Weber MD simulations. 
The eventual NNP will be validated by computing the radial distribution function and several mechanical properties, and compare
with SW. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Sampling initial set</span></b>}
To assemble the initial data set, we need to decide on the specific application of the NNP that we want to develop. 
We choose to construct a potential that can be used to simulate solid-state Si in a temperature range of <span style='color:#00a000;'>$T </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [0, 500] </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{K}$</span>. 
The system is prepared in a diamond cubic state with a lattice spacing of <span style='color:#800000;'>\SI</span>{5.431}{<span style='color:#800000;'>\angstrom</span>}, which is 
the crystalline structure of Si. We perform several simulations of the microcanonical ensemble with initial temperatures
in the above range and a time step of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t = 0.001 </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{ps}$</span>.
The SW potential is parametrized according to <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Stillinger85</span>}, with a force cutoff 
of <span style='color:#00a000;'>$r_c = </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{3.77118}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>. 
Periodic boundary conditions are imposed and the sampling algorithm 
<span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>} is applied in each simulation, resulting in 4406 structures and corresponding energies.
Note that if an <span style='color:#800000;'>\textit</span>{ab inito} NNP is to be constructed, the reference energies must be calculated with a
quantum mechanical method, which is outside the scope of this work.

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Initial symmetry function set</span></b>}
To get an idea of how many symmetry functions that are needed, we run a MD simulation as described above and 
measure the minimum, maximum and average coordination numbers as a function of time, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'># coordination number of each atom</span>
<span style='color:#a08000;'>compute neighbours all coord/atom cutoff 3.77118</span>

<span style='color:#a08000;'># global coordination numbers</span>
<span style='color:#a08000;'>compute minNeigh all reduce min c_neighbours</span>
<span style='color:#a08000;'>compute maxNeigh all reduce max c_neighbours</span>
<span style='color:#a08000;'>compute aveNeigh all reduce ave c_neighbours</span>

<span style='color:#a08000;'>thermo_style custom step c_minNeigh c_maxNeigh c_aveNeigh </span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
After 1e5 time steps, the minimum and maximum coordination number of any atom were 4 and 12 respectively, with an average 
of around 6. Thus, according to the discussion in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:initialSymmSet</span>}, 6-8 radial symmetry functions should be sufficient, 
while the angular symmetry functions should be at least twice as many. Note that this information must be obtained through 
emipirical knowledge of the system when constructing <span style='color:#800000;'>\textit</span>{ab inito} potentials. 

We have decided to use <span style='color:#00a000;'>$G_i^2$</span> as the radial function, 
since this is the better choice (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>}), and <span style='color:#00a000;'>$G_i^5$</span> as the angular counterpart
because of computational efficiency. The initial set thus consists of 8 equidistant <span style='color:#00a000;'>$G_i^2$</span> functions and 16 <span style='color:#00a000;'>$G_i^5$</span> functions, 
with parameters listed in Appendix <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>app:symmFuncParameters</span>}.

The initial radial and angular symmetry functions are shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:SiInitialSymmSet</span>}, as a function 
of inter-atomic distance <span style='color:#00a000;'>$r_{ij}$</span> and triplet angle <span style='color:#00a000;'>$</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>_{jik}$</span> respectively. 
Only the angular part of <span style='color:#00a000;'>$G_i^5$</span> is displayed. 
We have also calculated the radial and angular distribution of neighbouring atoms in the data set, displayed as dotted lines.
This is a convenient way to visualize how well the symmetry set represents the atomic environment:
functions that has no overlap with the area defined by the dotted lines does not contribute to the description. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
 <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minipage</span>}{0.48<span style='color:#800000;'>\linewidth</span>}
  <span style='color:#800000;'>\subcaption</span>{Radial symmetry functions}
  <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\textwidth</span>]{<span style='color:#0000d0;'>Figures/Results/SiInitialSymmG2.pdf</span>}
 <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minipage</span>}
<span style='color:#800000;'>\quad</span>
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minipage</span>}{0.48<span style='color:#800000;'>\linewidth</span>}
 <span style='color:#800000;'>\subcaption</span>{Angular symmetry functions}
 <b><span style='color:#800000;'>\includegraphics</span></b>[width=<span style='color:#800000;'>\textwidth</span>]{<span style='color:#0000d0;'>Figures/Results/SiInitialSymmG5.pdf</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minipage</span>} 
  <span style='color:#800000;'>\caption</span>{Initial set of 8 <span style='color:#00a000;'>$G_i^2$</span> and 16 <span style='color:#00a000;'>$G_i^5$</span> symmetry functions for constructing a Si NNP. Only the angular part <span style='color:#00a000;'>$G_i^</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>$</span> of 
           <span style='color:#00a000;'>$G_i^5$</span> is displayed. Further, only 8 of these are visible because pairs of <span style='color:#00a000;'>$G_i^5$</span> functions with identical
           angular parts overlap. 
           The dotted lines are the radial and angular distribution of neighbours defined 
           by the training set. The corresponding symmetry parameters are not included. }
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:SiInitialSymmSet</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Fitting the initial data set</span></b>}
According to the iterative scheme in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:iterativeMDsampling</span>}, the next step in the construction process
is to create a preliminary NNP by fitting the initial data set. We do not perform a systematic search for 
optimal hyperparameters at this intermediate stage. We have decided to employ the sigmoid activation function 
<span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>sigmoidActivationFunction</span>} for the remainder of the construction process
because it is more computationally efficient compared to the hyperbolic tangent, especially oc
its derivative,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> f^</span><span style='color:#606000;'>\prime</span><span style='color:#00a000;'>(x) = f(x) [1 - f(x)]</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Also, we have not experienced any significant improvement with the use of the hyperbolic tangent over the sigmoid.
Further, we have chosen the Adam optimizer for all training sessions, as this has been proven to outperform other 
schemes <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Kingma14</span>}. All the employed hyperparameters are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsSiInitial</span>}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>} 
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{12cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> ll}
      <span style='color:#800000;'>\toprule</span>
      Hyperparameters <b><span style='color:#002793;'>&amp;</span></b>  <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      Network structure <b><span style='color:#002793;'>&amp;</span></b> Inputs <b><span style='color:#002793;'>&amp;</span></b> 24 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Outputs <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden layers <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden nodes <b><span style='color:#002793;'>&amp;</span></b> 10 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Activation <b><span style='color:#002793;'>&amp;</span></b> Sigmoid <span style='color:#800000;'>\\</span> 
      Training <b><span style='color:#002793;'>&amp;</span></b> Training set size <b><span style='color:#002793;'>&amp;</span></b> 3900 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Test set size <b><span style='color:#002793;'>&amp;</span></b> 440 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Mini-batch size <b><span style='color:#002793;'>&amp;</span></b> 100 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Number of epochs <b><span style='color:#002793;'>&amp;</span></b> 20125 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Test set RMSE tolerance <b><span style='color:#002793;'>&amp;</span></b> 1.0 meV <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Optimizer <b><span style='color:#002793;'>&amp;</span></b> Adam <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Initial learning rate <b><span style='color:#002793;'>&amp;</span></b> 0.05 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Weight initialization <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$U </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [-0.1, 0.1]$</span> <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Bias initialization <b><span style='color:#002793;'>&amp;</span></b> Zeros <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Hyperparameters used in the fitting of the initial data set sampled from Stilling-Weber Si simulations. 
	       The data set is split into a training set and a test set, where the latter is 10 <span style='color:#800000;'>\%</span> of the total number of 
	       structures.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:hyperParamsSiInitial</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}
The symmetry-transformed training data has been shifted so that each symmetry function has zero-mean, and 
the minimum and maximum value of each symmetry function over the training set is stored for detecting 
extrapolating structures in the next step. 
Overfitting is detected by the following test,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
<b>if</b> testRMSE <b>/</b> trainRMSE <b>&gt;</b> <span style='color:#b08000;'>10</span>:
    <span style='color:#0057ae;'>print</span> <span style='color:#bf0303;'>'Overfitting is occurring, training ends'</span>
    <b>break</b>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
Several preliminary runs showed the test set RMSE to flatten out at about 1.0 meV, and we therefore decided 
to stop training if an error below this threshold was obtained, which occurred after 20125 epochs. 
The final test set RMSE was 0.992 meV. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Iterative sampling</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>sec:SiIterativeSampling</span>}
After the initial fit, we apply the preliminary NNP in MD simulations to find new relevant structures not present 
in the current training set. The Si system is prepared in the same manner as the Stillinger-Weber runs above. 
We use the minimum and maximum symmetry function values stored in the previous step to detect extrapolating
configurations according to the procedure described in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:iterativeMDsampling</span>}. Each time a structure
leads to a symmetry value outside the range of the above extreme values, we sample that structure, and add 
it to the training set, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{C++}
<span style='color:#0057ae;'>bool</span> extrapolation = <span style='color:#b08000;'>0</span>;
<b>for</b> (<span style='color:#0057ae;'>int</span> s=<span style='color:#b08000;'>0</span>; s &lt; m_numberOfSymmFunc; s++) {
  <b>if</b> (symmVector[s] &lt; m_minmax[s][<span style='color:#b08000;'>0</span>]) {
    m_minmax[s][<span style='color:#b08000;'>0</span>] = inputVector[s];
    extrapolation = <span style='color:#b08000;'>1</span>;
  }
  <b>else</b> <b>if</b> (symmVector[s] &gt; m_minmax[s][<span style='color:#b08000;'>1</span>]) {
    m_minmax[s][<span style='color:#b08000;'>1</span>] = inputVector[s];
    extrapolation = <span style='color:#b08000;'>1</span>;
  }
}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
where <span style='color:#800000;'>\texttt</span>{m<span style='color:#800000;'>\_</span>minmax} is a vector containing the minimum and maximum values of each symmetry function.
After a certain number of new structures is added, the NN fit is refined by training on the extended data set. 
The NNP simulations are carried out for several temperatures <span style='color:#00a000;'>$T </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [0,500] </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{K}$</span>. 
This iterative sampling process is continued until no new extrapolating structures are found, which occurs 
after three iterations. A total of 578 structures are added, resulting in an extended data set of 4984 training examples. 

Next, we employ the multiple-NN method (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:iterativeMDsampling</span>}) to identify new structures 
that are <span style='color:#800000;'>\textit</span>{within} the range of existing symmetry values, but located in regions of configuration space 
that are poorly represented in the training set. First, we train four NNs with different architectures on 
the complete training set to similar accuracies. 
Then, we run several MD simulations with one of these NNPs
to generate a large number configurations. The atomic coordinates are stored with the command
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>dump configs all custom 1 configs.txt id x y z vx vy vz</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
The same hyperparameters as in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsSiInitial</span>} are used, 
except that the RMSE tolerance is changed to 5 meV. Lastly, we run pseudo-simulations with the other three NNPs
on the configurations above, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>lstlisting</span>}[<span style='color:#a08000;'>style=lammps]</span>
<span style='color:#a08000;'>rerun configs.txt dump x y z vx vy vz</span>
<span style='color:#a08000;'>compute peAtom all pe/atom</span>
<span style='color:#a08000;'>dump potEnergy chosen custom 1 potEnergy.txt id c_peAtom</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>lstlisting</span>}
and compare the predicted potential energies of the atoms in group <span style='color:#800000;'>\texttt</span>{chosen}, which are randomly selected. 
Any structures for which the energy disagreement is significant, is added to the data set. An example of a 
potential energy comparison for a single atom is shown in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:multipleNNP</span>}. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Results/multipleNNP.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Example plot of the multiple-NN method. Four NNs with different architectures have been fitted to the same 
           data set to similar accuracies. Then, several configurations are generated with one of these NNPs, and 
           the potential energy predictions of each NNP for these structures are compared. The plot shows 
           the energies of a single atom.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:multipleNNP</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}
The simulations are carried out at an initial temperature of <span style='color:#00a000;'>$T = 300$</span> K, and the first 1000 time steps 
of one of these simulations are displayed. For each structure, we measure
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> E = E_{</span><span style='color:#606000;'>\max</span><span style='color:#00a000;'>} - E_{</span><span style='color:#606000;'>\min</span><span style='color:#00a000;'>}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> E$</span> is the difference between the maximum <span style='color:#00a000;'>$E_{</span><span style='color:#606000;'>\max</span><span style='color:#00a000;'>}$</span> and minimum <span style='color:#00a000;'>$E_{</span><span style='color:#606000;'>\min</span><span style='color:#00a000;'>}$</span> energy prediction
of the four NNPs on that structure. Any configuration for which, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> E &gt; </span><span style='color:#606000;'>\epsilon</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
is added to the training set, where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>$</span> is a constant of choice. We have found that setting 
<span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>$</span> equal to the RMSE tolerance, <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>=5$</span> meV, is an adequate choice. 

After adding 1595 new structures with the multiple-NN scheme, resulting in a final data set of 6579 training examples,
we consider the sampling as complete.



<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Fitting the final data set</span></b>}
Now that the iterative sampling scheme is finished, we are ready to construct the final NNP. 
To investigate if the symmetry function set needs to be adjusted, we calculate the range of each symmetry function 
on the training data. We find that the <span style='color:#00a000;'>$G_i^5$</span> functions with a cutoff of <span style='color:#00a000;'>$R_c = </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{3.0}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span> have a very small range 
of values, thus we change the cutoff to <span style='color:#00a000;'>$R_c = 4.0$</span>. Other than that, no adjustments are made. 

To find the optimal NN architecture, we perform a grid search of different 
number of hidden layers <span style='color:#00a000;'>$L$</span> and nodes in each hidden layer <span style='color:#00a000;'>$n_l$</span>. 
For each architecture <span style='color:#00a000;'>$(L,n_l)$</span> we perform a training session of 40000 epochs, and for every session we store 
the smallest RMSE obtained, together with the number of epochs and the time elapsed to reach this RMSE. 
We use the same hyperparameters as in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsSiInitial</span>} (with various architectures), 
except that no RMSE tolerance is set, and the mini-batch size is changed to 200 to decrease the error oscillations. 
We assume that no more than two layers are needed to fit the Si potential, and the maximum number of nodes has been 
set to 32.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>} 
<span style='color:#800000;'>\centering</span>
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{10cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> llll}
      <span style='color:#800000;'>\toprule</span>
      Layers <b><span style='color:#002793;'>&amp;</span></b> Nodes <b><span style='color:#002793;'>&amp;</span></b> RMSE <b><span style='color:#002793;'>&amp;</span></b> Epoch <b><span style='color:#002793;'>&amp;</span></b> Time <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      <span style='color:#00a000;'>$L=1$</span> <b><span style='color:#002793;'>&amp;</span></b> 4  <b><span style='color:#002793;'>&amp;</span></b> 4.445 <b><span style='color:#002793;'>&amp;</span></b> 37035 <b><span style='color:#002793;'>&amp;</span></b> 575 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 8  <b><span style='color:#002793;'>&amp;</span></b> 2.250 <b><span style='color:#002793;'>&amp;</span></b> 37305 <b><span style='color:#002793;'>&amp;</span></b> 570 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 12 <b><span style='color:#002793;'>&amp;</span></b> 2.303 <b><span style='color:#002793;'>&amp;</span></b> 37980 <b><span style='color:#002793;'>&amp;</span></b> 622 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> 2.201 <b><span style='color:#002793;'>&amp;</span></b> 39780 <b><span style='color:#002793;'>&amp;</span></b> 630 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 20 <b><span style='color:#002793;'>&amp;</span></b> 1.860 <b><span style='color:#002793;'>&amp;</span></b> 36180 <b><span style='color:#002793;'>&amp;</span></b> 617 <span style='color:#800000;'>\\</span> 
            <b><span style='color:#002793;'>&amp;</span></b> 24 <b><span style='color:#002793;'>&amp;</span></b> 1.928 <b><span style='color:#002793;'>&amp;</span></b> 37305 <b><span style='color:#002793;'>&amp;</span></b> 621 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 28 <b><span style='color:#002793;'>&amp;</span></b> 2.407 <b><span style='color:#002793;'>&amp;</span></b> 39375 <b><span style='color:#002793;'>&amp;</span></b> 697 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 32 <b><span style='color:#002793;'>&amp;</span></b> 2.214 <b><span style='color:#002793;'>&amp;</span></b> 38700 <b><span style='color:#002793;'>&amp;</span></b> 672 <span style='color:#800000;'>\\</span>
      <span style='color:#00a000;'>$L=2$</span> <b><span style='color:#002793;'>&amp;</span></b> 4  <b><span style='color:#002793;'>&amp;</span></b> 2.947 <b><span style='color:#002793;'>&amp;</span></b> 39960 <b><span style='color:#002793;'>&amp;</span></b> 750 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 8  <b><span style='color:#002793;'>&amp;</span></b> 1.933 <b><span style='color:#002793;'>&amp;</span></b> 36180 <b><span style='color:#002793;'>&amp;</span></b> 671 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 12 <b><span style='color:#002793;'>&amp;</span></b> 1.450 <b><span style='color:#002793;'>&amp;</span></b> 37350 <b><span style='color:#002793;'>&amp;</span></b> 766 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> 1.791 <b><span style='color:#002793;'>&amp;</span></b> 32265 <b><span style='color:#002793;'>&amp;</span></b> 633 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 20 <b><span style='color:#002793;'>&amp;</span></b> 1.492 <b><span style='color:#002793;'>&amp;</span></b> 24840 <b><span style='color:#002793;'>&amp;</span></b> 546 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 24 <b><span style='color:#002793;'>&amp;</span></b> 2.118 <b><span style='color:#002793;'>&amp;</span></b> 37620 <b><span style='color:#002793;'>&amp;</span></b> 819 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 28 <b><span style='color:#002793;'>&amp;</span></b> 1.455 <b><span style='color:#002793;'>&amp;</span></b> 37350 <b><span style='color:#002793;'>&amp;</span></b> 895 <span style='color:#800000;'>\\</span>
            <b><span style='color:#002793;'>&amp;</span></b> 32 <b><span style='color:#002793;'>&amp;</span></b> 2.008 <b><span style='color:#002793;'>&amp;</span></b> 14895 <b><span style='color:#002793;'>&amp;</span></b> 344 <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Results of a grid search for finding the optimal NN architecture to fit the final Si training set. 
               Each NN architecture is trained for 40000 epochs, and the smallest RMSE obtained during each session
               are listed, together with the number of epochs and time elapsed to reach this RMSE. 
               The RMSEs are in units of meV, while the time is measured in seconds.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:gridSearch</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}

The results are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:gridSearch</span>}. We immidiately observe that all RMSEs lie in a range of about 1.5-2.5 meV, except for the two cases where <span style='color:#00a000;'>$n_l = 4$</span>. 
This tells us that the quality of the fit is not very sensitive to the NN size. Some of the two-layer NNs
converge at an earlier epoch, but they do not produce significantly better fits, and since each epoch 
takes a longer time, they do not necessarily converge more <span style='color:#800000;'>\textit</span>{quickly}. Ideally, we should have trained each 
NN several times and calculated averages, but the above analysis is sufficient to conclude that the choice of NN size 
is not important to the quality of the fit. For computational efficiency of the eventual NNP,
we therefore decide to continue using 1 layer and 10 nodes.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>} 
<span style='color:#800000;'>\centering</span>
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{12cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> ll}
      <span style='color:#800000;'>\toprule</span>
      Hyperparameters <b><span style='color:#002793;'>&amp;</span></b>  <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      Network structure <b><span style='color:#002793;'>&amp;</span></b> Inputs <b><span style='color:#002793;'>&amp;</span></b> 24 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Outputs <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden layers <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Hidden nodes <b><span style='color:#002793;'>&amp;</span></b> 10 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Activation <b><span style='color:#002793;'>&amp;</span></b> Sigmoid <span style='color:#800000;'>\\</span> 
      Training <b><span style='color:#002793;'>&amp;</span></b> Training set size <b><span style='color:#002793;'>&amp;</span></b> 5922 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Test set size <b><span style='color:#002793;'>&amp;</span></b> 657 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Number of epochs <b><span style='color:#002793;'>&amp;</span></b> 11000 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Optimizer <b><span style='color:#002793;'>&amp;</span></b> Adam <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Initial learning rate <b><span style='color:#002793;'>&amp;</span></b> 0.05 <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Weight initialization <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$U </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [-0.1, 0.1]$</span> <span style='color:#800000;'>\\</span>
      <b><span style='color:#002793;'>&amp;</span></b> Bias initialization <b><span style='color:#002793;'>&amp;</span></b> Zeros <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Hyperparameters used in the fitting of the final training set for construction of the Si potential.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:hyperParamsSiFinal</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}

The hyperparameters of the final NN fit are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:hyperParamsSiFinal</span>}. As for the initial fit, 
the training data is shifted so that each symmetry function has zero-mean. This, in combination with zero-centered 
(small) weights and biases, prevent premature saturation of the sigmoid activation function, which is of great importance 
for the quality of the fit.

To investigate whether offline or online learning produce the best fit, we run several training sessions employing both. 
We find that offline learning consequently results in smaller RMSEs, thus we decide to use this method for the final fit. 
This is slower than online learning, but since we work with quite small data sets, convergence speed is not an issue, 
as we can see from <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:gridSearch</span>}. 

The NN is trained for 40000 epochs, and for every 1000 epochs the current NN parameters 
and RMSE are stored. When training is finished, the NN with the smallest RMSE is selected as the final NNP. 
The best fit was obtained after 11000 epochs, with an RMSE of 0.864 meV. This result is similar to other studies
employing the Behler-Parrinello method for NN interpolation <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler07,Artrith12,Natarajan16</span>} .  

We have only fitted energies, and not forces, thus the exact force RMSE is not easily available. This would have required 
the computation of Stillinger-Weber forces for all configurations sampled from NNP simulations, and vice versa.
However, a good estimate can be obtained by running a SW simulation to generate a large number of configurations, and 
store the total force on a single atom at each time step. Then, we run a pseudo-simulation with the NNP on these configurations, 
and compare the computed forces on the same atom. The resulting force RMSE is 41.2 meV, 
which is also similar to other studies <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Artrith12,Natarajan16</span>}. 
As for the Lennard-Jones NNP (<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:LJValidation</span>}), the RMSE of the forces is between 1-2 orders of magnitude 
larger than that for the energy.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.7<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Results/SiForces.pdf</span>}
  <span style='color:#800000;'>\caption</span>{The RMSE of the Si NNP force predictions on a single atom as a function of the total force on the atom,
	   demonstrating a clear correlation. 
	   The force errors were calculated based on 3000 configurations generated by Stillinger-Weber simulations.}
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:SiForces</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

We also wish to investigate if there is any correlation between the force error and the magnitude of the force. 
This can be done by binning the set of force errors calculated above, and computing the RMSE of each bin, shown 
in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:SiForces</span>}. The data points are placed at the bin centers. We clearly see that there 
is a correlation, and that the error of the strongest forces are significantly larger than for small forces. 
This tells us that the large potential gradient-regions of configuration space do not have a sufficiently
high density in the training set, despite the use of the sampling algorithm <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>}
for the initial data set. 



<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Applying the NNP</span></b>}
Calculating fitting errors is not sufficient for assessing the applicability of a NNP. 
To properly validate the potential we have constructed, we need to evaluate its accuracy in MD simulations.
For validation, we have chosen to confirm that the crystal structure of Si and various mechanical properties are reproduced by the NNP
by comparing with Stillinger-Weber results. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>figure</span>}
<span style='color:#800000;'>\centering</span>
  <b><span style='color:#800000;'>\includegraphics</span></b>[width = 0.8<span style='color:#800000;'>\linewidth</span>]{<span style='color:#0000d0;'>Figures/Results/radialDist.pdf</span>}
  <span style='color:#800000;'>\caption</span>{Comparison of the radial distribution function of a Stillinger-Weber (SW) simulation and 
           a NNP simulation of a Si crystal at temperature of <span style='color:#00a000;'>$T </span><span style='color:#606000;'>\approx</span><span style='color:#00a000;'> 150$</span> K. The crystal structure 
           of Si is reproduced by the NNP, with a small difference attributed to the increased kinetic 
           energy of the atoms due to the energy error. }
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>fig:radialDist</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>figure</span>}

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Radial distribution function</span></b>}
The radial distribution function <span style='color:#00a000;'>$g(r)$</span>, (or pair correlation function) of an atomic system is a measure of how 
particle density varies as a function of distance from a reference atom. If a system is randomly distributed, all distances
are equally probable, and <span style='color:#00a000;'>$g(r)$</span> is unity for all distances. Any deviations from unity signifies an ordered structure.
The typical shape of <span style='color:#00a000;'>$g(r)$</span> for a crystal lattice is a flat region for small distances due to repulsive forces, 
followed by a series of peaks corresponding to bond lengths. 

To measure <span style='color:#00a000;'>$g(r)$</span>, we prepare a system consisting of 1000 Si atoms on a diamond cubic lattice with a lattice constant 
of <span style='color:#00a000;'>$b = </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{5.431}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>. The simulation box is cubic with sides <span style='color:#00a000;'>$5L$</span>, where <span style='color:#00a000;'>$L$</span> is the length of one unit cube.
The initial temperature is <span style='color:#00a000;'>$T = 300$</span> K, and NVE-integration is performed, creating a system trajectory consistent with 
the microcanonical ensemble. The system is equilibrated for 10 ps, with a time step of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> t = 0.001$</span> ps. Then, 
<span style='color:#00a000;'>$g(r)$</span> is calculated every 100 time steps for 1000 time steps, and an average of these 10 measurements are obtained. 

The above simulation is executed with both the NNP and the SW potential. The resulting distributions are 
depicted in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>fig:radialDist</span>} for <span style='color:#00a000;'>$r </span><span style='color:#606000;'>\in</span><span style='color:#00a000;'> [2.0, r_c]$</span>, with a cutoff of <span style='color:#00a000;'>$r_c = 3.77118$</span> as above. 
We observe that the distinct crystal structure of Si is reproduced, but the peak centered at <span style='color:#00a000;'>$r </span><span style='color:#606000;'>\approx</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{2.3}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>
is slightly wider and less tall than than that for SW. This difference is attributed to the error of the energy fit, 
which leads to a slight increase in the kinetic energy of the atoms, resulting in larger vibrations. 

<span style='color:#f00000;'>\subsection</span>{<b><span style='color:#000000;'>Mechanical properties</span></b>}
To further validate the NNP, we measure the bulk modulus, the shear modulus and the Poisson ratio of the Si crystal in 
NNP simulations. Bulk modulus <span style='color:#00a000;'>$K$</span> is the coefficient that relates stress to strain under uniform compression,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{F}{A} = K</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> V}{V_0}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$F/A$</span> is the uniform pressure (stress), and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> V/V_0$</span> is the induced relative volume change (strain). 
A material with a large bulk modulus experiences a small volume change when pressure is applied. Note that the material 
only changes volume, not shape, under a uniform compression.

A force that is applied tangentially (or transversely) to one face of an object is called a <span style='color:#800000;'>\textit</span>{shear} stress. 
Shear stress changes the shape of an object, but not volume. 
Shear modulus <span style='color:#00a000;'>$G$</span> is the coefficient that relates shear stress to shear strain.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{F}{A} = G</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> x}{y}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$</span><span style='color:#606000;'>\Delta</span><span style='color:#00a000;'> x/y$</span> is the shear strain. 

When a material experiences a compressive force, it tends to expand in directions perpendicular to the direction of compression. 
Conversely, if the material is stretched, it tends to <span style='color:#800000;'>\textit</span>{contract} in the perpendicular direction. The Poisson 
ratio <span style='color:#00a000;'>$</span><span style='color:#606000;'>\nu</span><span style='color:#00a000;'>$</span> is the ratio of relative contraction to relative expansion, or vice versa. More generally, it is the ratio 
of transverse strain <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{trans}$</span> to longitudinal strain <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{long}$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\nu</span><span style='color:#00a000;'> = -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_{</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{trans}}}{</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{long}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
These quantities can be obtained by employing Hooke's law. For an anisotropic material like Si, this is written as a tensor 
equation
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_{ij} = c_{ijkl}</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_{kl}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where <span style='color:#00a000;'>$c_{ijkl}$</span> is a fourth rank <span style='color:#800000;'>\textit</span>{stiffness} tensor relating stress <span style='color:#00a000;'>$</span><span style='color:#606000;'>\sigma</span><span style='color:#00a000;'>_{ij}$</span> and strain <span style='color:#00a000;'>$</span><span style='color:#606000;'>\epsilon</span><span style='color:#00a000;'>_{kl}$</span>. 
However, the cubic symmetry of Si allows us to specify the stiffness tensor as a matrix (tensor of rank two)
with only three independent components <span style='color:#00a000;'>$c_{11}$</span>, <span style='color:#00a000;'>$c_{12}$</span> and <span style='color:#00a000;'>$c_{44}$</span>, called <span style='color:#800000;'>\textit</span>{elastic constants}. 
The bulk modulus, shear modulus and Poisson ratio 
is then calculated by the below formulas <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Hopcroft10</span>}. 

<span style='color:#800000;'>\noindent</span> Bulk modulus:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> B = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{c_{11} + 2c_{12}}{3}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Shear modulus: 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> S = c_{44}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Poisson ratio:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{1 + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{c_{11}}{c_{22}}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
To measure <span style='color:#00a000;'>$c_{11}$</span>, <span style='color:#00a000;'>$c_{12}$</span> and <span style='color:#00a000;'>$c_{44}$</span> in a simulation, we employ a set of example scripts contained in the LAMMPS distribution<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{https://github.com/johnands/lammps/tree/master/examples/ELASTIC}
{https://github.com/johnands/lammps/tree/master/examples/ELASTIC}}.
These are slightly modified so that our NNP is employed to compute energies and forces instead of SW. 
The elastic constants are obtained by deforming the simulation box and calculating the induced stress. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>}[h] 
<span style='color:#800000;'>\centering</span>
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{12cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> lll}
      <span style='color:#800000;'>\toprule</span>
      <b><span style='color:#002793;'>&amp;</span></b> NNP <b><span style='color:#002793;'>&amp;</span></b> Analytic SW <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Cowley88</span>} <b><span style='color:#002793;'>&amp;</span></b> Relative error  <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      Bulk modulus    <b><span style='color:#002793;'>&amp;</span></b> 103.0  <b><span style='color:#002793;'>&amp;</span></b> 101.4 <b><span style='color:#002793;'>&amp;</span></b>  1.58 <span style='color:#800000;'>\%</span> <span style='color:#800000;'>\\</span>
      Shear modulus   <b><span style='color:#002793;'>&amp;</span></b> 53.6   <b><span style='color:#002793;'>&amp;</span></b> 56.4  <b><span style='color:#002793;'>&amp;</span></b>  5.22 <span style='color:#800000;'>\%</span> <span style='color:#800000;'>\\</span>
      Poisson ratio   <b><span style='color:#002793;'>&amp;</span></b> 0.348  <b><span style='color:#002793;'>&amp;</span></b> 0.335 <b><span style='color:#002793;'>&amp;</span></b>  3.88 <span style='color:#800000;'>\%</span> <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Various mechanical properties measured from NNP simulations of Si. The analytical values 
	       for Stillinger-Weber are included, along with the relative error. The bulk modulus and 
	       shear modulus have units of GPa, while the Poisson ratio is unitless.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:SiMechanical</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}

The results are listed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>tab:SiMechanical</span>}. We observe that the SW values are reproduced with small relative errors, 
demonstrating that also the mechanical properties of the Si crystal are modelled correctly with the NNP.
Since SW has been used to compute reference energies for the NNP fit, we do not compare with experimental values, as 
this would only assess the accuracy of SW, not the NNP. 







<span style='color:#f00000;'>\part</span>{<b><span style='color:#000000;'>Conclusions and future work</span></b>}

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Summary and conclusion</span></b>}
In this thesis, we developed a method for constructing high-dimensional neural network potentials. This method was applied
for the construction of an inter-atomic potential for solid-state Si, and the radial distribution function,
bulk modulus, shear modulus and Poisson ratio were measured for benchmarking.

To find atomic structures that were relevant for the intended application of the potential, 
we implemented a custom <span style='color:#800000;'>\texttt</span>{compute} in LAMMPS 
that sampled atom-centered configurations and total potential energies according to a modified sampling algorithm. 
This <span style='color:#800000;'>\texttt</span>{compute} was used to sample configurations from Stillinger-Weber simulations, which constituted the initial 
training data for the construction of the Si potential. 

We implemented the Behler-Parrinello method to deal with the fixed dimensionality and symmetry problems of neural networks. 
A set of symmetry functions was created for the initial Si training data. This set was compared with 
the radial and angular distribution of atoms in the training data to evaluate its ability to adequately probe 
the atomic environments. The symmetry functions were also used in an iterative sampling scheme to find new relevant structures 
outside the range of existing symmetry values. Further, the multiple-NN method was employed to detect
regions of configurations space poorly represented in the training set. 

We developed a set of Python scripts to train neural networks to fit the sampled data sets. 
The machine learning library TensorFlow was used for this purpose. Various properties of the fitting procedure were 
investigated by training a one-dimensional neural network to reproduce the truncated and shifted Lennard-Jones potential.
We found that the NN were able to accurately fit the functional form of the LJ potential, and obtained a RMSE 
of 0.896 meV. The absolute error was plotted as a function of inter-atomic distance, demonstrating that the error 
was largest for small distances. 

Further, TensorFlow was used in combination with the BP method to construct a potential for Si by 
fitting data sets sampled from both Stillinger-Weber and preliminary NNP simulations. The fit was gradually refined 
with the iterative sampling scheme mentioned above. A final RMSE of 0.864 meV was obtained, 
which was comparable with other studies. 

Lastly, we applied the eventual Si NNP in MD simulations to evaluate its accuracy. 
The radial distribution function was measured, and was found to be in good agreement with Stillinger-Weber. 
The small deviations were attributed to an increase in the kinetic energy of the atoms due to the fitting error.
Further, various mechanical properties of the NNP Si system were calculated, specifically the bulk modulus, the shear 
modulus, and the Poisson ratio. The results had relative errors in the range 1.5-5.2 <span style='color:#800000;'>\%</span> when compared to the analytic 
Stillinger-Weber values. 


<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Properties of a high-dimensional neural network potential</span></b>} 
When developing a new PES, it is useful to define a set of properties that the ''ideal'' potential 
should possess. These may serve as guiding principles during the construction process, and is also a good 
starting point for comparing different types of potentials. Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler11general</span>} lists 10 requirements
that the ideal potential should meet. We will now evaluate our NNP in light of these requirements. 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>enumerate</span>}
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should be accurate, i.e.<span style='color:#800000;'>\ </span>produce results that are in good agreement with experiments. 
       Ideally, it should be constructed using <span style='color:#800000;'>\textit</span>{ab inito} methods.} 
       
       The NNP can be fitted to <span style='color:#800000;'>\textit</span>{ab inito} data to arbitrary accuracy, and 
       can therefore be considered an <span style='color:#800000;'>\textit</span>{ab inito} potential.  
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{There should be ways to systematically detect situations where the potential is not sufficiently accurate, 
       and improve it without too much effort.} 
       
       The NNP can be systematically improved by employing the iterative scheme proposed in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:selectingTrainingData</span>}.
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should be unbiased and applicable to all types of systems and chemical bonding.} 
       
       The NNP has no predefined functional form, and its applications are unlimited as 
       long as the production of <span style='color:#800000;'>\textit</span>{ab inito} reference energies is feasible. However, for systems containing 
       several different chemical elements, the symmetry function vectors grow very large, and the symmetry function sets 
       get increasingly difficult to set up.
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should enable the breaking and making of bonds.}
       
       The NNP is able to model the breaking and making of bonds as long as the reference set contains
       configurations that are relevant for these processes.  
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should be high-dimensional, i.e.<span style='color:#800000;'>\ </span>depend on all degrees of freedom of the system.}
       
       The NNP is high-dimensional because it depends on all the degrees of freedom of the chemical environment of each atom. 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The construction of the potential should be as automated as possible.} 
       
       The NN parameters, and hence the functional form of the NNP, is automatically optimized. However, 
       the determination of symmetry parameters and other hyperparameters is to a certain degree empirical. 
       Further, training data can be automatically produced by MD simulations, but the physical conditions 
       of each simulation and the sampling of configurations must be carefully considered. 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should be predictive, i.e.<span style='color:#800000;'>\ </span>be reliable for atomic configurations that have not been used in the 
       construction.}
       
       The NNP is generally not able to extrapolate, and is thus reliable only for configurations that are within 
       the configuration space spanned by the training set.
       
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The potential should be fast to evaluate.}
 
       The NNP is several orders of magnitude faster to evaluate than e.g.<span style='color:#800000;'>\ </span>DFT or Hartree-Fock potentials, 
       and thus enables MD simulations with <span style='color:#800000;'>\textit</span>{ab inito} accuracy on larger scales of space and time.  
       It is however notably slower compared to empirical potentials like Lennard-Jones or Stillinger-Weber. 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{The construction of the potential should not require much computation time.}
 
       Training does not require much computation time, but calculating reference energies for thousands of configurations
       is time consuming. 
 <span style='color:#800000;'>\item</span> <span style='color:#800000;'>\textit</span>{To compute forces, analytic derivatives of the potential should be available.}
       Analytic derivatives of both the NN and the symmetry functions are readily available. 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>enumerate</span>}
We observe that our NNP fulfills the majority of the 10 requirements stated above.  
None of the potentials currently available, including NNPs, are ideal. 
However, NNPs are an excellent tool for multi-scale physics. They connect the realms of quantum mechanics 
and molecular dynamics by enabling <span style='color:#800000;'>\textit</span>{ab inito} simulations on a larger scale of space and time than what is feasible 
for conventional <span style='color:#800000;'>\textit</span>{ab inito} MD. This property, in addition to the unbiased and transferable nature of the 
Behler-Parrinello approach, 
signify that research in this field will be increasingly important in the future. 

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Prospects and future work</span></b>}
An important aspect of the construction of NNPs is to employ a suitable <span style='color:#800000;'>\textit</span>{ab inito} method to calculate reference energies.
In this thesis, we have computed target energies with common empirical potentials, and thus not constructed <span style='color:#800000;'>\textit</span>{ab inito}
potentials. The most obvious extension to the current work is therefore to employ methods like Hartree-Fock or Density Functional theory 
to obtain energies. However, there is also much room for improvement in other areas. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Force accuracy</span></b>}
We saw in chapters <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:LJValidation</span>} and <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>} that the force RMSE is generally 
1-2 orders of magnitude larger than that for the energy. The forces determine the dynamics of a simulation, and their accuracy 
is therefore of vital importance. One way to improve the force RMSE is to employ the modified cost function
<span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>CFDA</span>} to fit both energies and forces. To do this, we must compute reference forces in addition to energies, 
which is obviously more computationally demanding. However, according to Behler <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler15</span>}, adding forces to the training 
data reduces the required number of reference energies, leading to a smaller data set. 

Further, we have repeatedly observed that the fitting error increases for large forces and energies, which can be avoided 
by including more high-gradient configurations in the training data. We employed a sampling algorithm
<span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>samplingAlgorithmModified</span>} for this purpose, and although this reduced the problem, it did not remove it entirely. 
Improved sampling algorithms could therefore be an improvement to this work. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Other systems</span></b>}
We have constructed a NNP for a system consisting of only one chemical element, but the Behler-Parrinello (BP) method 
is applicable to systems containing several elements. The BP scheme is high-dimensional, which 
makes it suitable for studying complex systems where many degrees of freedom must be included to provide an accurate description 
of the dynamics. The BP method can also be extended to include electrostatic contributions <span style='color:#f00000;'>\cite</span>{<span style='color:#0000d0;'>Behler15</span>}, 
and water <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{H_2O}$</span> is therefore an interesting candidate. 

NNPs are also able to model the breaking and making of bonds; studying chemical reaction dynamics is 
therefore a possibility. 

<span style='color:#f00000;'>\section</span>{<b><span style='color:#000000;'>Optimization</span></b>}
For NNPs to serve as a multi-scale physics tool, they must be much more computationally efficient than 
<span style='color:#800000;'>\textit</span>{ab inito} methods. Ideally, their computational efficiency should be comparable to that of empirical potentials. 
We have not performed any proper performance analysis in this thesis, but 
based on several simulations with the Si NNP, we have found that Stillinger-Weber is between 1-2 orders of magnitude faster.
Consequentially, the NNP is several orders of magnitude faster than <span style='color:#800000;'>\textit</span>{ab inito} methods<span style='color:#898887;'>%</span>
<span style='color:#800000;'>\footnote</span>{<span style='color:#800000;'>\href</span>{http://lammps.sandia.gov/bench.html}{lammps.sandia.gov/bench.html}}. 

However, optimizing the evaluations of the symmetry functions and the network may contribute to a significant speed-up. 
We have employed Armadillo to evaluate and differentiate NNPs, which may not the most efficient approach. Further, 
the size of the NN and the symmetry function set may be reduced by performing a more thorough hyperparameter optimization. 














<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>appendices</span>}

<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Symmetry function derivatives</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>app:appendixA1</span>}
According to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>forceAtomkChainRule</span>}, we need the derivatives of the symmetry functions defined in 
<span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmetryFunctions</span>} to obtain forces during simulations. Analytic derivatives have been obtained 
with SymPy, shown here for <span style='color:#00a000;'>$G_i^5$</span>,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>minted</span>}{Python}
<span style='color:#924c9d;'>from</span> sympy.utilities.codegen <span style='color:#924c9d;'>import</span> codegen
<span style='color:#924c9d;'>from</span> sympy <span style='color:#924c9d;'>import</span> <b>*</b>

<span style='color:#898887;'># variables</span>
xj, yj, zj, xk, yk, zk <b>=</b>  symbols(<span style='color:#bf0303;'>'xj yj zj xk yk zk'</span>)

<span style='color:#898887;'># parameters</span>
eta, Rc, zeta, Lambda <b>=</b> symbols(<span style='color:#bf0303;'>'eta Rc zeta Lambda'</span>)

<span style='color:#898887;'># substitution parameters</span>
Rj, Rk, Rj2, Rk2, RjDotRk, CosTheta, Fcj, Fck, dFcj, dFck <b>=</b> <b>\</b>
    symbols(<span style='color:#bf0303;'>'Rj Rk Rj2 Rk2 RjDotRk CosTheta Fcj Fck dFcj dFck'</span>)

rj <b>=</b> sqrt(xj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zj<b>**</b><span style='color:#b08000;'>2</span>)
rk <b>=</b> sqrt(xk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zk<b>**</b><span style='color:#b08000;'>2</span>)
fcj <b>=</b> <span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>rj<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>
fck <b>=</b> <span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>rk<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>
rjDotrk <b>=</b> (xj<b>*</b>xk <b>+</b> yj<b>*</b>yk <b>+</b> zj<b>*</b>zk)
cosTheta <b>=</b> rjDotrk <b>/</b> (rj<b>*</b>rk)

F1 <b>=</b> <span style='color:#b08000;'>2</span><b>**</b>(<span style='color:#b08000;'>1</span><b>-</b>zeta) <b>*</b> (<span style='color:#b08000;'>1</span> <b>+</b> Lambda<b>*</b>cosTheta)<b>**</b>zeta
F2 <b>=</b> exp(<b>-</b>eta<b>*</b>(rj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> rk<b>**</b><span style='color:#b08000;'>2</span>))
F3 <b>=</b> fcj<b>*</b>fck
G5 <b>=</b> F1<b>*</b>F2<b>*</b>F3

dG5dxj <b>=</b> diff(G5, xj)
dG5dxj <b>=</b> dG5dxj.subs(xj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zj<b>**</b><span style='color:#b08000;'>2</span>, Rj2)
dG5dxj <b>=</b> dG5dxj.subs(xk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zk<b>**</b><span style='color:#b08000;'>2</span>, Rk2)
dG5dxj <b>=</b> dG5dxj.subs(sqrt(Rj2), Rj)
dG5dxj <b>=</b> dG5dxj.subs(sqrt(Rk2), Rk)
dG5dxj <b>=</b> dG5dxj.subs(xj<b>*</b>xk <b>+</b> yj<b>*</b>yk <b>+</b> zj<b>*</b>zk, RjDotRk)
dG5dxj <b>=</b> dG5dxj.subs(RjDotRk<b>/</b>(Rj<b>*</b>Rk), CosTheta)
dG5dxj <b>=</b> dG5dxj.subs(<span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>Rj<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>, Fcj)
dG5dxj <b>=</b> dG5dxj.subs(<span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>Rk<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>, Fck)
dG5dxj <b>=</b> dG5dxj.subs(sin(pi<b>*</b>Rj<b>/</b>Rc)<b>/</b>(Rc), <b>-</b><span style='color:#b08000;'>2</span><b>*</b>dFcj<b>/</b>pi)

dG5dxk <b>=</b> diff(G5, xk)
dG5dxk <b>=</b> dG5dxk.subs(xj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yj<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zj<b>**</b><span style='color:#b08000;'>2</span>, Rj2)
dG5dxk <b>=</b> dG5dxk.subs(xk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> yk<b>**</b><span style='color:#b08000;'>2</span> <b>+</b> zk<b>**</b><span style='color:#b08000;'>2</span>, Rk2)
dG5dxk <b>=</b> dG5dxk.subs(sqrt(Rj2), Rj)
dG5dxk <b>=</b> dG5dxk.subs(sqrt(Rk2), Rk)
dG5dxk <b>=</b> dG5dxk.subs(xj<b>*</b>xk <b>+</b> yj<b>*</b>yk <b>+</b> zj<b>*</b>zk, RjDotRk)
dG5dxk <b>=</b> dG5dxk.subs(RjDotRk<b>/</b>(Rj<b>*</b>Rk), CosTheta)
dG5dxk <b>=</b> dG5dxk.subs(<span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>Rj<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>, Fcj)
dG5dxk <b>=</b> dG5dxk.subs(<span style='color:#b08000;'>0.5</span><b>*</b>cos(pi<b>*</b>Rk<b>/</b>Rc) <b>+</b> <span style='color:#b08000;'>0.5</span>, Fck)
dG5dxk <b>=</b> dG5dxk.subs(sin(pi<b>*</b>Rk<b>/</b>Rc)<b>/</b>(Rc), <b>-</b><span style='color:#b08000;'>2</span><b>*</b>dFck<b>/</b>pi)

<span style='color:#0057ae;'>print</span> codegen((<span style='color:#bf0303;'>&quot;dG5dxj&quot;</span>, simplify(dG5dxj)), <span style='color:#bf0303;'>&quot;C&quot;</span>, <span style='color:#bf0303;'>&quot;file&quot;</span>)[<span style='color:#b08000;'>0</span>][<span style='color:#b08000;'>1</span>]
<span style='color:#0057ae;'>print</span> codegen((<span style='color:#bf0303;'>&quot;dG5dxk&quot;</span>, simplify(dG5dxk)), <span style='color:#bf0303;'>&quot;C&quot;</span>, <span style='color:#bf0303;'>&quot;file&quot;</span>)[<span style='color:#b08000;'>0</span>][<span style='color:#b08000;'>1</span>]
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>minted</span>}
However, calculating the derivatives by hand allows us to write the expression for the derivatives in a more compact form, 
speeding up the force computations.
Thus, we compute the derivatives of all symmetry functions except <span style='color:#00a000;'>$G_i^3$</span> in the following. The SymPy expressions serve 
as a validation of our calculations. 

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Notation and symmetries</span></b>}
In some cases we only show the deriviative with respect to 
<span style='color:#00a000;'>$r_{ij}$</span>. Each component <span style='color:#00a000;'>$</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>$</span> can be calculated by
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\alpha</span><span style='color:#00a000;'>}{r_{ij}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The following notation applies,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> r_{ij} = </span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>(x_{ij}^2 + y_{ij}^2 + z_{ij}^2</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>)^{1/2} =</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>((x_j - x_i)^2 + (y_j - y_i)^2 + (z_j - z_i)^2</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>)^{1/2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
From <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:symmAndForces</span>} we have the identities,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_j} </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{symmetryDerivativeApp1} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} &amp;= -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_i} </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{symmetryDerivativeApp2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Derivatives of cutoff function and radial symmetry functions</span></b>}
Derivative of the cutoff function <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>cutoffFunction</span>}:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r)}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r} = -</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{2}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\pi</span><span style='color:#00a000;'>}{r_c} </span><span style='color:#606000;'>\sin\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\pi</span><span style='color:#00a000;'> r}{r_c}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) = M(r)</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutOffFunctionDerivative}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
This expression is valid for all combinations of indicies <span style='color:#00a000;'>$(i,j,k)$</span>. The derivatives w.r.t. to individual coordinates are
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = M(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutOffFunctionDerivative1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = M(r_{ik})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutOffFunctionDerivative1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
while
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = 0</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
according to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>symmetryDerivativeApp1</span>}. Also, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{jk})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = -M(r_{jk})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutOffFunctionDerivative1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{jk})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = M(r_{jk})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{cutOffFunctionDerivative1}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The derivative of each term in <span style='color:#00a000;'>$G_i^1$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G1</span>} is simply equal to <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>cutOffFunctionDerivative</span>}.

<span style='color:#800000;'>\noindent</span> Derivative of <span style='color:#00a000;'>$G_i^2$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G2</span>}:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^2}{r_{ij}} = </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>(r_{ij}-r_s)^2) </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>[2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>(r_s - r_{ij}) + </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>]</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}

<span style='color:#f00000;'>\subsubsection</span>{<b><span style='color:#000000;'>Derivatives of angular symmetry functions</span></b>}
<span style='color:#00a000;'>$G_i^4$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G4</span>}:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^4 = F_1(</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>)F_2(r_{ij},r_{ik},r_{jk})F_3(r_{ij},r_{ij},r_{jk})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> F_1(</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>) &amp;= 2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}(1 + </span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{ijk})^</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> F_2(r_{ij},r_{ik},r_{jk}) &amp;= </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> (r_{ij}^2 + r_{ik}^2 + r_{jk}^2)] </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> F_3(r_{ij},r_{ij},r_{jk}) &amp;= f_c(r_{ij}) f_c(r_{ik}) f_c (r_{jk})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
and where
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'> = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}x_{ik} + y_{ij}y_{ik} + z_{ij}z_{ik}}{r_{ij}r_{ik}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Using the product rule:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^4}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = &amp;</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_1}{x_{ij}} F_2 F_3 + </span>
<span style='color:#00a000;'> F_1 </span><span style='color:#606000;'>\left</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}F_3 + F_2</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\right</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_1}{x_{ij}} F_2 F_3 + </span>
<span style='color:#00a000;'> F_1 </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}F_3 + F_1F_2</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\label</span><span style='color:#00a000;'>{G4Derivative}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
We have
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_1}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_1}{</span><span style='color:#606000;'>\partial\cos\theta</span><span style='color:#00a000;'>}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial\cos\theta</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_1}{</span><span style='color:#606000;'>\partial\cos\theta</span><span style='color:#00a000;'>} = </span><span style='color:#606000;'>\lambda\zeta</span><span style='color:#00a000;'>(1 + </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>)^{</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>-1} = K</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial\cos\theta</span><span style='color:#00a000;'>}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ij}r_{ik}} - </span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ij}^2}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
To get the corresponding expression for <span style='color:#00a000;'>$x_{ik}$</span>, simply substitute <span style='color:#00a000;'>$x_{ij} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> x_{ik}$</span> and 
<span style='color:#00a000;'>$r_{ij} </span><span style='color:#606000;'>\rightarrow</span><span style='color:#00a000;'> r_{ik}$</span>. Further, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} &amp;= </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> r_{ij}}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}  = </span>
<span style='color:#00a000;'> (-2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> r_{ij}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}} - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> r_{jk}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}})</span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> (r_{ij}^2 + r_{ik}^2 + r_{jk}^2)] </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= -2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{ij} - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{jk}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The corresponding expression for <span style='color:#00a000;'>$x_{ik}$</span> is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = </span>
<span style='color:#00a000;'> -2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{ik} + 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{jk}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
Lastly, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} &amp;=  </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span>
<span style='color:#00a000;'> f_c(r_{ik}) f_c(r_{jk}) + f_c(r_{ij})f_c(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{jk})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= f_c(r_{ik})</span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>(M(r_{ij})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}f_c(r_{jk}) - </span>
<span style='color:#00a000;'> f_c(r_{ij}) M(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}} </span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
The corresponding expression for <span style='color:#00a000;'>$x_{ik}$</span> is
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>subequations</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} &amp;= f_c(r_{ij})</span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}}f_c(r_{jk}) + </span>
<span style='color:#00a000;'> f_c(r_{ik}) </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{jk})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} </span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;= f_c(r_{ij})</span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>(M(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}}f_c(r_{jk}) + </span>
<span style='color:#00a000;'> f_c(r_{ik}) M(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}} </span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>subequations</span>}
Then, taking <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G4Derivative</span>} into account:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^4}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = &amp;</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ij}r_{ik}} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ij}^2}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>)KF_2F_3 </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> - </span><span style='color:#606000;'>\\</span><span style='color:#00a000;'> &amp;2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 x_{jk} - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 x_{ij} </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;F_1F_2 f_c(r_{ik})</span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>(M(r_{ij})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}f_c(r_{jk}) +</span>
<span style='color:#00a000;'> f_c(r_{ij}) M(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}} </span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and for <span style='color:#00a000;'>$x_{ik}$</span>:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^4}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = &amp;</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}r_{ik}} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ik}^2}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>)KF_2F_3 </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span><span style='color:#00a000;'> &amp;2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 x_{jk} - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 x_{ik} </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;F_1F_2 f_c(r_{ij})</span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>(M(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}}f_c(r_{jk}) - </span>
<span style='color:#00a000;'> f_c(r_{ik}) M(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{jk}}{r_{jk}} </span><span style='color:#606000;'>\Biggr</span><span style='color:#00a000;'>)</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
or in terms of <span style='color:#00a000;'>$x_{ij}$</span> and <span style='color:#00a000;'>$x_{ik}$</span> and <span style='color:#00a000;'>$x_{jk}$</span>:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^4}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = &amp;x_{ij} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ij}^2}K</span>
<span style='color:#00a000;'> F_2F_3 - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span><span style='color:#00a000;'> </span>
<span style='color:#00a000;'> &amp;F_1F_2M(r_{ij})f_c(r_{ik})f_c(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{ij}}</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;x_{ik}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{KF_2F_3}{r_{ij}r_{ik}} - x_{jk}</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>(F_1F_2M(r_{jk})f_c(r_{ik})f_c(r_{ij}) + 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>)</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{jk}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^4}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = &amp;</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ik}^2}K</span>
<span style='color:#00a000;'> F_2F_3 - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;F_1F_2M(r_{ik})f_c(r_{ij})f_c(r_{jk})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{ik}}</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>) + </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;x_{ij}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{KF_2F_3}{r_{ij}r_{ik}} + x_{jk}</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>(F_1F_2M(r_{jk})f_c(r_{ij})f_c(r_{ik}) - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3</span><span style='color:#606000;'>\Bigr</span><span style='color:#00a000;'>)</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{jk}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
The derivative of <span style='color:#00a000;'>$G_i^5$</span> <span style='color:#f00000;'>\eqref</span>{<span style='color:#0000d0;'>G5</span>} is found in a similar way,
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> G_i^5 = F_1(</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>)F_2(r_{ij},r_{ik})F_3(r_{ij},r_{ij})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
where 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>align</span>}
<span style='color:#00a000;'> F_1(</span><span style='color:#606000;'>\theta</span><span style='color:#00a000;'>) &amp;= 2^{1-</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>}(1 + </span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>_{ijk})^</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'> </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> F_2(r_{ij},r_{ik}) &amp;= </span><span style='color:#606000;'>\exp</span><span style='color:#00a000;'>[-</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> (r_{ij}^2 + r_{ik}^2)] </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> F_3(r_{ij},r_{ij}) &amp;= f_c(r_{ij}) f_c(r_{ik})</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span>}
The derivative of <span style='color:#00a000;'>$F_1$</span> is the same, while for <span style='color:#00a000;'>$F_2$</span> we obtain
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = -2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{ij}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_2}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = -2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_2 x_{ik}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
For <span style='color:#00a000;'>$F_3$</span>, 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ij})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}</span>
<span style='color:#00a000;'> f_c(r_{ik}) = M(r_{ij})f_c(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#00a000;'>  </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> F_3}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = f_c(r_{ij})</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> f_c(r_{ik})}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}} = </span>
<span style='color:#00a000;'> f_c(r_{ij})M(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
so that
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^5}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = &amp;</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ij}r_{ik}} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ij}^2}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>)KF_2F_3 -</span>
<span style='color:#00a000;'> 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1 F_2F_3 x_{ij} </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;F_1F_2 M(r_{ij})f_c(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^5}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = &amp;</span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}}{r_{ij}r_{ik}} - </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ij}</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ik}^2}</span>
<span style='color:#00a000;'> </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>)KF_2F_3 -</span>
<span style='color:#00a000;'> 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1 F_2F_3 x_{ik} </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span><span style='color:#606000;'>\\</span>
<span style='color:#00a000;'> &amp;F_1F_2 M(r_{ik})f_c(r_{ij})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{x_{ik}}{r_{ik}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
In terms of <span style='color:#00a000;'>$x_{ij}$</span> and <span style='color:#00a000;'>$x_{ik}$</span>:
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^5}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ij}} = </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> &amp;x_{ij} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ij}^2}K</span>
<span style='color:#00a000;'> F_2F_3 - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 + </span><span style='color:#606000;'>\\</span><span style='color:#00a000;'> &amp;F_1F_2M(r_{ij})f_c(r_{ik})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{ij}} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span>
<span style='color:#00a000;'> x_{ik}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{KF_2F_3}{r_{ij}r_{ik}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}
and 
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>equation</span>}
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>aligned</span>}
<span style='color:#00a000;'> </span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> G_i^5}{</span><span style='color:#606000;'>\partial</span><span style='color:#00a000;'> x_{ik}} = </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> &amp;x_{ik} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>(-</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{</span><span style='color:#606000;'>\cos\theta</span><span style='color:#00a000;'>}{r_{ik}^2}K</span>
<span style='color:#00a000;'> F_2F_3 - 2</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'> F_1F_2F_3 + </span><span style='color:#606000;'>\\</span><span style='color:#00a000;'> &amp;F_1F_2M(r_{ik})f_c(r_{ij})</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{1}{r_{ik}} </span><span style='color:#606000;'>\biggr</span><span style='color:#00a000;'>) </span><span style='color:#606000;'>\,</span><span style='color:#00a000;'> + </span>
<span style='color:#00a000;'> x_{ij}</span><span style='color:#606000;'>\frac</span><span style='color:#00a000;'>{KF_2F_3}{r_{ij}r_{ik}}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>align</span><span style='color:#00a000;'>ed}</span>
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>equation</span>}


<span style='color:#f00000;'>\chapter</span>{<b><span style='color:#000000;'>Symmetry function parameters</span></b>} <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>app:symmFuncParameters</span>}
Here we list the symmetry parameters employed in the construction of the Si NNP in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}.
<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>}[h] 
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{6cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> ll}
      <span style='color:#800000;'>\toprule</span>
      <span style='color:#00a000;'>$r_c$</span> <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$r_s$</span> <span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 0.3 <b><span style='color:#002793;'>&amp;</span></b> 0 <span style='color:#800000;'>\\</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 0.1 <b><span style='color:#002793;'>&amp;</span></b> 0 <span style='color:#800000;'>\\</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 0.05 <b><span style='color:#002793;'>&amp;</span></b> 0 <span style='color:#800000;'>\\</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 0.005 <b><span style='color:#002793;'>&amp;</span></b> 0 <span style='color:#800000;'>\\</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 5.0 <b><span style='color:#002793;'>&amp;</span></b> 4.2 <span style='color:#800000;'>\\</span> 
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 5.0 <b><span style='color:#002793;'>&amp;</span></b> 3.5 <span style='color:#800000;'>\\</span>
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 5.0 <b><span style='color:#002793;'>&amp;</span></b> 2.7 <span style='color:#800000;'>\\</span> 
      6.0 <b><span style='color:#002793;'>&amp;</span></b> 5.0 <b><span style='color:#002793;'>&amp;</span></b> 2.0 <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Symmetry function parameters for <span style='color:#00a000;'>$G_i^2$</span> employed in the fitting of the Si NNP in <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}. 
	       The same parameters were used in the initial fit and all subsequent fits. <span style='color:#00a000;'>$r_c$</span> and <span style='color:#00a000;'>$r_s$</span> have units <span style='color:#800000;'>\SI</span>{}{<span style='color:#800000;'>\angstrom</span>}, 
	       while <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> is in units of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-2}$</span>.} 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:symmParamsInitialG2</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}

<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>table</span>}[h] 
  <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>tabular*</span>}{8cm}{l <span style='color:#002793;'>@{\extracolsep{\fill}}</span> llr}
      <span style='color:#800000;'>\toprule</span>
      <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$r_c$</span> <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> <b><span style='color:#002793;'>&amp;</span></b> <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span><span style='color:#800000;'>\\</span> 
      <span style='color:#800000;'>\hline</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 1 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 3.0 <b><span style='color:#002793;'>&amp;</span></b> 1 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span> 
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 1 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span> 
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 3.0 <b><span style='color:#002793;'>&amp;</span></b> 1 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 2 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 3.0 <b><span style='color:#002793;'>&amp;</span></b> 2 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 2 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 4.0 (3.0) <b><span style='color:#002793;'>&amp;</span></b> 2 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 4 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 4.0 (3.0) <b><span style='color:#002793;'>&amp;</span></b> 4 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 4 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 4.0 (3.0) <b><span style='color:#002793;'>&amp;</span></b> 4 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 4.0 (3.0) <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> 1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 6.0 <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      0.01 <b><span style='color:#002793;'>&amp;</span></b> 4.0 (3.0) <b><span style='color:#002793;'>&amp;</span></b> 16 <b><span style='color:#002793;'>&amp;</span></b> -1 <span style='color:#800000;'>\\</span>
      <span style='color:#800000;'>\bottomrule</span>
      <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>tabular*</span>} 
    <span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>center</span>}
    <span style='color:#800000;'>\captionsetup</span>{width=12cm}
      <span style='color:#800000;'>\caption</span>{Symmetry function parameters for <span style='color:#00a000;'>$G_i^5$</span> employed in the fitting of the Si NNP in 
               <span style='color:#f00000;'>\autoref</span>{<span style='color:#0000d0;'>sec:SiPotentialResults</span>}. For parameters that were adjusted between the initial and final fits, 
               the values of the initial fit are enclosed in parentheses. <span style='color:#00a000;'>$r_c$</span> have units <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}$</span>, while
               <span style='color:#00a000;'>$</span><span style='color:#606000;'>\eta</span><span style='color:#00a000;'>$</span> is in units of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\SI</span><span style='color:#00a000;'>{}{</span><span style='color:#606000;'>\angstrom</span><span style='color:#00a000;'>}^{-2}$</span>. <span style='color:#00a000;'>$</span><span style='color:#606000;'>\zeta</span><span style='color:#00a000;'>$</span> and <span style='color:#00a000;'>$</span><span style='color:#606000;'>\lambda</span><span style='color:#00a000;'>$</span> are unitless. } 
  <span style='color:#f00000;'>\label</span>{<span style='color:#0000d0;'>tab:symmParamsInitialG5</span>} 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>table</span>}



<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>appendices</span>}





<span style='color:#800000;'>\newpage</span>

<span style='color:#f00000;'>\begin</span>{<span style='color:#0000d0;'>thebibliography</span>}{56}

<span style='color:#800000;'>\bibitem</span>{Behler11general}
 J. Behler.
 ''Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations''.
 In: <span style='color:#800000;'>\textit</span>{Phys. Chem. Chem. Phys.} 13. (2011), 17930-17955. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1039/c1cp21668f}{10.1039/c1cp21668f}. 
 
<span style='color:#800000;'>\bibitem</span>{Behler11symmetry}
 J. Behler.
 ''Atom-centered symmetry functions for constructing high-dimensional neural network potentials''.
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 134. (2011), 074106. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.3553717}{10.1063/1.3553717}.
 
<span style='color:#800000;'>\bibitem</span>{Dragly14}
 S. A. Dragly.
 ''Bridging quantum mechanics and molecular dynamics with artificial neural networks''.
 MA thesis. University of Oslo (2014). <span style='color:#800000;'>\\</span>
 URL: <span style='color:#800000;'>\href</span>{https://www.duo.uio.no/handle/10852/41843}{https://www.duo.uio.no/handle/10852/41843}.

<span style='color:#800000;'>\bibitem</span>{Hornik89}
 K. Hornik, M. Stinchcombe, and H. White.
 ''Multilayer Feedforward Networks are Universal Approximators''.
 In: <span style='color:#800000;'>\textit</span>{Neural Networks} 2. (1989), 359. 
 DOI: <span style='color:#800000;'>\href</span>{https://doi.org/10.1016/0893-6080(89)90020-8}{10.1016/0893-6080(89)90020-8}
 
<span style='color:#800000;'>\bibitem</span>{Rojas96}
 R. Rojas. 
 <span style='color:#800000;'>\textit</span>{Neural Networks: A Systematic Introduction}.
 Springer, 1996.
 
<span style='color:#800000;'>\bibitem</span>{Karlik11}
 B. Karlik and A. V. Olgac.
 ''Performance Analysis of Various Activation Functions in Generalized MLP Architectures of Neural Networks''.
 In: <span style='color:#800000;'>\textit</span>{IJAE} 1. (2011), 111-122.
 
<span style='color:#800000;'>\bibitem</span>{LeCun15}
 Y. LeCun, Y. Bengio, and G. Hinton.
 ''Deep learning''.
 In: <span style='color:#800000;'>\textit</span>{Nature} 521. (2015), 436-444. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1038/nature14539}{10.1038/nature14539}
 
<span style='color:#800000;'>\bibitem</span>{Glorot11}
 X. Glorot, A. Bordes, and Y. Bengio.
 ''Deep Sparse Rectifier Neural Networks''.
 In: <span style='color:#800000;'>\textit</span>{JMLR} 15. (2011), 315-323.
 
<span style='color:#800000;'>\bibitem</span>{Rumelhart86}
 D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
 ''Learning representations by back-propagating errors''.
 In: <span style='color:#800000;'>\textit</span>{Nature} 323. (1986), 533-536.
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1038/323533a0}{10.1038/323533a0}
 
<span style='color:#800000;'>\bibitem</span>{Behler15}
 J. Behler.
 ''Constructing High-Dimensional Neural Network Potentials: A Tutorial Review''.
 In: <span style='color:#800000;'>\textit</span>{International Journal of Quantum Chemistry} 115. (2015), 1032-1050.
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1002/qua.24890}{10.1002/qua.24890}
 
<span style='color:#800000;'>\bibitem</span>{Tuckerman92}
 Tuckerman, Berne, and Martyna .
 ''Reversible multiscale molecular dynamics''.
 In: <span style='color:#800000;'>\textit</span>{J. Chem. Phys.} 97. (1992), 1990.
 
<span style='color:#800000;'>\bibitem</span>{Plimpton95}
 S. Plimpton. 
 ''Fast Parallel Algorithms for Short-Range Molecular Dynamics''.
 In: <span style='color:#800000;'>\textit</span>{Journal of Computational Physics} 117. (1995) 1-19. 
 DOI: <span style='color:#800000;'>\href</span>{https://doi.org/10.1006/jcph.1995.1039}{10.1006/jcph.1995.1039}. 
 
<span style='color:#800000;'>\bibitem</span>{Abadi15}
 M. Abadi, A. Agarwal, P. Barham, E. Brevdo,
 Z. Chen, C. Citro, G. S. Corrado, A. Davis,
 J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
 A. Harp, G. Irving, M. Isard, R. Jozefowicz, Y. Jia,
 L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, M. Schuster,
 R. Monga, S. Moore, D. Murray, C. Olah, J. Shlens,
 B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
 V. Vanhoucke, V. Vasudevan, F. Viégas,
 O. Vinyals, P. Warden, M. Wattenberg, M. Wicke,
 Y. Yu and X. Zheng.
 ''TensorFlow: Large-scale machine learning on heterogeneous systems''.
 <span style='color:#800000;'>\textit</span>{Google Research} (2015). 
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1603.04467}{1603.04467}
 
<span style='color:#800000;'>\bibitem</span>{Abadi16}
 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,
 M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,
 J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,
 V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng.
 ''TensorFlow: A System for Large-Scale Machine Learning''.
 <span style='color:#800000;'>\textit</span>{Google Brain} (2016).
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1605.08695}{1605.08695}
 
<span style='color:#800000;'>\bibitem</span>{Qian99}
 N. Qian.
 ''On the momentum term in gradient descent learning algorithms''
 In: <span style='color:#800000;'>\textit</span>{Neural Networks} 12(1). (1999) 145-151. 
 DOI: <span style='color:#800000;'>\href</span>{https://doi.org/10.1016/S0893-6080(98)00116-6}{10.1016/S0893-6080(98)00116-6}
 
<span style='color:#800000;'>\bibitem</span>{Duchi11}
 J. Duchi, E. Hazan, and Y. Singer.
 ''Adaptive Subgradient Methods for Online Learning and Stochastic Optimization''.
 In <span style='color:#800000;'>\textit</span>{The Journal of Machine Learning Research} 12. (2011) 2121-2159. 
 URL: <span style='color:#800000;'>\href</span>{http://jmlr.org/papers/v12/duchi11a.html}{http://jmlr.org/papers/v12/duchi11a.html}
 
<span style='color:#800000;'>\bibitem</span>{Zeiler12}
 M. D. Zeiler. 
 ''Adadelta: an adaptive learning rate method''. (2012).
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1212.5701}{1212.5701}
 
<span style='color:#800000;'>\bibitem</span>{Kingma14}
 D. P. Kingma and J. Ba.
 ''Adam: A Method for Stochastic Optimization''. (2014).
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1412.6980}{1412.6980}
 
<span style='color:#800000;'>\bibitem</span>{Ischtwan94}
 J. Ischtwan and M. A. Collins.
 ''Molecular potential energy surfaces by interpolation''.
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 100. (1994), 8080.
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.466801}{10.1063/1.466801}
 
<span style='color:#800000;'>\bibitem</span>{Raff05}
 L. M. Raff, M. Malshe, M. Hagan, D. I. Doughan, M. G. Rockley, and R. Komanduri.
 ''Ab inito potential-energy surfaces for complex, multichannel systems using modified
 novelty sampling and feedforward neural networks''. 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 122. (2005), 084104. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.1850458}{10.1063/1.1850458}
 
<span style='color:#800000;'>\bibitem</span>{Raff12}
 L. M. Raff, R. Komanduri, M. Hagan, and S. T. S. Bukkapatnam 
 <span style='color:#800000;'>\textit</span>{Neural networks in chemical reaction dynamics}.
 Oxford University Press, 2012.
 
<span style='color:#800000;'>\bibitem</span>{Pukrittayakamee09}
 A. Pukrittayakamee, M. Malshe, M. Hagan, L. M. Raff, and R. Narulkar, 
 S. Bukkapatnum and R. Komanduri. 
 ''Simultaneous fitting of a potential-energy surface and its corresponding force fields
 using feedforward neural networks''. 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 130. (2009) 134101.
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.3095491}{10.1063/1.3095491}
 
<span style='color:#800000;'>\bibitem</span>{McCulloch43}
 W. S. McCulloch and W. Pitts.
 ''A logical calculus of the ideas immanent in nervous activity''.
 In: <span style='color:#800000;'>\textit</span>{Bulletin of Mathematical Biophysics} 5. (1943), 115. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1007/BF02478259}{10.1007/BF02478259}
 
<span style='color:#800000;'>\bibitem</span>{Kriesel07}
 D. Kriesel. 
 ''A Breif Introduction to Neural Networks''
 Available at <span style='color:#800000;'>\href</span>{http://www.dkriesel.com}{http://www.dkriesel.com}
 
<span style='color:#800000;'>\bibitem</span>{LeCun99}
 Y. LeCun, P. Haffner, L. Bottou, and Y. Bengio. 
 ''Object recognition with Gradient-Based Learning''.
 In: <span style='color:#800000;'>\textit</span>{Shape, Contour and Grouping in Computer Vision. Lecture Notes in Computer Science}.
 1681. (1999) 319-345. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1007/3-540-46805-6_19}{10.1007/3-540-46805-6<span style='color:#800000;'>\_</span>19}
 
<span style='color:#800000;'>\bibitem</span>{Rosenblatt58}
 F. Rosenblatt. 
 ''The Perceptron: A probabilistic model for information storage and organization in the brain.''
 In: <span style='color:#800000;'>\textit</span>{Psychological Review} 65. (1958), 386-408. 
 
<span style='color:#800000;'>\bibitem</span>{Ruder16}
 S. Ruder.
 ''An overview of gradient descent optimization algorithms.''
 (2016). 
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1609.04747}{1609.04747}
 
<span style='color:#800000;'>\bibitem</span>{Dawes08}
 R. Dawes, D. L. Thompson, A. F. Wagner, and Michael Minkoff. 
 ''Interpolating moving least-squares methods for fitting potential energy surfaces:
 A strategy for efficient automatic data point placement in high dimensions''. 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 128. (2008), 084107.
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.2831790}{10.1063/1.2831790}
 
<span style='color:#800000;'>\bibitem</span>{Jones24}
 J. E. Jones. 
 ''On the Determination of Molecular Fields - II. From the Equation of State of a Gas.''
 In: <span style='color:#800000;'>\textit</span>{Proceedings of the Royal Society of London. Series A} 106. (1924), 463. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1098/rspa.1924.0082}{10.1098/rspa.1924.0082}
 
<span style='color:#800000;'>\bibitem</span>{Stillinger85}
 F. H. Stillinger and T. A. Weber. 
 ''Computer simulation of local order in condensed phases of silicon.'' 
 In: <span style='color:#800000;'>\textit</span>{Physical Review B} 31. (1985), 5262. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/0.1103/PhysRevB.31.5262}{10.1103/PhysRevB.31.5262}
 
<span style='color:#800000;'>\bibitem</span>{Molinero08}
 V. Molinero, E. B. Moore. 
 ''Water modeled as an intermediate element between carbon and silico.'' 
 In: <span style='color:#800000;'>\textit</span>{J. Phys. Chem. B} 113 (13). (2008), 4008-4016. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1021/jp805227c}{10.1021/jp805227c}
 
<span style='color:#800000;'>\bibitem</span>{Vashishta90}
 P. Vashishta, R. K. Kalia, J. P. Rino, and I. Ebbsj<span style='color:#800000;'>\&quot;</span>{o}.
 ''Interaction potential for <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span>: A molecular-dynamics study of structural correlations.''
 In: <span style='color:#800000;'>\textit</span>{Physical Review B} 41. (1990), 12197. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1103/PhysRevB.41.12197}{10.1103/PhysRevB.41.12197}
 
<span style='color:#800000;'>\bibitem</span>{Vashishta07}
 P. Vashishta, R. K. Kalia, A. Nakano, J. P. Rino. 
 ''Interaction potential for silicon carbide: A molecular dynamics study of elastic constants
 and vibrational density of states for crystalline and amorphous silicon carbide''. 
 In: <span style='color:#800000;'>\textit</span>{Journal of Applied Physics} 101. (2007), 103515. 
 
<span style='color:#800000;'>\bibitem</span>{Frenkel01}
 D. Frenkel and B. Smit. 
 ''Understanding molecular simulation: from algorithms to applications.''
 Academic Press, 2001.

<span style='color:#800000;'>\bibitem</span>{Agrawal06}
 P. M. Agrawal, L. M. Raff, M. T. Hagan, and R. Komanduri. 
 ''Molecular dynamics investigations of the dissociation of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{SiO}_2$</span> on an <span style='color:#800000;'>\textit</span>{ab inito}
 potential energy surface obtained using neural network methods.'' 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Phyics} 124. (2006), 134306. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.2185638}{10.1063/1.2185638}
 
<span style='color:#800000;'>\bibitem</span>{Bholoa07}
 A. Bholoa, S. D. Kenny, and R. Smith.
 ''A new approach to potential fitting using neural networks''.
 In: <span style='color:#800000;'>\textit</span>{Nuclear Instruments and Methods in Physics Research Section B: 
 Beam Interactions with Materials and Atoms} 255. (2007) 1.
 DOI: <span style='color:#800000;'>\href</span>{https://doi.org/10.1016/j.nimb.2006.11.040}{10.1016/j.nimb.2006.11.040}
 
<span style='color:#800000;'>\bibitem</span>{Behler07}
 J. Behler and Michele Parrinello. 
 ''Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces''.
 In: <span style='color:#800000;'>\textit</span>{Physical Review Letters} 98. (2007), 146401. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1103/PhysRevLett.98.146401}{10.1103/PhysRevLett.98.146401}
 
<span style='color:#800000;'>\bibitem</span>{LeCun12}
 Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller.
 ''Efficient backprop''. 
 In: G. Montavon, G. B. Orr, K.-R. Müller (Eds.), <span style='color:#800000;'>\textit</span>{Neural Networks: Tricks of the Trade} (2nd edition, pp. 9-48).
 Springer, 2012. 
 
<span style='color:#800000;'>\bibitem</span>{Bengio12}
 Y. Bengio. 
 ''Practical Recommendations for Gradient-Based Training of Deep Architectures.''
 In: G. Montavon, G. B. Orr, K.-R. Müller (Eds.), <span style='color:#800000;'>\textit</span>{Neural Networks: Tricks of the Trade} (2nd edition, pp. 437-478). 
 Springer, 2012. 
 arXiv: <span style='color:#800000;'>\href</span>{arxiv.org/abs/1206.5533v2}{1206.5533v2}
 
<span style='color:#800000;'>\bibitem</span>{Larochelle09}
 H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin. 
 ''Exploring Strategies for Training Deep Neural Networks''. 
 In: <span style='color:#800000;'>\textit</span>{Journal of Machine Learning Research} 10. (2009), 1-40. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1145/1577069.1577070}{10.1145/1577069.1577070}
 
<span style='color:#800000;'>\bibitem</span>{Witkoskie05}
 J. B. Witkoskie, and D. J. Doren. 
 ''Neural Network Models of Potential Energy Surfaces: Prototypical Examples''. 
 In: <span style='color:#800000;'>\textit</span>{Journal of Chemical Theory and Computation} 1. (2005), 14-23. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1021/ct049976i}{10.1021/ct049976i}
 
<span style='color:#800000;'>\bibitem</span>{Bengio07}
 Y. Bengio, and Y. LeCun. 
 ''Scaling learning algorithms towards AI''. 
 In: L. Bottou, O. Chapelle, D. DeCoste, and J. Weston (Eds.), 
 <span style='color:#800000;'>\textit</span>{Large-scale kernel machines}. MIT Press, 2007. 
 
<span style='color:#800000;'>\bibitem</span>{Glorot10}
 X. Glorot, and Y. Bengio. 
 ''Understanding the difficulty of training deep feedforward neural networks.''
 In: <span style='color:#800000;'>\textit</span>{Proceedings of AISTATS} 9. (2010), 249-256. 
 
<span style='color:#800000;'>\bibitem</span>{Krogh1992}
 A. Krogh, and J. A. Hertz. 
 ''A simple weight decay can improve generalization''.
 In: D. S. Lippman, J. E. Moody, and D. S. Touretzky (Eds.)
 <span style='color:#800000;'>\textit</span>{Advances in neural information processing systems vol. 4} (pp. 950-957). 
 Morgan Kaufmann. 
 
<span style='color:#800000;'>\bibitem</span>{Srivastava14}
 N Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov. 
 ''Dropout: A Simple Way to Prevent Neural Networks from Overfitting.''
 In: <span style='color:#800000;'>\textit</span>{Journal of Machine Learning Research} 15. (2014), 1929-1958. 
 
<span style='color:#800000;'>\bibitem</span>{Krizhevsky12}
 A. Krizhevsky, I. Sutskever, and G. Hinton.
 ''ImageNet classification with deep
 convolutional neural networks.'' 
 In: <span style='color:#800000;'>\textit</span>{Proc. Advances in Neural Information Processing Systems} 25. (2012), 1090–1098.
 
<span style='color:#800000;'>\bibitem</span>{Hinton12}
 G. Hinton, et al.<span style='color:#800000;'>\ </span>
 ''Deep neural networks for acoustic modeling in speechrecognition.'' 
 In: <span style='color:#800000;'>\textit</span>{IEEE Signal Processing Magazine} 29, (2012), 82–97.
 
<span style='color:#800000;'>\bibitem</span>{Collobert11}
 R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 
 ''Natural Language Processing (Almost) from Scratch''.
 In: <span style='color:#800000;'>\textit</span>{Journal of Machine Learning Research} 12. (2011), 2493-2537. 
 arXiv: <span style='color:#800000;'>\href</span>{https://arxiv.org/abs/1103.0398}{1103.0398}
 
<span style='color:#800000;'>\bibitem</span>{Ciodaro12}
 T. Ciodaro, D. Deva, J. M. de Seixas, and D. Damazio. 
 ''Online particle detection with Neural Networks based on topological calorimetry information''. 
 In: <span style='color:#800000;'>\textit</span>{Journal of Physics: Conference Series}, 368. (2012), 012030. 
 
<span style='color:#800000;'>\bibitem</span>{Helmstaedter13}
 M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, W. Denk. 
 ''Connectomic reconstruction of the inner plexiform layer in the mouse retina''. 
 In: <span style='color:#800000;'>\textit</span>{Nature} 500. (2013), 168-174. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1038/nature12346}{10.1038/nature12346}
 
 <span style='color:#800000;'>\bibitem</span>{Carleo17}
 G. Carleo, and M. Troyer. 
 ''Solving the quantum many-body problem with artifical neural networks''. 
 In: <span style='color:#800000;'>\textit</span>{Science} 355. (2017), 602-606. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1126/science.aag2302}{10.1126/science.aag2302}
 
<span style='color:#800000;'>\bibitem</span>{Krenn16}
 M. Krenn, M. Malik, R. Fickler, R. Lapkiewicz, and A. Zeilinger. 
 ''Automated Search for new Quantum Experiments''. 
 In: <span style='color:#800000;'>\textit</span>{Physical Review Letters} 116. (2016), 090405. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1103/PhysRevLett.116.090405}{10.1103/PhysRevLett.116.090405}
 
<span style='color:#800000;'>\bibitem</span>{Prudente98}
 F. V. Prudente, P. H. Acioli, and J. J. S. Neto. 
 ''The fitting of potential energy surfaces using neural networks: Application to the study of 
 vibrational levels of <span style='color:#00a000;'>$</span><span style='color:#606000;'>\mathrm</span><span style='color:#00a000;'>{H}_3^+$</span>''. 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 109. (1998), 8801. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.477550}{10.1063/1.477550}
 
<span style='color:#800000;'>\bibitem</span>{Natarajan16}
 S. K. Natarajan, and J. Behler. 
 ''Neural network molecular dynamics simulation of solid-liquid interfaces: water at low-index copper surfaces''. 
 In: <span style='color:#800000;'>\textit</span>{Phys. Chem. Chem. Phys} 18. (2016), 28704. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1039/c6cp05711j}{10.1039/c6cp05711j}
 
<span style='color:#800000;'>\bibitem</span>{Artrith12}
 N. Artrith, and J. Behler. 
 ''High-dimensional neural network potentials for metal surfaces: A prototype study for copper''. 
 In: <span style='color:#800000;'>\textit</span>{Physical Review B} 85. (2012), 045439. 
 DOI: <span style='color:#800000;'>\href</span>{http://doi.org/10.1103/PhysRevB.85.045439}{10.1103/PhysRevB.85.045439}
 
<span style='color:#800000;'>\bibitem</span>{Hopcroft10}
 M. A. Hopcroft, W. D. Nix, and T. W. Kenny. 
 ''What is the Young's Modulus of Silicon?''.
 In: <span style='color:#800000;'>\textit</span>{Journal of microelectromechanical systems} 19. (2010), 229-238. 
 
<span style='color:#800000;'>\bibitem</span>{Cowley88}
 E. R. Cowley. 
 ''Lattice Dynamics of Silicon with Empirical Many-Body Potentials''. 
 In: <span style='color:#800000;'>\textit</span>{Physical review letters} 60. (1988), 2379-2381. 
 
<span style='color:#800000;'>\bibitem</span>{Sherrill10}
 C. D. Sherrill. 
 ''Frontiers in electronic structure theory''. 
 In: <span style='color:#800000;'>\textit</span>{The Journal of Chemical Physics} 132. (2010), 110902. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1063/1.3369628}{10.1063/1.3369628}
 
<span style='color:#800000;'>\bibitem</span>{Eshet12}
 H. Eshet, R. Z. Khaliullin, T. D. Kühne, J. Behler, and M. Parrinello. 
 ''<span style='color:#800000;'>\textit</span>{Ab inito} quality neural-network potential for sodium''. 
 In: <span style='color:#800000;'>\textit</span>{Physical Review B} 81. (2010), 184107. 
 DOI: <span style='color:#800000;'>\href</span>{http://dx.doi.org/10.1103/PhysRevB.81.184107}{10.1103/PhysRevB.81.184107}
 
 
 
<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>thebibliography</span>}

<span style='color:#f00000;'>\end</span>{<span style='color:#0000d0;'>document</span>}</pre>
</body>
</html>
